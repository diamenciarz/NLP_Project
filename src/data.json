{"abstract": "  A fully differential calculation in perturbative quantum chromodynamics is\npresented for the production of massive photon pairs at hadron colliders. All\nnext-to-leading order perturbative contributions from quark-antiquark,\ngluon-(anti)quark, and gluon-gluon subprocesses are included, as well as\nall-orders resummation of initial-state gluon radiation valid at\nnext-to-next-to-leading logarithmic accuracy. The region of phase space is\nspecified in which the calculation is most reliable. Good agreement is\ndemonstrated with data from the Fermilab Tevatron, and predictions are made for\nmore detailed tests with CDF and DO data. Predictions are shown for\ndistributions of diphoton pairs produced at the energy of the Large Hadron\nCollider (LHC). Distributions of the diphoton pairs from the decay of a Higgs\nboson are contrasted with those produced from QCD processes at the LHC, showing\nthat enhanced sensitivity to the signal can be obtained with judicious\nselection of events.\n", "category": [3]}
{"abstract": "  We describe a new algorithm, the $(k,\\ell)$-pebble game with colors, and use\nit obtain a characterization of the family of $(k,\\ell)$-sparse graphs and\nalgorithmic solutions to a family of problems concerning tree decompositions of\ngraphs. Special instances of sparse graphs appear in rigidity theory and have\nreceived increased attention in recent years. In particular, our colored\npebbles generalize and strengthen the previous results of Lee and Streinu and\ngive a new proof of the Tutte-Nash-Williams characterization of arboricity. We\nalso present a new decomposition that certifies sparsity based on the\n$(k,\\ell)$-pebble game with colors. Our work also exposes connections between\npebble game algorithms and previous sparse graph algorithms by Gabow, Gabow and\nWestermann and Hendrickson.\n", "category": [2, 0]}
{"abstract": "  The evolution of Earth-Moon system is described by the dark matter field\nfluid model proposed in the Meeting of Division of Particle and Field 2004,\nAmerican Physical Society. The current behavior of the Earth-Moon system agrees\nwith this model very well and the general pattern of the evolution of the\nMoon-Earth system described by this model agrees with geological and fossil\nevidence. The closest distance of the Moon to Earth was about 259000 km at 4.5\nbillion years ago, which is far beyond the Roche's limit. The result suggests\nthat the tidal friction may not be the primary cause for the evolution of the\nEarth-Moon system. The average dark matter field fluid constant derived from\nEarth-Moon system data is 4.39 x 10^(-22) s^(-1)m^(-1). This model predicts\nthat the Mars's rotation is also slowing with the angular acceleration rate\nabout -4.38 x 10^(-22) rad s^(-2).\n", "category": [3]}
{"abstract": "  We show that a determinant of Stirling cycle numbers counts unlabeled acyclic\nsingle-source automata. The proof involves a bijection from these automata to\ncertain marked lattice paths and a sign-reversing involution to evaluate the\ndeterminant.\n", "category": [2]}
{"abstract": "  In this paper we show how to compute the $\\Lambda_{\\alpha}$ norm, $\\alpha\\ge\n0$, using the dyadic grid. This result is a consequence of the description of\nthe Hardy spaces $H^p(R^N)$ in terms of dyadic and special atoms.\n", "category": [2, 2]}
{"abstract": "  We study the two-particle wave function of paired atoms in a Fermi gas with\ntunable interaction strengths controlled by Feshbach resonance. The Cooper pair\nwave function is examined for its bosonic characters, which is quantified by\nthe correction of Bose enhancement factor associated with the creation and\nannihilation composite particle operators. An example is given for a\nthree-dimensional uniform gas. Two definitions of Cooper pair wave function are\nexamined. One of which is chosen to reflect the off-diagonal long range order\n(ODLRO). Another one corresponds to a pair projection of a BCS state. On the\nside with negative scattering length, we found that paired atoms described by\nODLRO are more bosonic than the pair projected definition. It is also found\nthat at $(k_F a)^{-1} \\ge 1$, both definitions give similar results, where more\nthan 90% of the atoms occupy the corresponding molecular condensates.\n", "category": [3]}
{"abstract": "  A rather non-standard quantum representation of the canonical commutation\nrelations of quantum mechanics systems, known as the polymer representation has\ngained some attention in recent years, due to its possible relation with Planck\nscale physics. In particular, this approach has been followed in a symmetric\nsector of loop quantum gravity known as loop quantum cosmology. Here we explore\ndifferent aspects of the relation between the ordinary Schroedinger theory and\nthe polymer description. The paper has two parts. In the first one, we derive\nthe polymer quantum mechanics starting from the ordinary Schroedinger theory\nand show that the polymer description arises as an appropriate limit. In the\nsecond part we consider the continuum limit of this theory, namely, the reverse\nprocess in which one starts from the discrete theory and tries to recover back\nthe ordinary Schroedinger quantum mechanics. We consider several examples of\ninterest, including the harmonic oscillator, the free particle and a simple\ncosmological model.\n", "category": [3]}
{"abstract": "  A general formulation was developed to represent material models for\napplications in dynamic loading. Numerical methods were devised to calculate\nresponse to shock and ramp compression, and ramp decompression, generalizing\nprevious solutions for scalar equations of state. The numerical methods were\nfound to be flexible and robust, and matched analytic results to a high\naccuracy. The basic ramp and shock solution methods were coupled to solve for\ncomposite deformation paths, such as shock-induced impacts, and shock\ninteractions with a planar interface between different materials. These\ncalculations capture much of the physics of typical material dynamics\nexperiments, without requiring spatially-resolving simulations. Example\ncalculations were made of loading histories in metals, illustrating the effects\nof plastic work on the temperatures induced in quasi-isentropic and\nshock-release experiments, and the effect of a phase transition.\n", "category": [3]}
{"abstract": "  Partial cubes are isometric subgraphs of hypercubes. Structures on a graph\ndefined by means of semicubes, and Djokovi\\'{c}'s and Winkler's relations play\nan important role in the theory of partial cubes. These structures are employed\nin the paper to characterize bipartite graphs and partial cubes of arbitrary\ndimension. New characterizations are established and new proofs of some known\nresults are given.\n  The operations of Cartesian product and pasting, and expansion and\ncontraction processes are utilized in the paper to construct new partial cubes\nfrom old ones. In particular, the isometric and lattice dimensions of finite\npartial cubes obtained by means of these operations are calculated.\n", "category": [2]}
{"abstract": "  In this paper we present an algorithm for computing Hecke eigensystems of\nHilbert-Siegel cusp forms over real quadratic fields of narrow class number\none. We give some illustrative examples using the quadratic field\n$\\Q(\\sqrt{5})$. In those examples, we identify Hilbert-Siegel eigenforms that\nare possible lifts from Hilbert eigenforms.\n", "category": [2, 2]}
{"abstract": "  Recently, Bruinier and Ono classified cusp forms $f(z) := \\sum_{n=0}^{\\infty}\na_f(n)q ^n \\in S_{\\lambda+1/2}(\\Gamma_0(N),\\chi)\\cap \\mathbb{Z}[[q]]$ that does\nnot satisfy a certain distribution property for modulo odd primes $p$. In this\npaper, using Rankin-Cohen Bracket, we extend this result to modular forms of\nhalf integral weight for primes $p \\geq 5$. As applications of our main theorem\nwe derive distribution properties, for modulo primes $p\\geq5$, of traces of\nsingular moduli and Hurwitz class number. We also study an analogue of Newman's\nconjecture for overpartitions.\n", "category": [2]}
{"abstract": "  Serre obtained the p-adic limit of the integral Fourier coefficient of\nmodular forms on $SL_2(\\mathbb{Z})$ for $p=2,3,5,7$. In this paper, we extend\nthe result of Serre to weakly holomorphic modular forms of half integral weight\non $\\Gamma_{0}(4N)$ for $N=1,2,4$. A proof is based on linear relations among\nFourier coefficients of modular forms of half integral weight. As applications\nwe obtain congruences of Borcherds exponents, congruences of quotient of\nEisentein series and congruences of values of $L$-functions at a certain point\nare also studied. Furthermore, the congruences of the Fourier coefficients of\nSiegel modular forms on Maass Space are obtained using Ikeda lifting.\n", "category": [2]}
{"abstract": "  In this article we discuss a relation between the string topology and\ndifferential forms based on the theory of Chen's iterated integrals and the\ncyclic bar complex.\n", "category": [2, 2]}
{"abstract": "  The pure spinor formulation of the ten-dimensional superstring leads to\nmanifestly supersymmetric loop amplitudes, expressed as integrals in pure\nspinor superspace. This paper explores different methods to evaluate these\nintegrals and then uses them to calculate the kinematic factors of the one-loop\nand two-loop massless four-point amplitudes involving two and four Ramond\nstates.\n", "category": [3]}
{"abstract": "  In this work, we evaluate the lifetimes of the doubly charmed baryons\n$\\Xi_{cc}^{+}$, $\\Xi_{cc}^{++}$ and $\\Omega_{cc}^{+}$. We carefully calculate\nthe non-spectator contributions at the quark level where the Cabibbo-suppressed\ndiagrams are also included. The hadronic matrix elements are evaluated in the\nsimple non-relativistic harmonic oscillator model. Our numerical results are\ngenerally consistent with that obtained by other authors who used the diquark\nmodel. However, all the theoretical predictions on the lifetimes are one order\nlarger than the upper limit set by the recent SELEX measurement. This\ndiscrepancy would be clarified by the future experiment, if more accurate\nexperiment still confirms the value of the SELEX collaboration, there must be\nsome unknown mechanism to be explored.\n", "category": [3]}
{"abstract": "  We give a prescription for how to compute the Callias index, using as\nregulator an exponential function. We find agreement with old results in all\nodd dimensions. We show that the problem of computing the dimension of the\nmoduli space of self-dual strings can be formulated as an index problem in\neven-dimensional (loop-)space. We think that the regulator used in this Letter\ncan be applied to this index problem.\n", "category": [3]}
{"abstract": "  In this note we give a new method for getting a series of approximations for\nthe extinction probability of the one-dimensional contact process by using the\nGr\\\"obner basis.\n", "category": [2, 2]}
{"abstract": "  The shape of the hadronic form factor f+(q2) in the decay D0 --> K- e+ nue\nhas been measured in a model independent analysis and compared with theoretical\ncalculations. We use 75 fb(-1) of data recorded by the BABAR detector at the\nPEPII electron-positron collider. The corresponding decay branching fraction,\nrelative to the decay D0 --> K- pi+, has also been measured to be RD = BR(D0\n--> K- e+ nue)/BR(D0 --> K- pi+) = 0.927 +/- 0.007 +/- 0.012. From these\nresults, and using the present world average value for BR(D0 --> K- pi+), the\nnormalization of the form factor at q2=0 is determined to be f+(0)=0.727 +/-\n0.007 +/- 0.005 +/- 0.007 where the uncertainties are statistical, systematic,\nand from external inputs, respectively.\n", "category": [3]}
{"abstract": "  Spatiotemporal pattern formation in a product-activated enzymic reaction at\nhigh enzyme concentrations is investigated. Stochastic simulations show that\ncatalytic turnover cycles of individual enzymes can become coherent and that\ncomplex wave patterns of molecular synchronization can develop. The analysis\nbased on the mean-field approximation indicates that the observed patterns\nresult from the presence of Hopf and wave bifurcations in the considered\nsystem.\n", "category": [3, 3, 4]}
{"abstract": "  We present Lie group integrators for nonlinear stochastic differential\nequations with non-commutative vector fields whose solution evolves on a smooth\nfinite dimensional manifold. Given a Lie group action that generates transport\nalong the manifold, we pull back the stochastic flow on the manifold to the Lie\ngroup via the action, and subsequently pull back the flow to the corresponding\nLie algebra via the exponential map. We construct an approximation to the\nstochastic flow in the Lie algebra via closed operations and then push back to\nthe Lie group and then to the manifold, thus ensuring our approximation lies in\nthe manifold. We call such schemes stochastic Munthe-Kaas methods after their\ndeterministic counterparts. We also present stochastic Lie group integration\nschemes based on Castell--Gaines methods. These involve using an underlying\nordinary differential integrator to approximate the flow generated by a\ntruncated stochastic exponential Lie series. They become stochastic Lie group\nintegrator schemes if we use Munthe-Kaas methods as the underlying ordinary\ndifferential integrator. Further, we show that some Castell--Gaines methods are\nuniformly more accurate than the corresponding stochastic Taylor schemes.\nLastly we demonstrate our methods by simulating the dynamics of a free rigid\nbody such as a satellite and an autonomous underwater vehicle both perturbed by\ntwo independent multiplicative stochastic noise processes.\n", "category": [2]}
{"abstract": "  The formation of quasi-2D spin-wave waveforms in longitudinally magnetized\nstripes of ferrimagnetic film was observed by using time- and space-resolved\nBrillouin light scattering technique. In the linear regime it was found that\nthe confinement decreases the amplitude of dynamic magnetization near the\nlateral stripe edges. Thus, the so-called effective dipolar pinning of dynamic\nmagnetization takes place at the edges.\n  In the nonlinear regime a new stable spin wave packet propagating along a\nwaveguide structure, for which both transversal instability and interaction\nwith the side walls of the waveguide are important was observed. The\nexperiments and a numerical simulation of the pulse evolution show that the\nshape of the formed waveforms and their behavior are strongly influenced by the\nconfinement.\n", "category": [3]}
{"abstract": "  We present recent advances in understanding of the ground and excited states\nof the electron-phonon coupled systems obtained by novel methods of\nDiagrammatic Monte Carlo and Stochastic Optimization, which enable the\napproximation-free calculation of Matsubara Green function in imaginary times\nand perform unbiased analytic continuation to real frequencies. We present\nexact numeric results on the ground state properties, Lehmann spectral function\nand optical conductivity of different strongly correlated systems: Frohlich\npolaron, Rashba-Pekar exciton-polaron, pseudo Jahn-Teller polaron, exciton, and\ninteracting with phonons hole in the t-J model.\n", "category": [3, 3]}
{"abstract": "  Zero-divisors (ZDs) derived by Cayley-Dickson Process (CDP) from\nN-dimensional hypercomplex numbers (N a power of 2, at least 4) can represent\nsingularities and, as N approaches infinite, fractals -- and thereby,scale-free\nnetworks. Any integer greater than 8 and not a power of 2 generates a\nmeta-fractal or \"Sky\" when it is interpreted as the \"strut constant\" (S) of an\nensemble of octahedral vertex figures called \"Box-Kites\" (the fundamental\nbuilding blocks of ZDs). Remarkably simple bit-manipulation rules or \"recipes\"\nprovide tools for transforming one fractal genus into others within the context\nof Wolfram's Class 4 complexity.\n", "category": [2]}
{"abstract": "  We describe a peculiar fine structure acquired by the in-plane optical phonon\nat the Gamma-point in graphene when it is brought into resonance with one of\nthe inter-Landau-level transitions in this material. The effect is most\npronounced when this lattice mode (associated with the G-band in graphene Raman\nspectrum) is in resonance with inter-Landau-level transitions 0 -> (+,1) and\n(-,1) -> 0, at a magnetic field B_0 ~ 30 T. It can be used to measure the\nstrength of the electron-phonon coupling directly, and its filling-factor\ndependence can be used experimentally to detect circularly polarized lattice\nmodes.\n", "category": [3]}
{"abstract": "  We prove pfaffian and hafnian versions of Lieb's inequalities on determinants\nand permanents of positive semi-definite matrices. We use the hafnian\ninequality to improve the lower bound of R\\'ev\\'esz and Sarantopoulos on the\nnorm of a product of linear functionals on a real Euclidean space (this subject\nis sometimes called the `real linear polarization constant' problem).\n", "category": [2, 2]}
{"abstract": "  In $\\XQM$, a quark can emit Goldstone bosons. The flavor symmetry breaking in\nthe Goldstone boson emission process is used to intepret the nucleon\nflavor-spin structure. In this paper, we study the inner structure of\nconstituent quarks implied in $\\XQM$ caused by the Goldstone boson emission\nprocess in nucleon. From a simplified model Hamiltonian derived from $\\XQM$,\nthe intrinsic wave functions of constituent quarks are determined. Then the\nobtained transition probabilities of the emission of Goldstone boson from a\nquark can give a reasonable interpretation to the flavor symmetry breaking in\nnucleon flavor-spin structure.\n", "category": [3]}
{"abstract": "  We investigate the effect of tuning the phonon energy on the correlation\neffects in models of electron-phonon interactions using DMFT. In the regime\nwhere itinerant electrons, instantaneous electron-phonon driven correlations\nand static distortions compete on similar energy scales, we find several\ninteresting results including (1) A crossover from band to Mott behavior in the\nspectral function, leading to hybrid band/Mott features in the spectral\nfunction for phonon frequencies slightly larger than the band width. (2) Since\nthe optical conductivity depends sensitively on the form of the spectral\nfunction, we show that such a regime should be observable through the low\nfrequency form of the optical conductivity. (3) The resistivity has a double\nkondo peak arrangement\n", "category": [3]}
{"abstract": "  We show that crystal can trap a broad (x, x', y, y', E) distribution of\nparticles and channel it preserved with a high precision. This sampled-and-hold\ndistribution can be steered by a bent crystal for analysis downstream. In\nsimulations for the 7 TeV Large Hadron Collider, a crystal adapted to the\naccelerator lattice traps 90% of diffractively scattered protons emerging from\nthe interaction point with a divergence 100 times the critical angle. We set\nthe criterion for crystal adaptation improving efficiency ~100-fold. Proton\nangles are preserved in crystal transmission with accuracy down to 0.1\nmicrorad. This makes feasible a crystal application for measuring very forward\nprotons at the LHC.\n", "category": [3]}
{"abstract": "  We analyze the possibility of probing non-standard neutrino interactions\n(NSI, for short) through the detection of neutrinos produced in a future\ngalactic supernova (SN).We consider the effect of NSI on the neutrino\npropagation through the SN envelope within a three-neutrino framework, paying\nspecial attention to the inclusion of NSI-induced resonant conversions, which\nmay take place in the most deleptonised inner layers. We study the possibility\nof detecting NSI effects in a Megaton water Cherenkov detector, either through\nmodulation effects in the $\\bar\\nu_e$ spectrum due to (i) the passage of shock\nwaves through the SN envelope, (ii) the time dependence of the electron\nfraction and (iii) the Earth matter effects; or, finally, through the possible\ndetectability of the neutronization $\\nu_e$ burst. We find that the $\\bar\\nu_e$\nspectrum can exhibit dramatic features due to the internal NSI-induced resonant\nconversion. This occurs for non-universal NSI strengths of a few %, and for\nvery small flavor-changing NSI above a few$\\times 10^{-5}$.\n", "category": [3]}
{"abstract": "  We performed a rigorous theoretical convergence analysis of the discrete\ndipole approximation (DDA). We prove that errors in any measured quantity are\nbounded by a sum of a linear and quadratic term in the size of a dipole d, when\nthe latter is in the range of DDA applicability. Moreover, the linear term is\nsignificantly smaller for cubically than for non-cubically shaped scatterers.\nTherefore, for small d errors for cubically shaped particles are much smaller\nthan for non-cubically shaped. The relative importance of the linear term\ndecreases with increasing size, hence convergence of DDA for large enough\nscatterers is quadratic in the common range of d. Extensive numerical\nsimulations were carried out for a wide range of d. Finally we discuss a number\nof new developments in DDA and their consequences for convergence.\n", "category": [3, 3]}
{"abstract": "  This is a supplement to the paper arXiv:q-bio/0701050, containing the text of\ncorrespondence sent to Nature in 1990.\n", "category": [4, 4]}
{"abstract": "  We propose an extrapolation technique that allows accuracy improvement of the\ndiscrete dipole approximation computations. The performance of this technique\nwas studied empirically based on extensive simulations for 5 test cases using\nmany different discretizations. The quality of the extrapolation improves with\nrefining discretization reaching extraordinary performance especially for\ncubically shaped particles. A two order of magnitude decrease of error was\ndemonstrated. We also propose estimates of the extrapolation error, which were\nproven to be reliable. Finally we propose a simple method to directly separate\nshape and discretization errors and illustrated this for one test case.\n", "category": [3, 3]}
{"abstract": "  The multisite phosphorylation-dephosphorylation cycle is a motif repeatedly\nused in cell signaling. This motif itself can generate a variety of dynamic\nbehaviors like bistability and ultrasensitivity without direct positive\nfeedbacks. In this paper, we study the number of positive steady states of a\ngeneral multisite phosphorylation-dephosphorylation cycle, and how the number\nof positive steady states varies by changing the biological parameters. We show\nanalytically that (1) for some parameter ranges, there are at least n+1 (if n\nis even) or n (if n is odd) steady states; (2) there never are more than 2n-1\nsteady states (in particular, this implies that for n=2, including single\nlevels of MAPK cascades, there are at most three steady states); (3) for\nparameters near the standard Michaelis-Menten quasi-steady state conditions,\nthere are at most n+1 steady states; and (4) for parameters far from the\nstandard Michaelis-Menten quasi-steady state conditions, there is at most one\nsteady state.\n", "category": [4, 4]}
{"abstract": "  In this manuscript we investigate the capabilities of the Discrete Dipole\nApproximation (DDA) to simulate scattering from particles that are much larger\nthan the wavelength of the incident light, and describe an optimized publicly\navailable DDA computer program that processes the large number of dipoles\nrequired for such simulations. Numerical simulations of light scattering by\nspheres with size parameters x up to 160 and 40 for refractive index m=1.05 and\n2 respectively are presented and compared with exact results of the Mie theory.\nErrors of both integral and angle-resolved scattering quantities generally\nincrease with m and show no systematic dependence on x. Computational times\nincrease steeply with both x and m, reaching values of more than 2 weeks on a\ncluster of 64 processors. The main distinctive feature of the computer program\nis the ability to parallelize a single DDA simulation over a cluster of\ncomputers, which allows it to simulate light scattering by very large\nparticles, like the ones that are considered in this manuscript. Current\nlimitations and possible ways for improvement are discussed.\n", "category": [3, 3]}
{"abstract": "  We present a review of the discrete dipole approximation (DDA), which is a\ngeneral method to simulate light scattering by arbitrarily shaped particles. We\nput the method in historical context and discuss recent developments, taking\nthe viewpoint of a general framework based on the integral equations for the\nelectric field. We review both the theory of the DDA and its numerical aspects,\nthe latter being of critical importance for any practical application of the\nmethod. Finally, the position of the DDA among other methods of light\nscattering simulation is shown and possible future developments are discussed.\n", "category": [3, 3]}
{"abstract": "  The quadratic pion scalar radius, \\la r^2\\ra^\\pi_s, plays an important role\nfor present precise determinations of \\pi\\pi scattering. Recently, Yndur\\'ain,\nusing an Omn\\`es representation of the null isospin(I) non-strange pion scalar\nform factor, obtains \\la r^2\\ra^\\pi_s=0.75\\pm 0.07 fm^2. This value is larger\nthan the one calculated by solving the corresponding Muskhelishvili-Omn\\`es\nequations, \\la r^2\\ra^\\pi_s=0.61\\pm 0.04 fm^2. A large discrepancy between both\nvalues, given the precision, then results. We reanalyze Yndur\\'ain's method and\nshow that by imposing continuity of the resulting pion scalar form factor under\ntiny changes in the input \\pi\\pi phase shifts, a zero in the form factor for\nsome S-wave I=0 T-matrices is then required. Once this is accounted for, the\nresulting value is \\la r^2\\ra_s^\\pi=0.65\\pm 0.05 fm^2. The main source of error\nin our determination is present experimental uncertainties in low energy S-wave\nI=0 \\pi\\pi phase shifts. Another important contribution to our error is the not\nyet settled asymptotic behaviour of the phase of the scalar form factor from\nQCD.\n", "category": [3, 3, 3]}
{"abstract": "  As in the cases of freeness and monotonic independence, the notion of\nconditional freeness is meaningful when complex-valued states are replaced by\npositive conditional expectations. In this framework, the paper presents\nseveral positivity results, a version of the central limit theorem and an\nanalogue of the conditionally free R-transform constructed by means of\nmultilinear function series.\n", "category": [2, 2]}
{"abstract": "  We formulate a quantum generalization of the notion of the group of\nRiemannian isometries for a compact Riemannian manifold, by introducing a\nnatural notion of smooth and isometric action by a compact quantum group on a\nclassical or noncommutative manifold described by spectral triples, and then\nproving the existence of a universal object (called the quantum isometry group)\nin the category of compact quantum groups acting smoothly and isometrically on\na given (possibly noncommutative) manifold satisfying certain regularity\nassumptions. In fact, we identify the quantum isometry group with the universal\nobject in a bigger category, namely the category of `quantum families of smooth\nisometries', defined along the line of Woronowicz and Soltan. We also construct\na spectral triple on the Hilbert space of forms on a noncommutative manifold\nwhich is equivariant with respect to a natural unitary representation of the\nquantum isometry group. We give explicit description of quantum isometry groups\nof commutative and noncommutative tori, and in this context, obtain the quantum\ndouble torus defined in \\cite{hajac} as the universal quantum group of\nholomorphic isometries of the noncommutative torus.\n", "category": [2, 2]}
{"abstract": "  It is outlined the possibility to extend the quantum formalism in relation to\nthe requirements of the general systems theory. It can be done by using a\nquantum semantics arising from the deep logical structure of quantum theory. It\nis so possible taking into account the logical openness relationship between\nobserver and system. We are going to show how considering the truth-values of\nquantum propositions within the context of the fuzzy sets is here more useful\nfor systemics . In conclusion we propose an example of formal quantum\ncoherence.\n", "category": [3]}
{"abstract": "  We construct a system of nonequilibrium entropy limiters for the lattice\nBoltzmann methods (LBM). These limiters erase spurious oscillations without\nblurring of shocks, and do not affect smooth solutions. In general, they do the\nsame work for LBM as flux limiters do for finite differences, finite volumes\nand finite elements methods, but for LBM the main idea behind the construction\nof nonequilibrium entropy limiter schemes is to transform a field of a scalar\nquantity - nonequilibrium entropy. There are two families of limiters: (i)\nbased on restriction of nonequilibrium entropy (entropy \"trimming\") and (ii)\nbased on filtering of nonequilibrium entropy (entropy filtering). The physical\nproperties of LBM provide some additional benefits: the control of entropy\nproduction and accurate estimate of introduced artificial dissipation are\npossible. The constructed limiters are tested on classical numerical examples:\n1D athermal shock tubes with an initial density ratio 1:2 and the 2D lid-driven\ncavity for Reynolds numbers Re between 2000 and 7500 on a coarse 100*100 grid.\nAll limiter constructions are applicable for both entropic and non-entropic\nquasiequilibria.\n", "category": [3, 3]}
{"abstract": "  We present a theoretical framework for plasma turbulence in astrophysical\nplasmas (solar wind, interstellar medium, galaxy clusters, accretion disks).\nThe key assumptions are that the turbulence is anisotropic with respect to the\nmean magnetic field and frequencies are low compared to the ion cyclotron\nfrequency. The energy injected at the outer scale scale has to be converted\ninto heat, which ultimately cannot be done without collisions. A KINETIC\nCASCADE develops that brings the energy to collisional scales both in space and\nvelocity. Its nature depends on the physics of plasma fluctuations. In each of\nthe physically distinct scale ranges, the kinetic problem is systematically\nreduced to a more tractable set of equations. In the \"inertial range\" above the\nion gyroscale, the kinetic cascade splits into a cascade of Alfvenic\nfluctuations, which are governed by the RMHD equations at both the collisional\nand collisionless scales, and a passive cascade of compressive fluctuations,\nwhich obey a linear kinetic equation along the moving field lines associated\nwith the Alfvenic component. In the \"dissipation range\" between the ion and\nelectron gyroscales, there are again two cascades: the kinetic-Alfven-wave\n(KAW) cascade governed by two fluid-like Electron RMHD equations and a passive\nphase-space cascade of ion entropy fluctuations. The latter cascade brings the\nenergy of the inertial-range fluctuations that was damped by collisionless\nwave-particle interaction at the ion gyroscale to collisional scales in the\nphase space and leads to ion heating. The KAW energy is similarly damped at the\nelectron gyroscale and converted into electron heat. Kolmogorov-style scaling\nrelations are derived for these cascades. Astrophysical and space-physical\napplications are discussed in detail.\n", "category": [3, 3, 3]}
{"abstract": "  This paper considers the propagation of shallow-water solitary and nonlinear\nperiodic waves over a gradual slope with bottom friction in the framework of a\nvariable-coefficient Korteweg-de Vries equation. We use the Whitham averaging\nmethod, using a recent development of this theory for perturbed integrable\nequations. This general approach enables us not only to improve known results\non the adiabatic evolution of isolated solitary waves and periodic wave trains\nin the presence of variable topography and bottom friction, modeled by the\nChezy law, but also importantly, to study the effects of these factors on the\npropagation of undular bores, which are essentially unsteady in the system\nunder consideration. In particular, it is shown that the combined action of\nvariable topography and bottom friction generally imposes certain global\nrestrictions on the undular bore propagation so that the evolution of the\nleading solitary wave can be substantially different from that of an isolated\nsolitary wave with the same initial amplitude. This non-local effect is due to\nnonlinear wave interactions within the undular bore and can lead to an\nadditional solitary wave amplitude growth, which cannot be predicted in the\nframework of the traditional adiabatic approach to the propagation of solitary\nwaves in slowly varying media.\n", "category": [3, 3]}
{"abstract": "  In a quantum mechanical model, Diosi, Feldmann and Kosloff arrived at a\nconjecture stating that the limit of the entropy of certain mixtures is the\nrelative entropy as system size goes to infinity. The conjecture is proven in\nthis paper for density matrices. The first proof is analytic and uses the\nquantum law of large numbers. The second one clarifies the relation to channel\ncapacity per unit cost for classical-quantum channels. Both proofs lead to\ngeneralization of the conjecture.\n", "category": [0, 2]}
{"abstract": "  The intelligent acoustic emission locator is described in Part I, while Part\nII discusses blind source separation, time delay estimation and location of two\nsimultaneously active continuous acoustic emission sources.\n  The location of acoustic emission on complicated aircraft frame structures is\na difficult problem of non-destructive testing. This article describes an\nintelligent acoustic emission source locator. The intelligent locator comprises\na sensor antenna and a general regression neural network, which solves the\nlocation problem based on learning from examples. Locator performance was\ntested on different test specimens. Tests have shown that the accuracy of\nlocation depends on sound velocity and attenuation in the specimen, the\ndimensions of the tested area, and the properties of stored data. The location\naccuracy achieved by the intelligent locator is comparable to that obtained by\nthe conventional triangulation method, while the applicability of the\nintelligent locator is more general since analysis of sonic ray paths is\navoided. This is a promising method for non-destructive testing of aircraft\nframe structures by the acoustic emission method.\n", "category": [0, 0]}
{"abstract": "  We report on the analysis of selected single source data sets from the first\nround of the Mock LISA Data Challenges (MLDC) for white dwarf binaries. We\nimplemented an end-to-end pipeline consisting of a grid-based coherent\npre-processing unit for signal detection, and an automatic Markov Chain Monte\nCarlo post-processing unit for signal evaluation. We demonstrate that signal\ndetection with our coherent approach is secure and accurate, and is increased\nin accuracy and supplemented with additional information on the signal\nparameters by our Markov Chain Monte Carlo approach. We also demonstrate that\nthe Markov Chain Monte Carlo routine is additionally able to determine\naccurately the noise level in the frequency window of interest.\n", "category": [3]}
{"abstract": "  We present an algorithm that produces the classification list of smooth Fano\nd-polytopes for any given d. The input of the algorithm is a single number,\nnamely the positive integer d. The algorithm has been used to classify smooth\nFano d-polytopes for d<=7. There are 7622 isomorphism classes of smooth Fano\n6-polytopes and 72256 isomorphism classes of smooth Fano 7-polytopes.\n", "category": [2]}
{"abstract": "  Part I describes an intelligent acoustic emission locator, while Part II\ndiscusses blind source separation, time delay estimation and location of two\ncontinuous acoustic emission sources.\n  Acoustic emission (AE) analysis is used for characterization and location of\ndeveloping defects in materials. AE sources often generate a mixture of various\nstatistically independent signals. A difficult problem of AE analysis is\nseparation and characterization of signal components when the signals from\nvarious sources and the mode of mixing are unknown. Recently, blind source\nseparation (BSS) by independent component analysis (ICA) has been used to solve\nthese problems. The purpose of this paper is to demonstrate the applicability\nof ICA to locate two independent simultaneously active acoustic emission\nsources on an aluminum band specimen. The method is promising for\nnon-destructive testing of aircraft frame structures by acoustic emission\nanalysis.\n", "category": [0, 0]}
{"abstract": "  A novel way of picturing the processing of quantum information is described,\nallowing a direct visualization of teleportation of quantum states and\nproviding a simple and intuitive understanding of this fascinating phenomenon.\nThe discussion is aimed at providing physicists a method of explaining\nteleportation to non-scientists. The basic ideas of quantum physics are first\nexplained in lay terms, after which these ideas are used with a graphical\ndescription, out of which teleportation arises naturally.\n", "category": [3]}
{"abstract": "  We study space-time symmetries in scalar quantum field theory (including\ninteracting theories) on static space-times. We first consider Euclidean\nquantum field theory on a static Riemannian manifold, and show that the\nisometry group is generated by one-parameter subgroups which have either\nself-adjoint or unitary quantizations. We analytically continue the\nself-adjoint semigroups to one-parameter unitary groups, and thus construct a\nunitary representation of the isometry group of the associated Lorentzian\nmanifold. The method is illustrated for the example of hyperbolic space, whose\nLorentzian continuation is Anti-de Sitter space.\n", "category": [3]}
{"abstract": "  The aim of the present paper is to provide a global presentation of the\ntheory of special Finsler manifolds. We introduce and investigate globally (or\nintrinsically, free from local coordinates) many of the most important and most\ncommonly used special Finsler manifolds: locally Minkowskian, Berwald,\nLandesberg, general Landesberg, $P$-reducible, $C$-reducible,\nsemi-$C$-reducible, quasi-$C$-reducible, $P^{*}$-Finsler, $C^{h}$-recurrent,\n$C^{v}$-recurrent, $C^{0}$-recurrent, $S^{v}$-recurrent, $S^{v}$-recurrent of\nthe second order, $C_{2}$-like, $S_{3}$-like, $S_{4}$-like, $P_{2}$-like,\n$R_{3}$-like, $P$-symmetric, $h$-isotropic, of scalar curvature, of constant\ncurvature, of $p$-scalar curvature, of $s$-$ps$-curvature. The global\ndefinitions of these special Finsler manifolds are introduced. Various\nrelationships between the different types of the considered special Finsler\nmanifolds are found. Many local results, known in the literature, are proved\nglobally and several new results are obtained. As a by-product, interesting\nidentities and properties concerning the torsion tensor fields and the\ncurvature tensor fields are deduced. Although our investigation is entirely\nglobal, we provide; for comparison reasons, an appendix presenting a local\ncounterpart of our global approach and the local definitions of the special\nFinsler spaces considered.\n", "category": [2, 3]}
{"abstract": "  In this paper we consider the Hardy-Lorentz spaces $H^{p,q}(R^n)$, with\n$0<p\\le 1$, $0<q\\le \\infty$. We discuss the atomic decomposition of the\nelements in these spaces, their interpolation properties, and the behavior of\nsingular integrals and other operators acting on them.\n", "category": [2, 2]}
{"abstract": "  Potassium intercalation in graphite is investigated by first-principles\ntheory. The bonding in the potassium-graphite compound is reasonably well\naccounted for by traditional semilocal density functional theory (DFT)\ncalculations. However, to investigate the intercalate formation energy from\npure potassium atoms and graphite requires use of a description of the graphite\ninterlayer binding and thus a consistent account of the nonlocal dispersive\ninteractions. This is included seamlessly with ordinary DFT by a van der Waals\ndensity functional (vdW-DF) approach [Phys. Rev. Lett. 92, 246401 (2004)]. The\nuse of the vdW-DF is found to stabilize the graphite crystal, with crystal\nparameters in fair agreement with experiments. For graphite and\npotassium-intercalated graphite structural parameters such as binding\nseparation, layer binding energy, formation energy, and bulk modulus are\nreported. Also the adsorption and sub-surface potassium absorption energies are\nreported. The vdW-DF description, compared with the traditional semilocal\napproach, is found to weakly soften the elastic response.\n", "category": [3, 3]}
{"abstract": "  We study a simple model of a nematic liquid crystal made of parallel\nellipsoidal particles interacting via a repulsive Gaussian law. After\nidentifying the relevant solid phases of the system through a careful\nzero-temperature scrutiny of as many as eleven candidate crystal structures, we\ndetermine the melting temperature for various pressure values, also with the\nhelp of exact free energy calculations. Among the prominent features of this\nmodel are pressure-driven reentrant melting and the stabilization of a columnar\nphase for intermediate temperatures.\n", "category": [3, 3]}
{"abstract": "  We study the interplay of crystal field splitting and Hund coupling in a\ntwo-orbital model which captures the essential physics of systems with two\nelectrons or holes in the e_g shell. We use single site dynamical mean field\ntheory with a recently developed impurity solver which is able to access strong\ncouplings and low temperatures. The fillings of the orbitals and the location\nof phase boundaries are computed as a function of Coulomb repulsion, exchange\ncoupling and crystal field splitting. We find that the Hund coupling can drive\nthe system into a novel Mott insulating phase with vanishing orbital\nsusceptibility. Away from half-filling, the crystal field splitting can induce\nan orbital selective Mott state.\n", "category": [3]}
{"abstract": "  I shall present three arguments for the proposition that intelligent life is\nvery rare in the universe. First, I shall summarize the consensus opinion of\nthe founders of the Modern Synthesis (Simpson, Dobzhanski, and Mayr) that the\nevolution of intelligent life is exceedingly improbable. Second, I shall\ndevelop the Fermi Paradox: if they existed they'd be here. Third, I shall show\nthat if intelligent life were too common, it would use up all available\nresources and die out. But I shall show that the quantum mechanical principle\nof unitarity (actually a form of teleology!) requires intelligent life to\nsurvive to the end of time. Finally, I shall argue that, if the universe is\nindeed accelerating, then survival to the end of time requires that intelligent\nlife, though rare, to have evolved several times in the visible universe. I\nshall argue that the acceleration is a consequence of the excess of matter over\nantimatter in the universe. I shall suggest experiments to test these claims.\n", "category": [3]}
{"abstract": "  We investigate the Coulomb excitation of low-lying states of unstable nuclei\nin intermediate energy collisions ($E_{lab}\\sim10-500$ MeV/nucleon). It is\nshown that the cross sections for the $E1$ and $E2$ transitions are larger at\nlower energies, much less than 10 MeV/nucleon. Retardation effects and Coulomb\ndistortion are found to be both relevant for energies as low as 10 MeV/nucleon\nand as high as 500 MeV/nucleon. Implications for studies at radioactive beam\nfacilities are discussed.\n", "category": [3]}
{"abstract": "  Intersection bodies represent a remarkable class of geometric objects\nassociated with sections of star bodies and invoking\n  Radon transforms, generalized cosine transforms, and the relevant Fourier\nanalysis. The main focus of this article is interrelation between generalized\ncosine transforms of different kinds in the context of their application to\ninvestigation of a certain family of intersection bodies, which we call\n$\\lam$-intersection bodies. The latter include $k$-intersection bodies (in the\nsense of A. Koldobsky) and unit balls of finite-dimensional subspaces of\n$L_p$-spaces. In particular, we show that restrictions onto lower dimensional\nsubspaces of the spherical Radon transforms and the generalized cosine\ntransforms preserve their integral-geometric structure. We apply this result to\nthe study of sections of $\\lam$-intersection bodies. New characterizations of\nthis class of bodies are obtained and examples are given. We also review some\nknown facts and give them new proofs.\n", "category": [2]}
{"abstract": "  In this paper, we introduce the on-line Viterbi algorithm for decoding hidden\nMarkov models (HMMs) in much smaller than linear space. Our analysis on\ntwo-state HMMs suggests that the expected maximum memory used to decode\nsequence of length $n$ with $m$-state HMM can be as low as $\\Theta(m\\log n)$,\nwithout a significant slow-down compared to the classical Viterbi algorithm.\nClassical Viterbi algorithm requires $O(mn)$ space, which is impractical for\nanalysis of long DNA sequences (such as complete human genome chromosomes) and\nfor continuous data streams. We also experimentally demonstrate the performance\nof the on-line Viterbi algorithm on a simple HMM for gene finding on both\nsimulated and real DNA sequences.\n", "category": [0]}
{"abstract": "  Neutrinoless double beta decay is one of the most sensitive approaches in\nnon-accelerator particle physics to take us into a regime of physics beyond the\nstandard model. This article is a brief review of the experiments in search of\nneutrinoless double beta decay from 76Ge. Following a brief introduction of the\nprocess of double beta decay from 76Ge, the results of the very first\nexperiments IGEX and Heidelberg-Moscow which give indications of the existence\nof possible neutrinoless double beta decay mode has been reviewed. Then ongoing\nefforts to substantiate the early findings are presented and the Majorana\nexperiment as a future experimental approach which will allow a very detailed\nstudy of the neutrinoless decay mode is discussed.\n", "category": [3]}
{"abstract": "  We capture the off-shell as well as the on-shell nilpotent\nBecchi-Rouet-Stora-Tyutin (BRST) and anti-BRST symmetry invariance of the\nLagrangian densities of the four (3 + 1)-dimensional (4D) (non-)Abelian 1-form\ngauge theories within the framework of the superfield formalism. In particular,\nwe provide the geometrical interpretations for (i) the above nilpotent symmetry\ninvariance, and (ii) the above Lagrangian densities, in the language of the\nspecific quantities defined in the domain of the above superfield formalism.\nSome of the subtle points, connected with the 4D (non-)Abelian 1-form gauge\ntheories, are clarified within the framework of the above superfield formalism\nwhere the 4D ordinary gauge theories are considered on the (4, 2)-dimensional\nsupermanifold parametrized by the four spacetime coordinates x^\\mu (with \\mu =\n0, 1, 2, 3) and a pair of Grassmannian variables \\theta and \\bar\\theta. One of\nthe key results of our present investigation is a great deal of simplification\nin the geometrical understanding of the nilpotent (anti-)BRST symmetry\ninvariance.\n", "category": [3]}
{"abstract": "  We introduce a family of rings of symmetric functions depending on an\ninfinite sequence of parameters. A distinguished basis of such a ring is\ncomprised by analogues of the Schur functions. The corresponding structure\ncoefficients are polynomials in the parameters which we call the\nLittlewood-Richardson polynomials. We give a combinatorial rule for their\ncalculation by modifying an earlier result of B. Sagan and the author. The new\nrule provides a formula for these polynomials which is manifestly positive in\nthe sense of W. Graham. We apply this formula for the calculation of the\nproduct of equivariant Schubert classes on Grassmannians which implies a\nstability property of the structure coefficients. The first manifestly positive\nformula for such an expansion was given by A. Knutson and T. Tao by using\ncombinatorics of puzzles while the stability property was not apparent from\nthat formula. We also use the Littlewood-Richardson polynomials to describe the\nmultiplication rule in the algebra of the Casimir elements for the general\nlinear Lie algebra in the basis of the quantum immanants constructed by A.\nOkounkov and G. Olshanski.\n", "category": [2, 2]}
{"abstract": "  Possible (algebraic) commutation relations in the Lagrangian quantum theory\nof free (scalar, spinor and vector) fields are considered from mathematical\nview-point. As sources of these relations are employed the Heisenberg\nequations/relations for the dynamical variables and a specific condition for\nuniqueness of the operators of the dynamical variables (with respect to some\nclass of Lagrangians). The paracommutation relations or some their\ngeneralizations are pointed as the most general ones that entail the validity\nof all Heisenberg equations. The simultaneous fulfillment of the Heisenberg\nequations and the uniqueness requirement turn to be impossible. This problem is\nsolved via a redefinition of the dynamical variables, similar to the normal\nordering procedure and containing it as a special case. That implies\ncorresponding changes in the admissible commutation relations. The introduction\nof the concept of the vacuum makes narrow the class of the possible commutation\nrelations; in particular, the mentioned redefinition of the dynamical variables\nis reduced to normal ordering. As a last restriction on that class is imposed\nthe requirement for existing of an effective procedure for calculating vacuum\nmean values. The standard bilinear commutation relations are pointed as the\nonly known ones that satisfy all of the mentioned conditions and do not\ncontradict to the existing data.\n", "category": [3]}
{"abstract": "  Epitaxial self-assembled quantum dots (SAQDs) are of interest for\nnanostructured optoelectronic and electronic devices such as lasers,\nphotodetectors and nanoscale logic. Spatial order and size order of SAQDs are\nimportant to the development of usable devices. It is likely that these two\ntypes of order are strongly linked; thus, a study of spatial order will also\nhave strong implications for size order. Here a study of spatial order is\nundertaken using a linear analysis of a commonly used model of SAQD formation\nbased on surface diffusion. Analytic formulas for film-height correlation\nfunctions are found that characterize quantum dot spatial order and\ncorresponding correlation lengths that quantify order. Initial atomic-scale\nrandom fluctuations result in relatively small correlation lengths (about two\ndots) when the effect of a wetting potential is negligible; however, the\ncorrelation lengths diverge when SAQDs are allowed to form at a near-critical\nfilm height. The present work reinforces previous findings about anisotropy and\nSAQD order and presents as explicit and transparent mechanism for ordering with\ncorresponding analytic equations. In addition, SAQD formation is by its nature\na stochastic process, and various mathematical aspects regarding statistical\nanalysis of SAQD formation and order are presented.\n", "category": [3]}
{"abstract": "  In the article [Petojevic 2006], A. Petojevi\\' c verified useful properties\nof the $K_{i}(z)$ functions which generalize Kurepa's [Kurepa 1971] left\nfactorial function. In this note, we present simplified proofs of two of these\nresults and we answer the open question stated in [Petojevic 2006]. Finally, we\ndiscuss the differential transcendency of the $K_{i}(z)$ functions.\n", "category": [2, 2]}
{"abstract": "  The goal of this paper is to construct invariant dynamical objects for a (not\nnecessarily invertible) smooth self map of a compact manifold. We prove a\nresult that takes advantage of differences in rates of expansion in the terms\nof a sheaf cohomological long exact sequence to create unique lifts of finite\ndimensional invariant subspaces of one term of the sequence to invariant\nsubspaces of the preceding term. This allows us to take invariant cohomological\nclasses and under the right circumstances construct unique currents of a given\ntype, including unique measures of a given type, that represent those classes\nand are invariant under pullback. A dynamically interesting self map may have a\nplethora of invariant measures, so the uniquess of the constructed currents is\nimportant. It means that if local growth is not too big compared to the growth\nrate of the cohomological class then the expanding cohomological class gives\nsufficient \"marching orders\" to the system to prohibit the formation of any\nother such invariant current of the same type (say from some local dynamical\nsubsystem). Because we use subsheaves of the sheaf of currents we give\nconditions under which a subsheaf will have the same cohomology as the sheaf\ncontaining it. Using a smoothing argument this allows us to show that the sheaf\ncohomology of the currents under consideration can be canonically identified\nwith the deRham cohomology groups. Our main theorem can be applied in both the\nsmooth and holomorphic setting.\n", "category": [2]}
{"abstract": "  The fractional Aharonov-Bohm oscillation (FABO) of narrow quantum rings with\ntwo electrons has been studied and has been explained in an analytical way, the\nevolution of the period and amplitudes against the magnetic field can be\nexactly described. Furthermore, the dipole transition of the ground state was\nfound to have essentially two frequencies, their difference appears as an\noscillation matching the oscillation of the persistent current exactly. A\nnumber of equalities relating the observables and dynamical parameters have\nbeen found.\n", "category": [3]}
{"abstract": "  No abstract given; compares pairs of languages from World Atlas of Language\nStructures.\n", "category": [3]}
{"abstract": "  In present paper we propose seemingly new method for finding solutions of\nsome types of nonlinear PDEs in closed form. The method is based on\ndecomposition of nonlinear operators on sequence of operators of lower orders.\nIt is shown that decomposition process can be done by iterative procedure(s),\neach step of which is reduced to solution of some auxiliary PDEs system(s) for\none dependent variable. Moreover, we find on this way the explicit expression\nof the first-order PDE(s) for first integral of decomposable initial PDE.\nRemarkably that this first-order PDE is linear if initial PDE is linear in its\nhighest derivatives.\n  The developed method is implemented in Maple procedure, which can really\nsolve many of different order PDEs with different number of independent\nvariables. Examples of PDEs with calculated their general solutions demonstrate\na potential of the method for automatic solving of nonlinear PDEs.\n", "category": [2]}
{"abstract": "  We treat Koll\\'ar's injectivity theorem from the analytic (or differential\ngeometric) viewpoint. More precisely, we give a curvature condition which\nimplies Koll\\'ar type cohomology injectivity theorems. Our main theorem is\nformulated for a compact K\\\"ahler manifold, but the proof uses the space of\nharmonic forms on a Zariski open set with a suitable complete K\\\"ahler metric.\nWe need neither covering tricks, desingularizations, nor Leray's spectral\nsequence.\n", "category": [2]}
{"abstract": "  This paper is an exposition of the so-called injective Morita contexts (in\nwhich the connecting bimodule morphisms are injective) and Morita\n$\\alpha$contexts (in which the connecting bimodules enjoy some local\nprojectivity in the sense of Zimmermann-Huisgen). Motivated by situations in\nwhich only one trace ideal is in action, or the compatibility between the\nbimodule morphisms is not needed, we introduce the notions of Morita\nsemi-contexts and Morita data, and investigate them. Injective Morita data will\nbe used (with the help of static and adstatic modules) to establish\nequivalences between some intersecting subcategories related to subcategories\nof modules that are localized or colocalized by trace ideals of a Morita datum.\nWe end up with applications of Morita $\\alpha$-contexts to $\\ast$-modules and\ninjective right wide Morita contexts.\n", "category": [2]}
{"abstract": "  There has been important experimental progress in the sector of heavy baryons\nin the past several years. We study the strong decays of the S-wave, P-wave,\nD-wave and radially excited charmed baryons using the $^3P_0$ model. After\ncomparing the calculated decay pattern and total width with the available data,\nwe discuss the possible internal structure and quantum numbers of those charmed\nbaryons observed recently.\n", "category": [3, 3, 3]}
{"abstract": "  Precision tests of the Kobayashi-Maskawa model of CP violation are discussed,\npointing out possible signatures for other sources of CP violation and for new\nflavor-changing operators. The current status of the most accurate tests is\nsummarized.\n", "category": [3, 3]}
{"abstract": "  The Dark Energy problem is forcing us to re-examine our models and our\nunderstanding of relativity and space-time. Here a novel idea of Fundamental\nForces is introduced. This allows us to perceive the General Theory of\nRelativity and Einstein's Equation from a new pesrpective. In addition to\nproviding us with an improved understanding of space and time, it will be shown\nhow it leads to a resolution of the Dark Energy problem.\n", "category": [3]}
{"abstract": "  We present a critical review about the study of linear perturbations of\nmatched spacetimes including gauge problems. We analyse the freedom introduced\nin the perturbed matching by the presence of background symmetries and revisit\nthe particular case of spherically symmetry in n-dimensions. This analysis\nincludes settings with boundary layers such as brane world models and shell\ncosmologies.\n", "category": [3]}
{"abstract": "  We define nonselfadjoint operator algebras with generators $L_{e_1},...,\nL_{e_n}, L_{f_1},...,L_{f_m}$ subject to the unitary commutation relations of\nthe form \\[ L_{e_i}L_{f_j} = \\sum_{k,l} u_{i,j,k,l} L_{f_l}L_{e_k}\\] where $u=\n(u_{i,j,k,l})$ is an $nm \\times nm$ unitary matrix. These algebras, which\ngeneralise the analytic Toeplitz algebras of rank 2 graphs with a single\nvertex, are classified up to isometric isomorphism in terms of the matrix $u$.\n", "category": [2]}
{"abstract": "  We discussed quantum deformations of D=4 Lorentz and Poincare algebras. In\nthe case of Poincare algebra it is shown that almost all classical r-matrices\nof S. Zakrzewski classification correspond to twisted deformations of Abelian\nand Jordanian types. A part of twists corresponding to the r-matrices of\nZakrzewski classification are given in explicit form.\n", "category": [2, 3, 2, 2]}
{"abstract": "  We investigate dynamical properties of bright solitons with a finite\nbackground in the F=1 spinor Bose-Einstein condensate (BEC), based on an\nintegrable spinor model which is equivalent to the matrix nonlinear\nSchr\\\"{o}dinger equation with a self-focusing nonlineality. We apply the\ninverse scattering method formulated for nonvanishing boundary conditions. The\nresulting soliton solutions can be regarded as a generalization of those under\nvanishing boundary conditions. One-soliton solutions are derived in an explicit\nmanner. According to the behaviors at the infinity, they are classified into\ntwo kinds, domain-wall (DW) type and phase-shift (PS) type. The DW-type implies\nthe ferromagnetic state with nonzero total spin and the PS-type implies the\npolar state, where the total spin amounts to zero. We also discuss two-soliton\ncollisions. In particular, the spin-mixing phenomenon is confirmed in a\ncollision involving the DW-type. The results are consistent with those of the\nprevious studies for bright solitons under vanishing boundary conditions and\ndark solitons. As a result, we establish the robustness and the usefulness of\nthe multiple matter-wave solitons in the spinor BECs.\n", "category": [3, 3]}
{"abstract": "  The path integral over Euclidean geometries for the recently suggested\ndensity matrix of the Universe is shown to describe a microcanonical ensemble\nin quantum cosmology. This ensemble corresponds to a uniform (weight one)\ndistribution in phase space of true physical variables, but in terms of the\nobservable spacetime geometry it is peaked about complex saddle-points of the\n{\\em Lorentzian} path integral. They are represented by the recently obtained\ncosmological instantons limited to a bounded range of the cosmological\nconstant. Inflationary cosmologies generated by these instantons at late stages\nof expansion undergo acceleration whose low-energy scale can be attained within\nthe concept of dynamically evolving extra dimensions. Thus, together with the\nbounded range of the early cosmological constant, this cosmological ensemble\nsuggests the mechanism of constraining the landscape of string vacua and,\nsimultaneously, a possible solution to the dark energy problem in the form of\nthe quasi-equilibrium decay of the microcanonical state of the Universe.\n", "category": [3]}
{"abstract": "  We employ granular hydrodynamics to investigate a paradigmatic problem of\nclustering of particles in a freely cooling dilute granular gas. We consider\nlarge-scale hydrodynamic motions where the viscosity and heat conduction can be\nneglected, and one arrives at the equations of ideal gas dynamics with an\nadditional term describing bulk energy losses due to inelastic collisions. We\nemploy Lagrangian coordinates and derive a broad family of exact non-stationary\nanalytical solutions that depend only on one spatial coordinate. These\nsolutions exhibit a new type of singularity, where the gas density blows up in\na finite time when starting from smooth initial conditions. The density blowups\nsignal formation of close-packed clusters of particles. As the density blow-up\ntime $t_c$ is approached, the maximum density exhibits a power law $\\sim\n(t_c-t)^{-2}$. The velocity gradient blows up as $\\sim - (t_c-t)^{-1}$ while\nthe velocity itself remains continuous and develops a cusp (rather than a shock\ndiscontinuity) at the singularity. The gas temperature vanishes at the\nsingularity, and the singularity follows the isobaric scenario: the gas\npressure remains finite and approximately uniform in space and constant in time\nclose to the singularity. An additional exact solution shows that the density\nblowup, of the same type, may coexist with an \"ordinary\" shock, at which the\nhydrodynamic fields are discontinuous but finite. We confirm stability of the\nexact solutions with respect to small one-dimensional perturbations by solving\nthe ideal hydrodynamic equations numerically. Furthermore, numerical solutions\nshow that the local features of the density blowup hold universally,\nindependently of details of the initial and boundary conditions.\n", "category": [3, 3, 3]}
{"abstract": "  We discuss a universality property of any covariant field theory in\nspace-time expanded around pp-wave backgrounds. According to this property the\nspace-time lagrangian density evaluated on a restricted set of field\nconfigurations, called universal sector, turns out to be same around all the\npp-waves, even off-shell, with same transverse space and same profiles for the\nbackground scalars. In this paper we restrict our discussion to tensorial\nfields only. In the context of bosonic string theory we consider on-shell\npp-waves and argue that universality requires the existence of a universal\nsector of world-sheet operators whose correlation functions are insensitive to\nthe pp-wave nature of the metric and the background gauge flux. Such results\ncan also be reproduced using the world-sheet conformal field theory. We also\nstudy such pp-waves in non-polynomial closed string field theory (CSFT). In\nparticular, we argue that for an off-shell pp-wave ansatz with flat transverse\nspace and dilaton independent of transverse coordinates the field redefinition\nrelating the low energy effective field theory and CSFT with all the massive\nmodes integrated out is at most quadratic in fields. Because of this\nsimplification it is expected that the off-shell pp-waves can be identified on\nthe two sides. Furthermore, given the massless pp-wave field configurations, an\niterative method for computing the higher massive modes using the CSFT\nequations of motion has been discussed. All our bosonic string theory analyses\ncan be generalised to the common Neveu-Schwarz sector of superstrings.\n", "category": [3]}
{"abstract": "  We give a quantitative analysis of clustering in a stochastic model of\none-dimensional gas. At time zero, the gas consists of $n$ identical particles\nthat are randomly distributed on the real line and have zero initial speeds.\nParticles begin to move under the forces of mutual attraction. When particles\ncollide, they stick together forming a new particle, called cluster, whose mass\nand speed are defined by the laws of conservation. We are interested in the\nasymptotic behavior of $K_n(t)$ as $n\\to \\infty$, where $K_n(t)$ denotes the\nnumber of clusters at time $t$ in the system with $n$ initial particles. Our\nmain result is a functional limit theorem for $K_n(t)$. Its proof is based on\nthe discovered localization property of the aggregation process, which states\nthat the behavior of each particle is essentially defined by the motion of\nneighbor particles.\n", "category": [2]}
{"abstract": "  Our main result in this paper is the following: Given $H^m, H^n$ hyperbolic\nspaces of dimensional $m$ and $n$ corresponding, and given a Holder function\n$f=(s^1,...,f^{n-1}):\\partial H^m\\to \\partial H^n$ between geometric boundaries\nof $H^m$ and $H^n$. Then for each $\\epsilon >0$ there exists a harmonic map\n$u:H^m\\to H^n$ which is continuous up to the boundary (in the sense of\nEuclidean) and $u|_{\\partial H^m}=(f^1,...,f^{n-1},\\epsilon)$.\n", "category": [2]}
{"abstract": "  The results of the spectral, energetical and temporal characteristics of\nradiation in the presence of the photonic flame effect are presented.\nArtificial opal posed on Cu plate at the temperature of liquid nitrogen boiling\npoint (77 K) being irradiated by nanosecond ruby laser pulse produces long-\nterm luminiscence with a duration till ten seconds with a finely structured\nspectrum in the the antistocks part of the spectrum. Analogous visible\nluminescence manifesting time delay appeared in other samples of the artificial\nopals posed on the same plate. In the case of the opal infiltrated with\ndifferent nonlinear liquids the threshold of the luminiscence is reduced and\nthe spatial disribution of the bright emmiting area on the opal surface is\nbeing changed. In the case of the putting the frozen nonlinear liquids on the\nCu plate long-term blue bright luminiscence took place in the frozen species of\nthe liquids. Temporal characteristics of this luminiscence are nearly the same\nas in opal matrixes.\n", "category": [3]}
{"abstract": "  Statistical modeling of experimental physical laws is based on the\nprobability density function of measured variables. It is expressed by\nexperimental data via a kernel estimator. The kernel is determined objectively\nby the scattering of data during calibration of experimental setup. A physical\nlaw, which relates measured variables, is optimally extracted from experimental\ndata by the conditional average estimator. It is derived directly from the\nkernel estimator and corresponds to a general nonparametric regression. The\nproposed method is demonstrated by the modeling of a return map of noisy\nchaotic data. In this example, the nonparametric regression is used to predict\na future value of chaotic time series from the present one. The mean predictor\nerror is used in the definition of predictor quality, while the redundancy is\nexpressed by the mean square distance between data points. Both statistics are\nused in a new definition of predictor cost function. From the minimum of the\npredictor cost function, a proper number of data in the model is estimated.\n", "category": [3, 3]}
{"abstract": "  Real Options for Project Schedules (ROPS) has three recursive\nsampling/optimization shells. An outer Adaptive Simulated Annealing (ASA)\noptimization shell optimizes parameters of strategic Plans containing multiple\nProjects containing ordered Tasks. A middle shell samples probability\ndistributions of durations of Tasks. An inner shell samples probability\ndistributions of costs of Tasks. PATHTREE is used to develop options on\nschedules.. Algorithms used for Trading in Risk Dimensions (TRD) are applied to\ndevelop a relative risk analysis among projects.\n", "category": [0, 3, 0, 0, 3]}
{"abstract": "  We combine classical methods of combinatorial group theory with the theory of\nsmall cancellations over relatively hyperbolic groups to construct finitely\ngenerated torsion-free groups that have only finitely many classes of conjugate\nelements. Moreover, we present several results concerning embeddings into such\ngroups.\n  As another application of these techniques, we prove that every countable\ngroup $C$ can be realized as a group of outer automorphisms of a group $N$,\nwhere $N$ is a finitely generated group having Kazhdan's property (T) and\ncontaining exactly two conjugacy classes.\n", "category": [2]}
{"abstract": "  We study a recently proposed formulation of overlap fermions at finite\ndensity. In particular we compute the energy density as a function of the\nchemical potential and the temperature. It is shown that overlap fermions with\nchemical potential reproduce the correct continuum behavior.\n", "category": [3, 3]}
{"abstract": "  Lattice contribution to the electronic self-energy in complex correlated\noxides is a fascinating subject that has lately stimulated lively discussions.\nExpectations of electron-phonon self-energy effects for simpler materials, such\nas Pd and Al, have resulted in several misconceptions in strongly correlated\noxides. Here we analyze a number of arguments claiming that phonons cannot be\nthe origin of certain self-energy effects seen in high-$T_c$ cuprate\nsuperconductors via angle resolved photoemission experiments (ARPES), including\nthe temperature dependence, doping dependence of the renormalization effects,\nthe inter-band scattering in the bilayer systems, and impurity substitution. We\nshow that in light of experimental evidences and detailed simulations, these\narguments are not well founded.\n", "category": [3, 3]}
{"abstract": "  We get asymptotics for the volume of large balls in an arbitrary locally\ncompact group G with polynomial growth. This is done via a study of the\ngeometry of G and a generalization of P. Pansu's thesis. In particular, we show\nthat any such G is weakly commensurable to some simply connected solvable Lie\ngroup S, the Lie shadow of G. We also show that large balls in G have an\nasymptotic shape, i.e. after a suitable renormalization, they converge to a\nlimiting compact set which can be interpreted geometrically. We then discuss\nthe speed of convergence, treat some examples and give an application to\nergodic theory. We also answer a question of Burago about left invariant\nmetrics and recover some results of Stoll on the irrationality of growth series\nof nilpotent groups.\n", "category": [2, 2]}
{"abstract": "  In this note we present three representations of a 248-dimensional Lie\nalgebra, namely the algebra of Lie point symmetries admitted by a system of\nfive trivial ordinary differential equations each of order forty-four, that\nadmitted by a system of seven trivial ordinary differential equations each of\norder twenty-eight and that admitted by one trivial ordinary differential\nequation of order two hundred and forty-four.\n", "category": [3]}
{"abstract": "  We review recent progress in operator algebraic approach to conformal quantum\nfield theory. Our emphasis is on use of representation theory in classification\ntheory. This is based on a series of joint works with R. Longo.\n", "category": [2, 2]}
{"abstract": "  Sparse Code Division Multiple Access (CDMA), a variation on the standard CDMA\nmethod in which the spreading (signature) matrix contains only a relatively\nsmall number of non-zero elements, is presented and analysed using methods of\nstatistical physics. The analysis provides results on the performance of\nmaximum likelihood decoding for sparse spreading codes in the large system\nlimit. We present results for both cases of regular and irregular spreading\nmatrices for the binary additive white Gaussian noise channel (BIAWGN) with a\ncomparison to the canonical (dense) random spreading code.\n", "category": [0, 2]}
{"abstract": "  For positive semidefinite matrices $A$ and $B$, Ando and Zhan proved the\ninequalities $||| f(A)+f(B) ||| \\ge ||| f(A+B) |||$ and $||| g(A)+g(B) ||| \\le\n||| g(A+B) |||$, for any unitarily invariant norm, and for any non-negative\noperator monotone $f$ on $[0,\\infty)$ with inverse function $g$. These\ninequalities have very recently been generalised to non-negative concave\nfunctions $f$ and non-negative convex functions $g$, by Bourin and Uchiyama,\nand Kosem, respectively.\n  In this paper we consider the related question whether the inequalities $|||\nf(A)-f(B) ||| \\le ||| f(|A-B|) |||$, and $||| g(A)-g(B) ||| \\ge ||| g(|A-B|)\n|||$, obtained by Ando, for operator monotone $f$ with inverse $g$, also have a\nsimilar generalisation to non-negative concave $f$ and convex $g$. We answer\nexactly this question, in the negative for general matrices, and affirmatively\nin the special case when $A\\ge ||B||$.\n  In the course of this work, we introduce the novel notion of $Y$-dominated\nmajorisation between the spectra of two Hermitian matrices, where $Y$ is itself\na Hermitian matrix, and prove a certain property of this relation that allows\nto strengthen the results of Bourin-Uchiyama and Kosem, mentioned above.\n", "category": [2]}
{"abstract": "  The topological structure of the event horizon has been investigated in terms\nof the Morse theory. The elementary process of topological evolution can be\nunderstood as a handle attachment. It has been found that there are certain\nconstraints on the nature of black hole topological evolution: (i) There are n\nkinds of handle attachments in (n+1)-dimensional black hole space-times. (ii)\nHandles are further classified as either of black or white type, and only black\nhandles appear in real black hole space-times. (iii) The spatial section of an\nexterior of the black hole region is always connected. As a corollary, it is\nshown that the formation of a black hole with an S**(n-2) x S**1 horizon from\nthat with an S**(n-1) horizon must be non-axisymmetric in asymptotically flat\nspace-times.\n", "category": [3]}
{"abstract": "  In this contribution we go through the developments that in the years 1968 to\n1974 led from the Veneziano model to the bosonic string.\n", "category": [3]}
{"abstract": "  We prove a duality theorem for certain graded algebras and show by various\nexamples different kinds of failure of tameness of local cohomology.\n", "category": [2, 2]}
{"abstract": "  The physical consistency of the match of piecewise-$C^0$ metrics is\ndiscussed. The mathematical theory of gravitational discontinuity hypersurfaces\nis generalized to cover the match of regularly discontinuous metrics. The\nmean-value differential geometry framework on a hypersurface is introduced, and\ncorresponding compatibility conditions are deduced. Examples of generalized\nboundary layers, gravitational shock waves and thin shells are studied.\n", "category": [3]}
{"abstract": "  Given an orientable weakly self-dual manifold X of rank two, we build a\ngeometric realization of the Lie algebra sl(6,C) as a naturally defined algebra\nL of endomorphisms of the space of differential forms of X. We provide an\nexplicit description of Serre generators in terms of natural generators of L.\nThis construction gives a bundle on X which is related to the search for a\nnatural Gauge theory on X. We consider this paper as a first step in the study\nof a rich and interesting algebraic structure.\n", "category": [2, 2]}
{"abstract": "  We show that there is an hierarchy of intersection rigidity properties of\nsets in a closed symplectic manifold: some sets cannot be displaced by\nsymplectomorphisms from more sets than the others. We also find new examples of\nrigidity of intersections involving, in particular, specific fibers of moment\nmaps of Hamiltonian torus actions, monotone Lagrangian submanifolds (following\nthe works of P.Albers and P.Biran-O.Cornea), as well as certain, possibly\nsingular, sets defined in terms of Poisson-commutative subalgebras of smooth\nfunctions. In addition, we get some geometric obstructions to semi-simplicity\nof the quantum homology of symplectic manifolds. The proofs are based on the\nFloer-theoretical machinery of partial symplectic quasi-states.\n", "category": [2]}
{"abstract": "  Modifications to quark and antiquark fragmentation functions due to\nquark-quark (antiquark) double scattering in nuclear medium are studied\nsystematically up to order \\cal{O}(\\alpha_{s}^2)$ in deeply inelastic\nscattering (DIS) off nuclear targets. At the order $\\cal{O}(\\alpha_s^2)$,\ntwist-four contributions from quark-quark (antiquark) rescattering also exhibit\nthe Landau-Pomeranchuck-Midgal (LPM) interference feature similar to gluon\nbremsstrahlung induced by multiple parton scattering. Compared to quark-gluon\nscattering, the modification, which is dominated by $t$-channel quark-quark\n(antiquark) scattering, is only smaller by a factor of $C_F/C_A=4/9$ times the\nratio of quark and gluon distributions in the medium. Such a modification is\nnot negligible for realistic kinematics and finite medium size. The\nmodifications to quark (antiquark) fragmentation functions from quark-antiquark\nannihilation processes are shown to be determined by the antiquark (quark)\ndistribution density in the medium. The asymmetry in quark and antiquark\ndistributions in nuclei will lead to different modifications of quark and\nantiquark fragmentation functions inside a nucleus, which qualitatively\nexplains the experimentally observed flavor dependence of the leading hadron\nsuppression in semi-inclusive DIS off nuclear targets. The quark-antiquark\nannihilation processes also mix quark and gluon fragmentation functions in the\nlarge fractional momentum region, leading to a flavor dependence of jet\nquenching in heavy-ion collisions.\n", "category": [3, 3]}
{"abstract": "  A physical law is represented by the probability distribution of a measured\nvariable. The probability density is described by measured data using an\nestimator whose kernel is the instrument scattering function. The experimental\ninformation and data redundancy are defined in terms of information entropy.\nThe model cost function, comprised of data redundancy and estimation error, is\nminimized by the creation-annihilation process.\n", "category": [3]}
{"abstract": "  Description of a polynomial time reduction of SAT to 2-SAT of polynomial\nsize.\n", "category": [0]}
{"abstract": "  From first-principles calculations, we predict that transition metal (TM)\natom doped silicon nanowires have a half-metallic ground state. They are\ninsulators for one spin-direction, but show metallic properties for the\nopposite spin direction. At high coverage of TM atoms, ferromagnetic silicon\nnanowires become metallic for both spin-directions with high magnetic moment\nand may have also significant spin-polarization at the Fermi level. The\nspin-dependent electronic properties can be engineered by changing the type of\ndopant TM atoms, as well as the diameter of the nanowire. Present results are\nnot only of scientific interest, but can also initiate new research on\nspintronic applications of silicon nanowires.\n", "category": [3, 3]}
{"abstract": "  We prove that an arbitrary (not necessarily countably generated) Hilbert\n$G$-$\\cla$ module on a G-C^* algebra $\\cla$ admits an equivariant embedding\ninto a trivial $G-\\cla$ module, provided G is a compact Lie group and its\naction on $\\cla$ is ergodic.\n", "category": [2]}
{"abstract": "  We give details of the proof of the remark made in \\cite{G2} that the Chern\ncharacters of the canonical generators on the K homology of the quantum group\n$SU_q(2)$ are not invariant under the natural $SU_q(2)$ coaction. Furthermore,\nthe conjecture made in \\cite{G2} about the nontriviality of the twisted Chern\ncharacter coming from an odd equivariant spectral triple on $SU_q(2)$ is\nsettled in the affirmative.\n", "category": [2, 2]}
{"abstract": "  Zero-divisors (ZDs) derived by Cayley-Dickson Process (CDP) from\nN-dimensional hypercomplex numbers (N a power of 2, at least 4) can represent\nsingularities and, as N approaches infinite, fractals -- and thereby,scale-free\nnetworks. Any integer greater than 8 and not a power of 2 generates a\nmeta-fractal or \"Sky\" when it is interpreted as the \"strut constant\" (S) of an\nensemble of octahedral vertex figures called \"Box-Kites\" (the fundamental\nbuilding blocks of ZDs). Remarkably simple bit-manipulation rules or \"recipes\"\nprovide tools for transforming one fractal genus into others within the context\nof Wolfram's Class 4 complexity.\n", "category": [2]}
{"abstract": "  Single walled carbon nanotubes exhibit advanced electrical and surface\nproperties useful for high performance nanoelectronics. Important to future\nmanufacturing of nanotube circuits is large scale assembly of SWNTs into\naligned forms. Despite progress in assembly and oriented synthesis, pristine\nSWNTs in aligned and close-packed form remain elusive and needed for high\ncurrent, speed and density devices through collective operations of parallel\nSWNTs. Here, we develop a Langmuir Blodgett method achieving monolayers of\naligned SWNTs with dense packing, central to which is a non covalent polymer\nfunctionalization by PmPV imparting high solubility and stability of SWNTs in\nan organic solvent DCE. Pressure cycling or annealing during LB film\ncompression reduces hysteresis and facilitates high degree alignment and\npacking of SWNTs characterized by microscopy and polarized Raman spectroscopy.\nThe monolayer SWNTs are readily patterned for device integration by\nmicrofabrication, enabling the highest currents 3mA through the narrowest\nregions packed with aligned SWNTs thus far.\n", "category": [3]}
{"abstract": "  We study the S=1/2 Heisenberg antiferromagnet on a square lattice with\nnearest-neighbor and plaquette four-spin exchanges (introduced by A.W. Sandvik,\nPhys. Rev. Lett. {\\bf 98}, 227202 (2007).)\n  This model undergoes a quantum phase transition from a spontaneously\ndimerized phase to N\\'eel order at a critical coupling. We show that as the\ncritical point is approached from the dimerized side, the system exhibits\nstrong fluctuations in the dimer background, reflected in the presence of a\nlow-energy singlet mode, with a simultaneous rise in the triplet quasiparticle\ndensity. We find that both singlet and triplet modes of high density condense\nat the transition, signaling restoration of lattice symmetry. In our approach,\nwhich goes beyond mean-field theory in terms of the triplet excitations, the\ntransition appears sharp; however since our method breaks down near the\ncritical point, we argue that we cannot make a definite conclusion regarding\nthe order of the transition.\n", "category": [3]}
{"abstract": "  We will prove the relative homotopy principle for smooth maps with\nsingularities of a given {\\cal K}-invariant class with a mild condition. We\nnext study a filtration of the group of homotopy self-equivalences of a given\nmanifold P by considering singularities of non-negative {\\cal K}-codimensions.\n", "category": [2]}
{"abstract": "  We consider the variation of the surface spanned by closed strings in a\nspacetime manifold. Using the Nambu-Goto string action, we induce the geodesic\nsurface equation, the geodesic surface deviation equation which yields a Jacobi\nfield, and we define the index form of a geodesic surface as in the case of\npoint particles to discuss conjugate strings on the geodesic surface.\n", "category": [3, 2]}
{"abstract": "  We report on the layer-by-layer growth of single-crystal Al2O3 thin-films on\nNb (110). Single-crystal Nb films are first prepared on A-plane sapphire,\nfollowed by the evaporation of Al in an O2 background. The first stages of\nAl2O3 growth are layer-by-layer with hexagonal symmetry. Electron and x-ray\ndiffraction measurements indicate the Al2O3 initially grows clamped to the Nb\nlattice with a tensile strain near 10%. This strain relaxes with further\ndeposition, and beyond about 5 nm we observe the onset of island growth.\nDespite the asymmetric misfit between the Al2O3 film and the Nb under-layer,\nthe observed strain is surprisingly isotropic.\n", "category": [3, 3]}
{"abstract": "  We have successfully grown the single crystals of CeAg$_2$Ge$_2$, for the\nfirst time, by flux method and studied the anisotropic physical properties by\nmeasuring the electrical resistivity, magnetic susceptibility and specific\nheat. We found that CeAg$_2$Ge$_2$ undergoes an antiferromagnetic transition at\n$T_{\\rm N}$ = 4.6 K. The electrical resistivity and susceptibility data reveal\nstrong anisotropic magnetic properties. The magnetization measured at $T$ = 2 K\nexhibited two metamagnetic transitions at $H_{\\rm m1}$ = 31 kOe and $H_{\\rm\nm2}$ = 44.7 kOe, for $H \\parallel$ [100] with a saturation magnetization of 1.6\n$\\mu_{\\rm B}$/Ce. The crystalline electric field (CEF) analysis of the inverse\nsusceptibility data reveals that the ground state and the first excited states\nof CeAg$_2$Ge$_2$ are closely spaced indicating a quasi-quartet ground state.\nThe specific heat data lend further support to the presence of closely spaced\nenergy levels.\n", "category": [3]}
{"abstract": "  Most recently, both BaBar and Belle experiments found evidences of neutral\n$D$ mixing. In this paper, we discuss the constraints on the strong phase\ndifference in $D^0 \\to K\\pi$ decay from the measurements of the mixing\nparameters, $y^\\prime$, $y_{CP}$ and $x$ at the $B$ factories. The sensitivity\nof the measurement of the mixing parameter $y$ is estimated in BES-III\nexperiment at $\\psi(3770)$ peak. We also make an estimate on the measurements\nof the mixing rate $R_M$. Finally, the sensitivity of the strong phase\ndifference at BES-III are obtained by using data near the $D\\bar{D}$ threshold\nwith CP tag technique at BES-III experiment.\n", "category": [3, 3]}
{"abstract": "  Starting from the N=1 SU(N_c) x SU(N_c') gauge theory with fundamental and\nbifundamental flavors, we apply the Seiberg dual to the first gauge group and\nobtain the N=1 dual gauge theory with dual matters including the gauge\nsinglets. By analyzing the F-term equations of the superpotential, we describe\nthe intersecting type IIA brane configuration for the meta-stable\nnonsupersymmetric vacua of this gauge theory. By introducing an orientifold\n6-plane, we generalize to the case for N=1 SU(N_c) x SO(N_c') gauge theory with\nfundamental and bifundamental flavors. Finally, the N=1 SU(N_c) x Sp(N_c')\ngauge theory with matters is also described very briefly.\n", "category": [3]}
{"abstract": "  Magnetic dipole-dipole interaction dominated Bose-Einstein condensates are\ndiscussed under spinful situations. We treat the spin degrees of freedom as a\nclassical spin vector, approaching from large spin limit to obtain an effective\nminimal Hamiltonian; a version extended from a non-linear sigma model. By\nsolving the Gross-Pitaevskii equation we find several novel spin textures where\nthe mass density and spin density are strongly coupled, depending upon trap\ngeometries due to the long-range and anisotropic natures of the dipole-dipole\ninteraction.\n", "category": [3]}
{"abstract": "  The microwave phonon stimulated emission (SE) has been experimentally and\nnumerically investigated in a nonautonomous microwave acoustic quantum\ngenerator, called also microwave phonon laser or phaser (see previous works\narXiv:cond-mat/0303188 ; arXiv:cond-mat/0402640 ; arXiv:nlin.CG/0703050)\nPhenomena of branching and long-time refractority (absence of the reaction on\nthe external pulses) for deterministic chaotic and regular processes of SE were\nobserved in experiments with various levels of electromagnetic pumping. At the\npumping level growth, the clearly depined increasing of the number of\ncoexisting SE states has been observed both in real physical experiments and in\ncomputer simulations. This confirms the analytical estimations of the branching\ndensity in the phase space. The nature of the refractority of SE pulses is\nclosely connected with the pointed branching and reflects the crises of strange\nattractors, i.e. their collisions with unstable periodic components of the\nhigher branches of SE states in the nonautonomous microwave phonon laser.\n", "category": [3, 3, 3]}
{"abstract": "  We prove the existence of global Bishop discs in a strictly pseudoconvex\nStein domain in an almost complex manifold of complex dimension 2.\n", "category": [2]}
{"abstract": "  In this note we develop tools and techniques for the treatment of anisotropic\nthermo-elasticity in two space dimensions. We use a diagonalisation technique\nto obtain properties of the characteristic roots of the full symbol of the\nsystem in order to prove $L^p$--$L^q$ decay rates for its solutions.\n", "category": [2]}
{"abstract": "  The current-voltage (I-V) characteristics of various MgB2 films have been\nstudied at different magnetic fields parallel to c-axis. At fields \\mu0H\nbetween 0 and 5T, vortex liquid-glass transitions were found in the I-V\nisotherms. Consistently, the I-V curves measured at different temperatures show\na scaling behavior in the framework of quasi-two-dimension (quasi-2D) vortex\nglass theory. However, at \\mu0 H >= 5T, a finite dissipation was observed down\nto the lowest temperature here, T=1.7K, and the I-V isotherms did not scale in\nterms of any known scaling law, of any dimensionality. We suggest that this may\nbe caused by a mixture of \\sigma band vortices and \\pi band quasiparticles.\nInterestingly, the I-V curves at zero magnetic field can still be scaled\naccording to the quasi-2D vortex glass formalism, indicating an equivalent\neffect of self-field due to persistent current and applied magnetic field.\n", "category": [3]}
{"abstract": "  Sub-100 nm nanomagnets not only are technologically important, but also\nexhibit complex magnetization reversal behaviors as their dimensions are\ncomparable to typical magnetic domain wall widths. Here we capture magnetic\n\"fingerprints\" of 1 billion Fe nanodots as they undergo a single domain to\nvortex state transition, using a first-order reversal curve (FORC) method. As\nthe nanodot size increases from 52 nm to 67 nm, the FORC diagrams reveal\nstriking differences, despite only subtle changes in their major hysteresis\nloops. The 52 nm nanodots exhibit single domain behavior and the coercivity\ndistribution extracted from the FORC distribution agrees well with a\ncalculation based on the measured nanodot size distribution. The 58 and 67 nm\nnanodots exhibit vortex states, where the nucleation and annihilation of the\nvortices are manifested as butterfly-like features in the FORC distribution and\nconfirmed by micromagnetic simulations. Furthermore, the FORC method gives\nquantitative measures of the magnetic phase fractions, and vortex nucleation\nand annihilation fields.\n", "category": [3]}
{"abstract": "  In this paper we show that the quotient Aubry set associated to certain\nLagrangians is totally disconnected (i.e., every connected component consists\nof a single point). Moreover, we discuss the relation between this problem and\na Morse-Sard type property for (difference of) critical subsolutions of\nHamilton-Jacobi equations.\n", "category": [2, 2, 2]}
{"abstract": "  Over algebraically closed fields of characteristic p>2, prolongations of the\nsimple finite dimensional Lie algebras and Lie superalgebras with Cartan matrix\nare studied for certain simplest gradings of these algebras. Several new simple\nLie superalgebras are discovered, serial and exceptional, including superBrown\nand superMelikyan superalgebras. Simple Lie superalgebras with Cartan matrix of\nrank 2 are classified.\n", "category": [2]}
{"abstract": "  Based on overall experimental observations, especially the pair processes, I\ndeveloped a model structure of the vacuum along with a basic-particle formation\nscheme begun in 2000 (with collaborator P-I Johansson). The model consists in\nthat the vacuum is, briefly, filled of neutral but polarizable vacuuons,\nconsisting each of a p-vaculeon and n- vaculeon of charges $+e$ and $-e$ of\nzero rest masses but with spin motions, assumed interacting each other with a\nCoulomb force. The model has been introduced in full in a book (Nova Sci, 2005)\nand referred to in a number of journal/E-print papers. I outline in this easier\naccessible paper the detailed derivation of the model and a corresponding\nquantitative determination of the vacuuon size.\n", "category": [3]}
{"abstract": "  We study the interaction between two adjacent but electrically isolated\nquantum point contacts (QPCs). At high enough source-drain bias on one QPC, the\ndrive QPC, we detect a finite electric current in the second, unbiased,\ndetector QPC. The current generated at the detector QPC always flows in the\nopposite direction than the current of the drive QPC. The generated current is\nmaximal, if the detector QPC is tuned to a transition region between its\nquantized conductance plateaus and the drive QPC is almost pinched-off. We\ninterpret this counterflow phenomenon in terms of an asymmetric phonon-induced\nexcitation of electrons in the leads of the detector QPC.\n", "category": [3]}
{"abstract": "  In 2nd order causal dissipative theory, space-time evolution of QGP fluid is\nstudied in 2+1 dimensions. Relaxation equations for shear stress tensors are\nsolved simultaneously with the energy-momentum conservation equations.\nComparison of evolution of ideal and viscous QGP fluid, initialized under the\nsame conditions, e.g. same equilibration time, energy density and velocity\nprofile, indicate that in a viscous dynamics, energy density or temperature of\nthe fluid evolve slowly, than in an ideal fluid. Cooling gets slower as\nviscosity increases. Transverse expansion also increases in a viscous dynamics.\nFor the first time we have also studied elliptic flow of 'quarks' in causal\nviscous dynamics. It is shown that elliptic flow of quarks saturates due to\nnon-equilibrium correction to equilibrium distribution function, and can not be\nmimicked by an ideal hydrodynamics.\n", "category": [3]}
{"abstract": "  The 32-dimensional compounding fields and their quantum interplays in the\ntrigintaduonion space can be presented by analogy with octonion and sedenion\nelectromagnetic, gravitational, strong and weak interactions. In the\ntrigintaduonion fields which are associated with the electromagnetic,\ngravitational, strong and weak interactions, the study deduces some conclusions\nof field source particles (quarks and leptons) and intermediate particles which\nare consistent with current some sorts of interaction theories. In the\ntrigintaduonion fields which are associated with the hyper-strong and\nstrong-weak fields, the paper draws some predicts and conclusions of the field\nsource particles (sub-quarks) and intermediate particles. The research results\nshow that there may exist some new particles in the nature.\n", "category": [3]}
{"abstract": "  We have performed a detailed analysis of orbital motion in the vicinity of a\nnearly extremal Kerr black hole. For very rapidly rotating black holes (spin\na=J/M>0.9524M) we have found a class of very strong field eccentric orbits\nwhose angular momentum L_z increases with the orbit's inclination with respect\nto the equatorial plane, while keeping latus rectum and eccentricity fixed.\nThis behavior is in contrast with Newtonian intuition, and is in fact opposite\nto the \"normal\" behavior of black hole orbits. Such behavior was noted\npreviously for circular orbits; since it only applies to orbits very close to\nthe black hole, they were named \"nearly horizon-skimming orbits\". Our analysis\ngeneralizes this result, mapping out the full generic (inclined and eccentric)\nfamily of nearly horizon-skimming orbits. The earlier work on circular orbits\nreported that, under gravitational radiation emission, nearly horizon-skimming\norbits tend to evolve to smaller orbit inclination, toward prograde equatorial\nconfiguration. Normal orbits, by contrast, always demonstrate slowly growing\norbit inclination (orbits evolve toward the retrograde equatorial\nconfiguration). Using up-to-date Teukolsky-fluxes, we have concluded that the\nearlier result was incorrect: all circular orbits, including nearly\nhorizon-skimming ones, exhibit growing orbit inclination. Using kludge fluxes\nbased on a Post-Newtonian expansion corrected with fits to circular and to\nequatorial Teukolsky-fluxes, we argue that the inclination grows also for\neccentric nearly horizon-skimming orbits. We also find that the inclination\nchange is, in any case, very small. As such, we conclude that these orbits are\nnot likely to have a clear and peculiar imprint on the gravitational waveforms\nexpected to be measured by the space-based detector LISA.\n", "category": [3]}
{"abstract": "  Using the AdS/CFT correspondence we derive a formula for the entanglement\nentropy of the anti-de Sitter black hole in two spacetime dimensions. The\nleading term in the large black hole mass expansion of our formula reproduces\nexactly the Bekenstein-Hawking entropy S_{BH}, whereas the subleading term\nbehaves as ln S_{BH}. This subleading term has the universal form typical for\nthe entanglement entropy of physical systems described by effective conformal\nfields theories (e.g. one-dimensional statistical models at the critical\npoint). The well-known form of the entanglement entropy for a two-dimensional\nconformal field theory is obtained as analytic continuation of our result and\nis related with the entanglement entropy of a black hole with negative mass.\n", "category": [3]}
{"abstract": "  The possibility of self-consistent determination of instanton liquid\nparameters is discussed together with the definition of optimal pseudo-particle\nconfigurations and comparing the various pseudo-particle ensembles. The\nweakening of repulsive interactions between pseudo-particles is argued and\nestimated.\n", "category": [3]}
{"abstract": "  A nonperturbative renormalization of the phi^4 model is considered. First we\nintegrate out only a single pair of conjugated modes with wave vectors +/- q.\nThen we are looking for the RG equation which would describe the transformation\nof the Hamiltonian under the integration over a shell Lambda - d Lambda < k <\nLambda, where d Lambda -> 0. We show that the known Wegner--Houghton equation\nis consistent with the assumption of a simple superposition of the integration\nresults for +/- q. The renormalized action can be expanded in powers of the\nphi^4 coupling constant u in the high temperature phase at u -> 0. We compare\nthe expansion coefficients with those exactly calculated by the diagrammatic\nperturbative method, and find some inconsistency. It causes a question in which\nsense the Wegner-Houghton equation is really exact.\n", "category": [3]}
{"abstract": "  Instanton liquid in heated and strongly interacting matter is studied using\nthe variational principle. The dependence of the instanton liquid density\n(gluon condensate) on the temperature and the quark chemical potential is\ndetermined under the assumption that, at finite temperatures, the dominant\ncontribution is given by an ensemble of calorons. The respective one-loop\neffective quark Lagrangian is used.\n", "category": [3]}
{"abstract": "  We model the essential features of eternal inflation on the landscape of a\ndense discretuum of vacua by the potential $V(\\phi)=V_{0}+\\delta V(\\phi)$,\nwhere $|\\delta V(\\phi)|\\ll V_{0}$ is random. We find that the diffusion of the\ndistribution function $\\rho(\\phi,t)$ of the inflaton expectation value in\ndifferent Hubble patches may be suppressed due to the effect analogous to the\nAnderson localization in disordered quantum systems. At $t \\to \\infty$ only the\nlocalized part of the distribution function $\\rho (\\phi, t)$ survives which\nleads to dynamical selection principle on the landscape. The probability to\nmeasure any but a small value of the cosmological constant in a given Hubble\npatch on the landscape is exponentially suppressed at $t\\to \\infty$.\n", "category": [3, 3]}
{"abstract": "  Since the past Iagrg meeting in December 2004, new developments in loop\nquantum cosmology have taken place, especially with regards to the resolution\nof the Big Bang singularity in the isotropic models. The singularity resolution\nissue has been discussed in terms of physical quantities (expectation values of\nDirac observables) and there is also an ``improved'' quantization of the\nHamiltonian constraint. These developments are briefly discussed.\n  This is an expanded version of the review talk given at the\n24$^{\\mathrm{th}}$ IAGRG meeting in February 2007.\n", "category": [3]}
{"abstract": "  Vortices are pervasive in nature, representing the breakdown of laminar fluid\nflow and hence playing a key role in turbulence. The fluid rotation associated\nwith a vortex can be parameterized by the circulation $\\Gamma=\\oint {\\rm d}{\\bf\nr}\\cdot{\\bf v}({\\bf r})$ about the vortex, where ${\\bf v}({\\bf r})$ is the\nfluid velocity field. While classical vortices can take any value of\ncirculation, superfluids are irrotational, and any rotation or angular momentum\nis constrained to occur through vortices with quantized circulation. Quantized\nvortices also play a key role in the dissipation of transport in superfluids.\nIn BECs quantized vortices have been observed in several forms, including\nsingle vortices, vortex lattices, and vortex pairs and rings. The recent\nobservation of quantized vortices in a fermionic gas was taken as a clear\nsignature of the underlying condensation and superfluidity of fermion pairs. In\naddition to BECs, quantized vortices also occur in superfluid Helium, nonlinear\noptics, and type-II superconductors.\n", "category": [3]}
{"abstract": "  Statistical ensemble formalism of Kim, Mandel and Wolf (J. Opt. Soc. Am. A 4,\n433 (1987)) offers a realistic model for characterizing the effect of\nstochastic non-image forming optical media on the state of polarization of\ntransmittedlight. With suitable choice of the Jones ensemble, various Mueller\ntransformations - some of which have been unknown so far - are deduced. It is\nobserved that the ensemble approach is formally identical to the positive\noperator valued measures (POVM) on the quantum density matrix. This\nobservation, in combination with the recent suggestion by Ahnert and Payne\n(Phys. Rev. A 71, 012330, (2005)) - in the context of generalized quantum\nmeasurement on single photon polarization states - that linear optics elements\ncan be employed in setting up all possible POVMs, enables us to propose a way\nof realizing different types of Mueller devices.\n", "category": [3, 3]}
{"abstract": "  The longitudinal and transversal spin decoherence times, $T_1$ and $T_2$, in\nsemiconductor quantum dots are investigated from equation-of-motion approach\nfor different magnetic fields, quantum dot sizes, and temperatures. Various\nmechanisms, such as the hyperfine interaction with the surrounding nuclei, the\nDresselhaus spin-orbit coupling together with the electron--bulk-phonon\ninteraction, the $g$-factor fluctuations, the direct spin-phonon coupling due\nto the phonon-induced strain, and the coaction of the\nelectron--bulk/surface-phonon interaction together with the hyperfine\ninteraction are included. The relative contributions from these spin\ndecoherence mechanisms are compared in detail. In our calculation, the\nspin-orbit coupling is included in each mechanism and is shown to have marked\neffect in most cases. The equation-of-motion approach is applied in studying\nboth the spin relaxation time $T_1$ and the spin dephasing time $T_2$, either\nin Markovian or in non-Markovian limit. When many levels are involved at finite\ntemperature, we demonstrate how to obtain the spin relaxation time from the\nFermi Golden rule in the limit of weak spin-orbit coupling. However, at high\ntemperature and/or for large spin-orbit coupling, one has to use the\nequation-of-motion approach when many levels are involved. Moreover, spin\ndephasing can be much more efficient than spin relaxation at high temperature,\nthough the two only differs by a factor of two at low temperature.\n", "category": [3]}
{"abstract": "  This lecture is devoted to the problem of computing initial data for the\nCauchy problem of 3+1 general relativity. The main task is to solve the\nconstraint equations. The conformal technique, introduced by Lichnerowicz and\nenhanced by York, is presented. Two standard methods, the conformal\ntransverse-traceless one and the conformal thin sandwich, are discussed and\nillustrated by some simple examples. Finally a short review regarding initial\ndata for binary systems (black holes and neutron stars) is given.\n", "category": [3]}
{"abstract": "  The magnetic and thermodynamic properties of spin-1/2 Heisenberg diamond\nchains are investigated in three different cases: (a) J1, J2, J3>0\n(frustrated); (b) J1, J3<0, J2>0 (frustrated); and (c) J1, J2>0, J3<0\n(non-frustrated). The density matrix renormalization group (DMRG) technique is\ninvoked to study the properties of the system in the ground state, while the\ntransfer matrix renormalization group (TMRG) technique is applied to explore\nthe thermodynamic properties. The local magnetic moments, spin correlation\nfunctions, and static structure factors are discussed in the ground state for\nthe three cases. It is shown that the static structure factor S(q) shows peaks\nat wavevectors $q=a\\pi /3$ (a=0,1,2,3,4,5) for different couplings in a zero\nmagnetic field, which, however in the magnetic fields where the magnetization\nplateau with m=1/6 pertains, exhibits the peaks only at q=0, $2\\pi /3$ and\n$4\\pi /3$, which are found to be couplings-independent. The DMRG results of the\nzero-field static structure factor can be nicely fitted by a linear\nsuperposition of six modes, where two fitting equations are proposed. It is\nobserved that the six modes are closely related to the low-lying excitations of\nthe system. At finite temperatures, the double-peak structures of the\nsusceptibility and specific heat against temperature are obtained, where the\npeak positions and heights are found to depend on the competition of the\ncouplings. It is also uncovered that the XXZ anisotropy of F and AF couplings\nleads the system of case (c) to display quite different behaviors. In addition,\nthe experimental data of the susceptibility, specific heat and magnetization\nfor the compound Cu$_{3}$(CO$_{3}$)$_{2}$(OH)$_{2}$ are fairly compared with\nour TMRG results.\n", "category": [3]}
{"abstract": "  The extraction of a physical law y=yo(x) from joint experimental data about x\nand y is treated. The joint, the marginal and the conditional probability\ndensity functions (PDF) are expressed by given data over an estimator whose\nkernel is the instrument scattering function. As an optimal estimator of yo(x)\nthe conditional average is proposed. The analysis of its properties is based\nupon a new definition of prediction quality. The joint experimental information\nand the redundancy of joint measurements are expressed by the relative entropy.\nWith the number of experiments the redundancy on average increases, while the\nexperimental information converges to a certain limit value. The difference\nbetween this limit value and the experimental information at a finite number of\ndata represents the discrepancy between the experimentally determined and the\ntrue properties of the phenomenon. The sum of the discrepancy measure and the\nredundancy is utilized as a cost function. By its minimum a reasonable number\nof data for the extraction of the law yo(x) is specified. The mutual\ninformation is defined by the marginal and the conditional PDFs of the\nvariables. The ratio between mutual information and marginal information is\nused to indicate which variable is the independent one. The properties of the\nintroduced statistics are demonstrated on deterministically and randomly\nrelated variables.\n", "category": [3, 3]}
{"abstract": "  The solutions of the Wigner-transformed time-dependent\nHartree--Fock--Bogoliubov equations are studied in the constant-$\\Delta$\napproximation. This approximation is known to violate particle-number\nconservation. As a consequence, the density fluctuation and the longitudinal\nresponse function given by this approximation contain spurious contributions. A\nsimple prescription for restoring both local and global particle-number\nconservation is proposed. Explicit expressions for the eigenfrequencies of the\ncorrelated systems and for the density response function are derived and it is\nshown that the semiclassical analogous of the quantum single--particle spectrum\nhas an excitation gap of $2\\Delta$, in agreement with the quantum result. The\ncollective response is studied for a simplified form of the residual\ninteraction.\n", "category": [3]}
{"abstract": "  Classical oscillator differential equation is replaced by the corresponding\n(finite time) difference equation. The equation is, then, symmetrized so that\nit remains invariant under the change d going to -d, where d is the smallest\nspan of time. This symmetric equation has solutions, which come in reciprocally\nrelated pairs. One member of a pair agrees with the classical solution and the\nother is an oscillating solution and does not converge to a limit as d goes to\n0. This solution contributes to oscillator energy a term which is a multiple of\nhalf-integers.\n", "category": [3]}
{"abstract": "  In this talk we briefly summarize our theoretical understanding of in-medium\nselfenergies of hadrons. With the special case of the $\\omega$ meson we\ndemonstrate that earlier calculations that predicted a significant lowering of\nthe mass in medium are based on an incorrect treatment of the model Lagrangian;\nmore consistent calculations lead to a significant broadening, but hardly any\nmass shift. We stress that the experimental reconstruction of hadron spectral\nfunctions from measured decay products always requires knowledge of the decay\nbranching ratios which may also be strongly mass-dependent. It also requires a\nquantitatively reliable treatment of final state interactions which has to be\npart of any reliable theory.\n", "category": [3, 3, 3]}
{"abstract": "  An overview of some analytical approaches to the computation of the\nstructural and thermodynamic properties of single component and multicomponent\nhard-sphere fluids is provided. For the structural properties, they yield a\nthermodynamically consistent formulation, thus improving and extending the\nknown analytical results of the Percus-Yevick theory. Approximate expressions\nfor the contact values of the radial distribution functions and the\ncorresponding analytical equations of state are also discussed. Extensions of\nthis methodology to related systems, such as sticky hard spheres and\nsquare-well fluids, as well as its use in connection with the perturbation\ntheory of fluids are briefly addressed.\n", "category": [3, 3, 3]}
{"abstract": "  By means of the diffusion entropy approach, we detect the scale-invariance\ncharacteristics embedded in the 4737 human promoter sequences. The exponent for\nthe scale-invariance is in a wide range of $[ {0.3,0.9} ]$, which centered at\n$\\delta_c = 0.66$. The distribution of the exponent can be separated into left\nand right branches with respect to the maximum. The left and right branches are\nasymmetric and can be fitted exactly with Gaussian form with different widths,\nrespectively.\n", "category": [4]}
{"abstract": "  We present a new high-resolution angle-resolved photoemission study of\n1\\textit{T}-TiSe$_{2}$ in both, its room-temperature, normal phase and its\nlow-temperature, charge-density wave phase. At low temperature the\nphotoemission spectra are strongly modified, with large band renormalisations\nat high-symmetry points of the Brillouin zone and a very large transfer of\nspectral weight to backfolded bands. A theoretical calculation of the spectral\nfunction for an excitonic insulator phase reproduces the experimental features\nwith very good agreement. This gives strong evidence in favour of the excitonic\ninsulator scenario as a driving force for the charge-density wave transition in\n1\\textit{T}-TiSe$_{2}$.\n", "category": [3, 3]}
{"abstract": "  We have studied Zn(CN)2 at high pressure using Raman spectroscopy, and report\nGruneisen parameters of the soft phonons. The phonon frequencies and eigen\nvectors obtained from ab-initio calculations are used for the assignment of the\nobserved phonon spectra. Out of the eleven zone-centre optical modes, six modes\nexhibit negative Gruneisen parameter. The calculations suggest that the soft\nphonons correspond to the librational and translational modes of CN rigid unit,\nwith librational modes contributing more to thermal expansion. A rapid\ndisordering of the lattice is found above 1.6 GPa from X-ray diffraction.\n", "category": [3]}
{"abstract": "  Redundancy of experimental data is the basic statistic from which the\ncomplexity of a natural phenomenon and the proper number of experiments needed\nfor its exploration can be estimated. The redundancy is expressed by the\nentropy of information pertaining to the probability density function of\nexperimental variables. Since the calculation of entropy is inconvenient due to\nintegration over a range of variables, an approximate expression for redundancy\nis derived that includes only a sum over the set of experimental data about\nthese variables. The approximation makes feasible an efficient estimation of\nthe redundancy of data along with the related experimental information and\ninformation cost function. From the experimental information the complexity of\nthe phenomenon can be simply estimated, while the proper number of experiments\nneeded for its exploration can be determined from the minimum of the cost\nfunction. The performance of the approximate estimation of these statistics is\ndemonstrated on two-dimensional normally distributed random data.\n", "category": [3, 3]}
{"abstract": "  Classical effective potentials are indispensable for any large-scale\natomistic simulations, and the relevance of simulation results crucially\ndepends on the quality of the potentials used. For complex alloys like\nquasicrystals, however, realistic effective potentials are practically\ninexistent. We report here on our efforts to develop effective potentials\nespecially for quasicrystalline alloy systems. We use the so-called force\nmatching method, in which the potential parameters are adapted so as to\noptimally reproduce the forces and energies in a set of suitably chosen\nreference configurations. These reference data are calculated with ab-initio\nmethods. As a first application, EAM potentials for decagonal Al-Ni-Co,\nicosahedral Ca-Cd, and both icosahedral and decagonal Mg-Zn quasicrystals have\nbeen constructed. The influence of the potential range and degree of\nspecialisation on the accuracy and other properties is discussed and compared.\n", "category": [3]}
{"abstract": "  Let $M$ be a smooth manifold and let $\\F$ be a codimension one, $C^\\infty$\nfoliation on $M$, with isolated singularities of Morse type. The study and\nclassification of pairs $(M,\\F)$ is a challenging (and difficult) problem. In\nthis setting, a classical result due to Reeb \\cite{Reeb} states that a manifold\nadmitting a foliation with exactly two center-type singularities is a sphere.\nIn particular this is true if the foliation is given by a function. Along these\nlines a result due to Eells and Kuiper \\cite{Ku-Ee} classify manifolds having a\nreal-valued function admitting exactly three non-degenerate singular points. In\nthe present paper, we prove a generalization of the above mentioned results. To\ndo this, we first describe the possible arrangements of pairs of singularities\nand the corresponding codimension one invariant sets, and then we give an\nelimination procedure for suitable center-saddle and some saddle-saddle\nconfigurations (of consecutive indices). In the second part, we investigate if\nother classical results, such as Haefliger and Novikov (Compact Leaf) theorems,\nproved for regular foliations, still hold true in presence of singularities. At\nthis purpose, in the singular set, $Sing(\\F)$ of the foliation $\\F$, we\nconsider {\\em{weakly stable}} components, that we define as those components\nadmitting a neighborhood where all leaves are compact. If $Sing(\\F)$ admits\nonly weakly stable components, given by smoothly embedded curves diffeomorphic\nto $S^1$, we are able to extend Haefliger's theorem. Finally, the existence of\na closed curve, transverse to the foliation, leads us to state a Novikov-type\nresult.\n", "category": [2]}
{"abstract": "  Let g be a finite dimensional complex semisimple Lie algebra, and let V be a\nfinite dimensional represenation of g. We give a closed formula for the mth\nFrobenius-Schur indicator, m>1, of V in representation-theoretic terms. We\ndeduce that the indicators take integer values, and that for a large enough m,\nthe mth indicator of V equals the dimension of the zero weight space of V. For\nthe classical Lie algebras sl(n), so(2n), so(2n+1) and sp(2n), this is the case\nfor m greater or equal to 2n-1, 4n-5, 4n-3 and 2n+1, respectively.\n", "category": [2, 2]}
{"abstract": "  In this paper we consider quiver gauge theories with fractional branes whose\ninfrared dynamics removes the classical supersymmetric vacua (DSB branes). We\nshow that addition of flavors to these theories (via additional non-compact\nbranes) leads to local meta-stable supersymmetry breaking minima, closely\nrelated to those of SQCD with massive flavors. We simplify the study of the\none-loop lifting of the accidental classical flat directions by direct\ncomputation of the pseudomoduli masses via Feynman diagrams. This new approach\nallows to obtain analytic results for all these theories. This work extends the\nresults for the $dP_1$ theory in hep-th/0607218. The new approach allows to\ngeneralize the computation to general examples of DSB branes, and for arbitrary\nvalues of the superpotential couplings.\n", "category": [3]}
{"abstract": "  I discuss low energy aspects of heavy meson decays, where there is at least\none heavy meson in the final state. Examples are $B -\\bar{B}$ mixing, $B \\to D\n\\bar{D}$, $B \\to D \\eta'$, and $B \\to D \\gamma$. %and $B \\to D W $ (Isgur-Wise\nfunction). The analysis is performed in the heavy quark limit within\nheavy-light chiral perturbation theory. Coefficients of $1/N_c$ suppressed\nchiral Lagrangian terms (beyond factorization) have been estimated by means of\na heavy-light chiral quark model.\n", "category": [3]}
{"abstract": "  The investigation on the production of particles in slowly varying but\nextremely intense magnetic field in extended to the case of axions. The\nmotivation is, as for some previously considered cases, the possibility that\nsuch kind of magnetic field may exist around very compact astrophysical\nobjects.\n", "category": [3]}
{"abstract": "  The sequence of phase transitions and the symmetry of in particular the low\ntemperature incommensurate and spin-Peierls phases of the quasi one-dimensional\ninorganic spin-Peierls system TiOX (TiOBr and TiOCl) have been studied using\ninelastic light scattering experiments. The anomalous first-order character of\nthe transition to the spin-Peierls phase is found to be a consequence of the\ndifferent symmetries of the incommensurate and spin-Peierls (P$2_{1}/m$)\nphases.\n  The pressure dependence of the lowest transition temperature strongly\nsuggests that magnetic interchain interactions play an important role in the\nformation of the spin-Peierls and the incommensurate phases. Finally, a\ncomparison of Raman data on VOCl to the TiOX spectra shows that the high energy\nscattering observed previously has a phononic origin.\n", "category": [3, 3]}
{"abstract": "  We show that temperature and magnetic field properties of the entanglement\nbetween spins on the two-dimensional Shastry-Sutherland lattice can be\nqualitatively described by analytical results for a qubit tetramer. Exact\ndiagonalization of clusters with up to 20 sites reveals that the regime of\nfully entangled neighboring pairs coincides with the regime of finite spin gap\nin the spectrum. Additionally, the results for the regime of vanishing spin gap\nare discussed and related to the Heisenberg limit of the model.\n", "category": [3, 3]}
{"abstract": "  We investigate the bonding of H in O vacancies of ZnO using density\nfunctional calculations. We find that H is anionic and does not form\nmulticenter bonds with Zn in this compound.\n", "category": [3]}
{"abstract": "  The extraction of the weak phase $\\alpha$ from $B\\to\\pi\\pi$ decays has been\ncontroversial from a statistical point of view, as the frequentist vs. bayesian\nconfrontation shows. We analyse several relevant questions which have not\ndeserved full attention and pervade the extraction of $\\alpha$.\nReparametrization Invariance proves appropriate to understand those issues. We\nshow that some Standard Model inspired parametrizations can be senseless or\ninadequate if they go beyond the minimal Gronau and London assumptions: the\nsingle weak phase $\\alpha$ just in the $\\Delta I=3/2$ amplitudes, the isospin\nrelations and experimental data. Beside those analyses, we extract $\\alpha$\nthrough the use of several adequate parametrizations, showing that there is no\nrelevant discrepancy between frequentist and bayesian results. The most\nrelevant information, in terms of $\\alpha$, is the exclusion of values around\n$\\alpha\\sim \\pi/4$; this result is valid in the presence of arbitrary New\nPhysics contributions to the $\\Delta I=1/2$ piece.\n", "category": [3, 3]}
{"abstract": "  Although the Gauss-Bonnet term is a topological invariant for general\nrelativity, it couples naturally to a quintessence scalar field, modifying\ngravity at solar system scales. We determine the solar system constraints due\nto this term by evaluating the post-Newtonian metric for a distributional\nsource. We find a mass dependent, 1/r^7 correction to the Newtonian potential,\nand also deviations from the Einstein gravity prediction for light-bending. We\nconstrain the parameters of the theory using planetary orbits, the Cassini\nspacecraft data, and a laboratory test of Newton's law, always finding\nextremely tight bounds on the energy associated to the Gauss-Bonnet term. We\ndiscuss the relevance of these constraints to late-time cosmological\nacceleration.\n", "category": [3, 3]}
{"abstract": "  The electronic transport properties and switching mechanism of single\nphotochromic diarylethene derivatives sandwiched between two gold surfaces with\nclosed and open configurations are investigated by a fully self-consistent\nnonequilibrium Green's function method combined with density functional theory.\nThe calculated transmission spectra of two configurations are strikingly\ndistinctive. The open form lacks any significant transmission peak within a\nwide energy window, while the closed structure has two significant transmission\npeaks on the both sides of the Fermi level. The electronic transport properties\nof the molecular junction with closed structure under a small bias voltage are\nmainly determined by the tail of the transmission peak contributed unusually by\nthe perturbed lowest perturbed unoccupied molecular orbital. The calculated\non-off ratio of currents between the closed and open configurations is about\ntwo orders of magnitude, which reproduces the essential features of the\nexperimental measured results. Moreover, we find that the switching behavior\nwithin a wide bias voltage window is extremely robust to both substituting F or\nS for H or O and varying end anchoring atoms from S to Se and Te.\n", "category": [3, 3]}
{"abstract": "  Using the recently reported mode locking effect we demonstrate a highly\nrobust control of electron spin coherence in an ensemble of (In,Ga)As quantum\ndots during the single spin coherence time. The spin precession in a transverse\nmagnetic field can be fully controlled up to 25 K by the parameters of the\nexciting pulsed laser protocol such as the pulse train sequence, leading to\nadjustable quantum beat bursts in Faraday rotation. Flipping of the electron\nspin precession phase was demonstrated by inverting the polarization within a\npulse doublet sequence.\n", "category": [3]}
{"abstract": "  We calculate the equation of state of dense hydrogen within the chemical\npicture. Fluid variational theory is generalized for a multi-component system\nof molecules, atoms, electrons, and protons. Chemical equilibrium is supposed\nfor the reactions dissociation and ionization. We identify the region of\nthermodynamic instability which is related to the plasma phase transition. The\nreflectivity is calculated along the Hugoniot curve and compared with\nexperimental results. The equation-of-state data is used to calculate the\npressure and temperature profiles for the interior of Jupiter.\n", "category": [3]}
{"abstract": "  Some aspects, both experimental and theoretical, of extracting the neutron\nskin $\\Delta R$ from properties of isovector giant resonances are discussed.\nExisting proposals are critically reviewed. The method relying on the energy\ndifference between the GTR and IAS is shown to lack sensitivity to $\\Delta R$.\nA simple explanation of the linear relation between the symmetry energy and the\nneutron skin is also given.\n", "category": [3]}
{"abstract": "  We investigate the use of a Genetic Algorithm (GA) to design a set of\nphotonic crystals (PCs) in one and two dimensions. Our flexible design\nmethodology allows us to optimize PC structures which are optimized for\nspecific objectives. In this paper, we report the results of several such\nGA-based PC optimizations. We show that the GA performs well even in very\ncomplex design spaces, and therefore has great potential for use as a robust\ndesign tool in present and future applications.\n", "category": [3, 3]}
{"abstract": "  We present an x-ray spectromicroscopic investigation of single-crystalline\nmagnetic FeMn/Co bilayers on Cu(001), using X-ray magnetic circular (XMCD) and\nlinear (XMLD) dichroism at the Co and Fe L3 absorption edges in combination\nwith photoelectron emission microscopy (PEEM). Using the magnetic coupling\nbetween the ferromagnetic Co layer and the antiferromagnetic FeMn layer we are\nable to produce magnetic domains with two different crystallographic\norientations of the magnetic easy axis within the same sample at the same time.\nWe find a huge difference in the XMLD contrast between the two types of\nmagnetic domains, which we discuss in terms of intrinsic magneto-crystalline\nanisotropy of XMLD of the Co layer. We also demonstrate that due to the high\nsensitivity of the method, the small number of induced ferromagnetic Fe moments\nat the FeMn-Co interface is sufficient to obtain magnetic contrast from XMLD in\na metallic system.\n", "category": [3]}
{"abstract": "  This paper examines the effect of temperature on the structural stability and\nmechanical properties of 20 layered (10,10) single walled carbon nanotubes\n(SWCNTs) under tensile loading using an O(N) tight binding molecular dynamics\n(TBMD) simulation method. We observed that (10,10) tube can sustain its\nstructural stability for the strain values of 0.23 in elongation and 0.06 in\ncompression at 300K. Bond breaking strain value decreases with increasing\ntemperature under streching but not under compression. The elastic limit,\nYoung's modulus, tensile strength and Poisson ratio are calculated as 0.10,\n0.395 TPa, 83.23 GPa, 0.285, respectively, at 300K. In the temperature range\nfrom 300K to 900K; Young's modulus and the tensile strengths are decreasing\nwith increasing temperature while the Poisson ratio is increasing. At higher\ntemperatures, Young's modulus starts to increase while the Poisson ratio and\ntensile strength decrease. In the temperature range from 1200K to 1800K, the\nSWCNT is already deformed and softened. Applying strain on these deformed and\nsoftened SWCNTs do not follow the same pattern as in the temperature range of\n300K to 900K.\n", "category": [3]}
{"abstract": "  We present a program called potfit which generates an effective atomic\ninteraction potential by matching it to a set of reference data computed in\nfirst-principles calculations. It thus allows to perform large-scale atomistic\nsimulations of materials with physically justified potentials. We describe the\nfundamental principles behind the program, emphasizing its flexibility in\nadapting to different systems and potential models, while also discussing its\nlimitations. The program has been used successfully in creating effective\npotentials for a number of complex intermetallic alloys, notably quasicrystals.\n", "category": [3]}
{"abstract": "  We consider a Mass Varying Neutrinos (MaVaNs) model in supersymmetric theory.\nThe model includes effects of supersymmetry breaking transmitted by the\ngravitational interaction from the hidden sector, in which supersymmetry was\nbroken, to the dark energy sector. Then evolutions of the neutrino mass and the\nequation of state parameter of the dark energy are presented in the model. It\nis remarked that only the mass of a sterile neutrino is variable in the case of\nthe vanishing mixing between the left-handed and a sterile neutrino on\ncosmological time scale. The finite mixing makes the mass of the left-handed\nneutrino variable.\n", "category": [3]}
{"abstract": "  We develop rigorous, analytic techniques to study the behaviour of biased\nrandom walks on combs. This enables us to calculate exactly the spectral\ndimension of random comb ensembles for any bias scenario in the teeth or spine.\nTwo specific examples of random comb ensembles are discussed; the random comb\nwith nonzero probability of an infinitely long tooth at each vertex on the\nspine and the random comb with a power law distribution of tooth lengths. We\nalso analyze transport properties along the spine for these probability\nmeasures.\n", "category": [3]}
{"abstract": "  The groups G_{k,1} of Richard Thompson and Graham Higman can be generalized\nin a natural way to monoids, that we call M_{k,1}, and to inverse monoids,\ncalled Inv_{k,1}; this is done by simply generalizing bijections to partial\nfunctions or partial injective functions. The monoids M_{k,1} have connections\nwith circuit complexity (studied in another paper). Here we prove that M_{k,1}\nand Inv_{k,1} are congruence-simple for all k. Their Green relations J and D\nare characterized: M_{k,1} and Inv_{k,1} are J-0-simple, and they have k-1\nnon-zero D-classes. They are submonoids of the multiplicative part of the Cuntz\nalgebra O_k. They are finitely generated, and their word problem over any\nfinite generating set is in P. Their word problem is coNP-complete over certain\ninfinite generating sets.\n  Changes in this version: Section 4 has been thoroughly revised, and errors\nhave been corrected; however, the main results of Section 4 do not change.\nSections 1, 2, and 3 are unchanged, except for the proof of Theorem 2.3, which\nwas incomplete; a complete proof was published in the Appendix of reference\n[6], and is also given here.\n", "category": [2]}
{"abstract": "  In the SK analysis of the neutrino events for [Fully Contained Events] and\n[Partially Contained Events] on their zenith angle distribution, it is assumed\nthat the zenith angle of the incident neutrino is the same as that of the\ndetected charged lepton. In the present paper, we examine the validity of [the\nSK assumption on the direction] of the incident neutrinos. Concretely speaking,\nwe analyze muon-like events due to QEL. For the purpose, we develop [Time\nSequential Monte Carlo Simulation] to extract the conclusion on the validity of\nthe SK assumption. In our [Time Sequential Simulation], we simulate every\nphysical process concerned as exactly as possible without any approximation.\n  From the comparison between the zenith angle distributon of the emitted muons\nunder [the SK assumption on the direction] and the corresponding one obtained\nunder our [Time Sequential Simulation], it is concluded that the measurement of\nthe direction of the incident neutrino for the neutrino events occurring inside\nthe detector in the SK analysis turns out to be unreliable, which holds\nirrespective of the existence and/or non-existence of the neutrino oscillation.\n", "category": [3, 3]}
{"abstract": "  A number of recently discovered protein structures incorporate a rather\nunexpected structural feature: a knot in the polypeptide backbone. These knots\nare extremely rare, but their occurrence is likely connected to protein\nfunction in as yet unexplored fashion. Our analysis of the complete Protein\nData Bank reveals several new knots which, along with previously discovered\nones, can shed light on such connections. In particular, we identify the most\ncomplex knot discovered to date in human ubiquitin hydrolase, and suggest that\nits entangled topology protects it against unfolding and degradation by the\nproteasome. Knots in proteins are typically preserved across species and\nsometimes even across kingdoms. However, we also identify a knot which only\nappears in some transcarbamylases while being absent in homologous proteins of\nsimilar structure. The emergence of the knot is accompanied by a shift in the\nenzymatic function of the protein. We suggest that the simple insertion of a\nshort DNA fragment into the gene may suffice to turn an unknotted into a\nknotted structure in this protein.\n", "category": [3, 3, 4]}
{"abstract": "  We study the real-time domain-wall dynamics near a quantum critical point of\nthe one-dimensional anisotropic ferromagnetic spin 1/2 chain. By numerical\nsimulation, we find the domain wall is dynamically stable in the\nHeisenberg-Ising model. Near the quantum critical point, the width of the\ndomain wall diverges as $(\\Delta -1) ^{-1/2}$.\n", "category": [3, 3]}
{"abstract": "  We consider an electrostatic qubit, interacting with a fluctuating charge of\nsingle electron transistor (SET) in the framework of exactly solvable model.\nThe SET plays a role of the fluctuating environment affecting the qubit's\nparameters in a controllable way. We derive the rate equations describing\ndynamics of the entire system for both weak and strong qubit-SET coupling.\nSolving these equation we obtain decoherence and relaxation rates of the qubit,\nas well as the spectral density of the fluctuating qubit's parameters. We found\nthat in the weak coupling regime the decoherence and relaxation rates are\ndirectly related to the spectral density taken at Rabi or at zero frequency,\ndepending on what a particular qubit's parameters is fluctuating. This relation\nholds also in the presence of weak back-action of the qubit on the fluctuating\nenvironment. In the case of strong back-action, such simple relationship no\nlonger holds, even if the qubit-SET coupling is small. It does not hold either\nin the strong-coupling regime, even in the absence of the back-action. In\naddition, we found that our model predicts localization of the qubit in the\nstrong-coupling regime, resembling that of the spin-boson model.\n", "category": [3]}
{"abstract": "  We characterize a natural class of modular categories of prime power\nFrobenius-Perron dimension as representation categories of twisted doubles of\nfinite p-groups. We also show that a nilpotent braided fusion category C admits\nan analogue of the Sylow decomposition. If the simple objects of C have\nintegral Frobenius-Perron dimensions then C is group-theoretical. As a\nconsequence, we obtain that semisimple quasi-Hopf algebras of prime power\ndimension are group-theoretical. Our arguments are based on a reconstruction of\ntwisted group doubles from Lagrangian subcategories of modular categories (this\nis reminiscent to the characterization of doubles of quasi-Lie bialgebras in\nterms of Manin pairs).\n", "category": [2, 2]}
{"abstract": "  We calculate the N_c dependence of the decay widths of exotic eikosiheptaplet\nwithin the framework of Chral Quark Soliton Model. We also discuss\ngeneralizations of regular baryon representations for arbitrary N_c.\n", "category": [3]}
{"abstract": "  In this work we consider random Boolean networks that provide a general model\nfor genetic regulatory networks. We extend the analysis of James Lynch who was\nable to proof Kauffman's conjecture that in the ordered phase of random\nnetworks, the number of ineffective and freezing gates is large, where as in\nthe disordered phase their number is small. Lynch proved the conjecture only\nfor networks with connectivity two and non-uniform probabilities for the\nBoolean functions. We show how to apply the proof to networks with arbitrary\nconnectivity $K$ and to random networks with biased Boolean functions. It turns\nout that in these cases Lynch's parameter $\\lambda$ is equivalent to the\nexpectation of average sensitivity of the Boolean functions used to construct\nthe network. Hence we can apply a known theorem for the expectation of the\naverage sensitivity. In order to prove the results for networks with biased\nfunctions, we deduct the expectation of the average sensitivity when only\nfunctions with specific connectivity and specific bias are chosen at random.\n", "category": [3]}
{"abstract": "  We calculate the intensity of the polariton mediated inelastic light\nscattering in semiconductor microcavities. We treat the exciton-photon coupling\nnonperturbatively and incorporate lifetime effects in both excitons and\nphotons, and a coupling of the photons to the electron-hole continuum. Taking\nthe matrix elements as fitting parameters, the results are in excellent\nagreement with measured Raman intensities due to optical phonons resonant with\nthe upper polariton branches in II-VI microcavities with embedded CdTe quantum\nwells.\n", "category": [3, 3]}
{"abstract": "  Given a finite irreducible Coxeter group $W$, a positive integer $d$, and\ntypes $T_1,T_2,...,T_d$ (in the sense of the classification of finite Coxeter\ngroups), we compute the number of decompositions $c=\\si_1\\si_2 cdots\\si_d$ of a\nCoxeter element $c$ of $W$, such that $\\si_i$ is a Coxeter element in a\nsubgroup of type $T_i$ in $W$, $i=1,2,...,d$, and such that the factorisation\nis \"minimal\" in the sense that the sum of the ranks of the $T_i$'s,\n$i=1,2,...,d$, equals the rank of $W$. For the exceptional types, these\ndecomposition numbers have been computed by the first author. The type $A_n$\ndecomposition numbers have been computed by Goulden and Jackson, albeit using a\nsomewhat different language. We explain how to extract the type $B_n$\ndecomposition numbers from results of B\\'ona, Bousquet, Labelle and Leroux on\nmap enumeration. Our formula for the type $D_n$ decomposition numbers is new.\nThese results are then used to determine, for a fixed positive integer $l$ and\nfixed integers $r_1\\le r_2\\le ...\\le r_l$, the number of multi-chains $\\pi_1\\le\n\\pi_2\\le ...\\le \\pi_l$ in Armstrong's generalised non-crossing partitions\nposet, where the poset rank of $\\pi_i$ equals $r_i$, and where the \"block\nstructure\" of $\\pi_1$ is prescribed. We demonstrate that this result implies\nall known enumerative results on ordinary and generalised non-crossing\npartitions via appropriate summations. Surprisingly, this result on multi-chain\nenumeration is new even for the original non-crossing partitions of Kreweras.\nMoreover, the result allows one to solve the problem of rank-selected chain\nenumeration in the type $D_n$ generalised non-crossing partitions poset, which,\nin turn, leads to a proof of Armstrong's $F=M$ Conjecture in type $D_n$.\n", "category": [2, 2]}
{"abstract": "  The electromagnetic polarizabilities of the nucleon are shown to be\nessentially composed of the nonresonant $\\alpha_p(E_{0+})=+3.2$,\n$\\alpha_n(E_{0+})=+4.1$,the $t$-channel $\\alpha^t_{p,n}=-\\beta^t_{p,n}=+7.6$\nand the resonant $\\beta_{p,n}(P_{33}(1232))=+8.3$ contributions (in units of\n$10^{-4}$fm$^3$. The remaining deviations from the experimental data\n$\\Delta\\alpha_p=1.2\\pm 0.6$, $\\Delta\\beta_p=1.2\\mp 0.6$, \\Delta\\alpha_n=0.8\\pm\n1.7$ and $\\Delta\\beta_n=2.0\\mp 1.8$ are contributed by a larger number of\nresonant and nonresonant processes with cancellations between the\ncontributions. This result confirms that dominant contributions to the electric\nand magnetic polarizabilities may be represented in terms of two-photon\ncouplings to the $\\sigma$-meson having the predicted mass $m_\\sigma=666$ MeV\nand two-photon width $\\Gamma_{\\gamma\\gamma}=2.6$ keV.\n", "category": [3, 3]}
{"abstract": "  Associated to the classical Weyl groups, we introduce the notion of\ndegenerate spin affine Hecke algebras and affine Hecke-Clifford algebras. For\nthese algebras, we establish the PBW properties, formulate the intertwiners,\nand describe the centers. We further develop connections of these algebras with\nthe usual degenerate (i.e. graded) affine Hecke algebras of Lusztig by\nintroducing a notion of degenerate covering affine Hecke algebras.\n", "category": [2, 2]}
{"abstract": "  We present a theory of transport through interacting quantum dots coupled to\nnormal and superconducting leads in the limit of weak tunnel coupling. A\nJosephson current between two superconducting leads, carried by first-order\ntunnel processes, can be established by non-equilibrium proximity effect. Both\nAndreev and Josephson current is suppressed for bias voltages below a threshold\nset by the Coulomb charging energy. A $\\pi$-transition of the supercurrent can\nbe driven by tuning gate or bias voltages.\n", "category": [3, 3]}
{"abstract": "  We theoretically investigate the possibility of observing resonant activation\nin the hopping dynamics of two-mode semiconductor lasers. We present a series\nof simulations of a rate-equations model under random and periodic modulation\nof the bias current. In both cases, for an optimal choice of the modulation\ntime-scale, the hopping times between the stable lasing modes attain a minimum.\nThe simulation data are understood by means of an effective one-dimensional\nLangevin equation with multiplicative fluctuations. Our conclusions apply to\nboth Edge Emitting and Vertical Cavity Lasers, thus opening the way to several\nexperimental tests in such optical systems.\n", "category": [3]}
{"abstract": "  Some of the means through which the possible presence of nearly deconfined\nquarks in neutron stars can be detected by astrophysical observations of\nneutron stars from their birth to old age are highlighted.\n", "category": [3, 3]}
{"abstract": "  We classify all fusion categories for a given set of fusion rules with three\nsimple object types. If a conjecture of Ostrik is true, our classification\ncompletes the classification of fusion categories with three simple object\ntypes. To facilitate the discussion we describe a convenient, concrete and\nuseful variation of graphical calculus for fusion categories, discuss\npivotality and sphericity in this framework, and give a short and elementary\nre-proof of the fact that the quadruple dual functor is naturally isomorphic to\nthe identity.\n", "category": [2, 2]}
{"abstract": "  We extend our previous classification of superpotentials of ``scalar\ncurvature type\" for the cohomogeneity one Ricci-flat equations. We now consider\nthe case not covered in our previous paper, i.e., when some weight vector of\nthe superpotential lies outside (a scaled translate of) the convex hull of the\nweight vectors associated with the scalar curvature function of the principal\norbit. In this situation we show that either the isotropy representation has at\nmost 3 irreducible summands or the first order subsystem associated to the\nsuperpotential is of the same form as the Calabi-Yau condition for submersion\ntype metrics on complex line bundles over a Fano K\\\"ahler-Einstein product.\n", "category": [2]}
{"abstract": "  The minimum semi-degree of a digraph D is the minimum of its minimum\noutdegree and its minimum indegree. We show that every sufficiently large\ndigraph D with minimum semi-degree at least n/2 +k-1 is k-linked. The bound on\nthe minimum semi-degree is best possible and confirms a conjecture of\nManoussakis from 1990. We also determine the smallest minimum semi-degree which\nensures that a sufficiently large digraph D is k-ordered, i.e. that for every\nordered sequence of k distinct vertices of D there is a directed cycle which\nencounters these vertices in this order.\n", "category": [2]}
{"abstract": "  We study cosmological perturbations in two-field inflation, allowing for\nnon-standard kinetic terms. We calculate analytically the spectra of curvature\nand isocurvature modes at Hubble crossing, up to first order in the slow-roll\nparameters. We also compute numerically the evolution of the curvature and\nisocurvature modes from well within the Hubble radius until the end of\ninflation. We show explicitly for a few examples, including the recently\nproposed model of `roulette' inflation, how isocurvature perturbations affect\nsignificantly the curvature perturbation between Hubble crossing and the end of\ninflation.\n", "category": [3, 3]}
{"abstract": "  This article has been withdrawn because it has been merged with the earlier\narticle GCT3 (arXiv: CS/0501076 [cs.CC]) in the series. The merged article is\nnow available as:\n  Geometric Complexity Theory III: on deciding nonvanishing of a\nLittlewood-Richardson Coefficient, Journal of Algebraic Combinatorics, vol. 36,\nissue 1, 2012, pp. 103-110. (Authors: Ketan Mulmuley, Hari Narayanan and Milind\nSohoni)\n  The new article in this GCT5 slot in the series is:\n  Geometric Complexity Theory V: Equivalence between blackbox derandomization\nof polynomial identity testing and derandomization of Noether's Normalization\nLemma, in the Proceedings of FOCS 2012 (abstract), arXiv:1209.5993 [cs.CC]\n(full version) (Author: Ketan Mulmuley)\n", "category": [0]}
{"abstract": "  One-dimensional scattering problem admitting a complex, PT-symmetric\nshort-range potential V(x) is considered. Using a Runge-Kutta-discretized\nversion of Schroedinger equation we derive the formulae for the reflection and\ntransmission coefficients and emphasize that the only innovation emerges in\nfact via a complexification of one of the potential-characterizing parameters.\n", "category": [3]}
{"abstract": "  In this note we consider the time of the collision $\\tau$ for $n$ independent\nBrownian motions $X^1_t,...,X_t^n$ with drifts $a_1,...,a_n$, each starting\nfrom $x=(x_1,...,x_n)$, where $x_1<...<x_n$. We show the exact asymptotics of\n$P_x(\\tau>t) = C h(x)t^{-\\alpha}e^{-\\gamma t}(1 + o(1))$ as $t\\to\\infty$ and\nidentify $C,h(x),\\alpha,\\gamma$ in terms of the drifts.\n", "category": [2]}
{"abstract": "  Employing density-functional calculations we study single and double graphene\nlayers on Si- and C-terminated 1x1-6H-SiC surfaces. We show that, in contrast\nto earlier assumptions, the first carbon layer is covalently bonded to the\nsubstrate, and cannot be responsible for the graphene-type electronic spectrum\nobserved experimentally. The characteristic spectrum of free-standing graphene\nappears with the second carbon layer, which exhibits a weak van der Waals\nbonding to the underlying structure. For Si-terminated substrate, the interface\nis metallic, whereas on C-face it is semiconducting or semimetallic for single\nor double graphene coverage, respectively.\n", "category": [3]}
{"abstract": "  Given a multiple-input multiple-output (MIMO) channel, feedback from the\nreceiver can be used to specify a transmit precoding matrix, which selectively\nactivates the strongest channel modes. Here we analyze the performance of\nRandom Vector Quantization (RVQ), in which the precoding matrix is selected\nfrom a random codebook containing independent, isotropically distributed\nentries. We assume that channel elements are i.i.d. and known to the receiver,\nwhich relays the optimal (rate-maximizing) precoder codebook index to the\ntransmitter using B bits. We first derive the large system capacity of\nbeamforming (rank-one precoding matrix) as a function of B, where large system\nrefers to the limit as B and the number of transmit and receive antennas all go\nto infinity with fixed ratios. With beamforming RVQ is asymptotically optimal,\ni.e., no other quantization scheme can achieve a larger asymptotic rate. The\nperformance of RVQ is also compared with that of a simpler reduced-rank scalar\nquantization scheme in which the beamformer is constrained to lie in a random\nsubspace. We subsequently consider a precoding matrix with arbitrary rank, and\napproximate the asymptotic RVQ performance with optimal and linear receivers\n(matched filter and Minimum Mean Squared Error (MMSE)). Numerical examples show\nthat these approximations accurately predict the performance of finite-size\nsystems of interest. Given a target spectral efficiency, numerical examples\nshow that the amount of feedback required by the linear MMSE receiver is only\nslightly more than that required by the optimal receiver, whereas the matched\nfilter can require significantly more feedback.\n", "category": [0, 2]}
{"abstract": "  In some particular cases we give criteria for morphic sequences to be almost\nperiodic (=uniformly recurrent). Namely, we deal with fixed points of\nnon-erasing morphisms and with automatic sequences. In both cases a\npolynomial-time algorithm solving the problem is found. A result more or less\nsupporting the conjecture of decidability of the general problem is given.\n", "category": [0, 0]}
{"abstract": "  Two-particle correlations have shown modification to the away-side shape in\ncentral Au+Au collisions relative to $pp$, d+Au and peripheral Au+Au\ncollisions. Different scenarios can explain this modification including: large\nangle gluon radiation, jets deflected by transverse flow, path length dependent\nenergy loss, Cerenkov gluon radiation of fast moving particles, and conical\nflow generated by hydrodynamic Mach-cone shock-waves. Three-particle\ncorrelations have the power to distinguish the scenarios with conical emission,\nconical flow and Cerenkov radiation, from other scenarios. In addition, the\ndependence of the observed shapes on the $p_T$ of the associated particles can\nbe used to distinguish conical emission from a sonic boom (Mach-cone) and from\nQCD-Cerenkov radiation. We present results from STAR on 3-particle azimuthal\ncorrelations for a high $p_T$ trigger particle with two softer particles.\nResults are shown for $pp$, d+Au and high statistics Au+Au collisions at\n$\\sqrt{s_{NN}}$=200 GeV. An important aspect of the analysis is the subtraction\nof combinatorial backgrounds. Systematic uncertainties due to this subtraction\nand the flow harmonics v2 and v4 are investigated in detail. The implications\nof the results for the presence or absence of conical flow from Mach-cones are\ndiscussed.\n", "category": [3]}
{"abstract": "  We demonstrate that as we extrapolate the current $\\Lambda$CDM universe\nforward in time, all evidence of the Hubble expansion will disappear, so that\nobservers in our \"island universe\" will be fundamentally incapable of\ndetermining the true nature of the universe, including the existence of the\nhighly dominant vacuum energy, the existence of the CMB, and the primordial\norigin of light elements. With these pillars of the modern Big Bang gone, this\nepoch will mark the end of cosmology and the return of a static universe. In\nthis sense, the coordinate system appropriate for future observers will perhaps\nfittingly resemble the static coordinate system in which the de Sitter universe\nwas first presented.\n", "category": [3, 3, 3]}
{"abstract": "  We present results from STAR on 3-particle azimuthal correlations for a\n$3<p_T<4$ GeV/c trigger particle with two softer $1<p_T<2$ GeV/c particles.\nResults are shown for pp, d+Au and high statistics Au+Au collisions at\n$\\sqrt{s_{NN}}=200 GeV$. We observe a 3-particle correlation in central Au+Au\ncollisions which may indicate the presence of conical emission. In addition,\nthe dependence of the observed signal angular position on the $p_T$ of the\nassociated particles can be used to distinguish conical flow from simple\nQCD-\\v{C}erenkov radiation. An important aspect of the analysis is the\nsubtraction of combinatorial backgrounds. Systematic uncertainties due to this\nsubtraction and the flow harmonics $v_2$ and $v_4$ are investigated in detail.\n", "category": [3]}
{"abstract": "  A systematic study of isotopic effects in the break-up of projectile\nspectators at relativistic energies has been performed at the GSI laboratory\nwith the ALADiN spectrometer coupled to the LAND neutron detector. Besides a\nprimary beam of 124Sn, also secondary beams of 124La and 107Sn produced at the\nFRS fragment separator have been used in order to extend the range of isotopic\ncompositions. The gross properties of projectile fragmentation are very similar\nfor all the studied systems but specific isotopic effects have been observed in\nboth neutron and charged particle production. The breakup temperatures obtained\nfrom the double ratios of isotopic yields have been extracted and compared with\nthe limiting-temperature expectation.\n", "category": [3]}
{"abstract": "  Starting with a field theoretic approach in Minkowski space, the\ngravitational energy momentum tensor is derived from the Einstein equations in\na straightforward manner. This allows to present them as {\\it acceleration\ntensor} = const. $\\times$ {\\it total energy momentum tensor}. For flat space\ncosmology the gravitational energy is negative and cancels the material energy.\nIn the relativistic theory of gravitation a bimetric coupling between the\nRiemann and Minkowski metrics breaks general coordinate invariance. The case of\na positive cosmological constant is considered. A singularity free version of\nthe Schwarzschild black hole is solved analytically. In the interior the\ncomponents of the metric tensor quickly die out, but do not change sign,\nleaving the role of time as usual. For cosmology the $\\Lambda$CDM model is\ncovered, while there appears a form of inflation at early times. Here both the\ntotal energy and the zero point energy vanish.\n", "category": [3]}
{"abstract": "  This article belongs to a series on geometric complexity theory (GCT), an\napproach to the P vs. NP and related problems through algebraic geometry and\nrepresentation theory. The basic principle behind this approach is called the\nflip. In essence, it reduces the negative hypothesis in complexity theory (the\nlower bound problems), such as the P vs. NP problem in characteristic zero, to\nthe positive hypothesis in complexity theory (the upper bound problems):\nspecifically, to showing that the problems of deciding nonvanishing of the\nfundamental structural constants in representation theory and algebraic\ngeometry, such as the well known plethysm constants--or rather certain relaxed\nforms of these decision probelms--belong to the complexity class P. In this\narticle, we suggest a plan for implementing the flip, i.e., for showing that\nthese relaxed decision problems belong to P. This is based on the reduction of\nthe preceding complexity-theoretic positive hypotheses to mathematical\npositivity hypotheses: specifically, to showing that there exist positive\nformulae--i.e. formulae with nonnegative coefficients--for the structural\nconstants under consideration and certain functions associated with them. These\nturn out be intimately related to the similar positivity properties of the\nKazhdan-Lusztig polynomials and the multiplicative structural constants of the\ncanonical (global crystal) bases in the theory of Drinfeld-Jimbo quantum\ngroups. The known proofs of these positivity properties depend on the Riemann\nhypothesis over finite fields and the related results. Thus the reduction here,\nin conjunction with the flip, in essence, says that the validity of the P vs.\nNP conjecture in characteristic zero is intimately linked to the Riemann\nhypothesis over finite fields and related problems.\n", "category": [0]}
{"abstract": "  We provide a description of the interpolating and sampling sequences on a\nspace of holomorphic functions with a uniform growth restriction defined on\nfinite Riemann surfaces.\n", "category": [2]}
{"abstract": "  In this expository article we review recent advances in our understanding of\nthe combinatorial and algebraic structure of perturbation theory in terms of\nFeynman graphs, and Dyson-Schwinger equations. Starting from Lie and Hopf\nalgebras of Feynman graphs, perturbative renormalization is rephrased\nalgebraically. The Hochschild cohomology of these Hopf algebras leads the way\nto Slavnov-Taylor identities and Dyson-Schwinger equations. We discuss recent\nprogress in solving simple Dyson-Schwinger equations in the high energy sector\nusing the algebraic machinery. Finally there is a short account on a relation\nto algebraic geometry and number theory: understanding Feynman integrals as\nperiods of mixed (Tate) motives.\n", "category": [3]}
{"abstract": "  A perturbative model is studied for the tunneling of many-particle states\nfrom the ground band to the first excited energy band, mimicking Landau-Zener\ndecay for ultracold, spinless atoms in quasi-one dimensional optical lattices\nsubjected to a tunable tilting force. The distributions of the computed\ntunneling rates provide an independent and experimentally accessible signature\nof the regular-chaotic transition in the strongly correlated many-body dynamics\nof the ground band.\n", "category": [3]}
{"abstract": "  Apart from its mass and width, the most important property of a new charged\ngauge boson, $W'$, is the helicity of its couplings to the SM fermions. Such\nparticles are expected to exist in many extensions of the Standard Model. In\nthis paper we explore the capability of the LHC to determine the $W'$ coupling\nhelicity at low integrated luminosities in the $\\ell +E_T^{miss}$ discovery\nchannel. We find that measurements of the transverse mass distribution,\nreconstructed from this final state in the $W-W'$ interference region, provides\nthe best determination of this quantity. To make such measurements requires\nintegrated luminosities of $\\sim 10(60) fb^{-1}$ assuming $M_{W'}=1.5(2.5)$ TeV\nand provided that the $W'$ couplings have Standard Model magnitude. This\nhelicity determination can be further strengthened by the use of various\ndiscovery channel leptonic asymmetries, also measured in the same interference\nregime, but with higher integrated luminosities.\n", "category": [3, 3]}
{"abstract": "  We prove that the limit hypersurfaces of converging curvature flows are\nstable, if the initial velocity has a weak sign, and give a survey of the\nexistence and regularity results.\n", "category": [2]}
{"abstract": "  An analytical description of the interface motion of a collapsing\nnanometer-sized spherical cavity in water is presented by a modification of the\nRayleigh-Plesset equation in conjunction with explicit solvent molecular\ndynamics simulations. Quantitative agreement is found between the two\napproaches for the time-dependent cavity radius $R(t)$ at different solvent\nconditions while in the continuum picture the solvent viscosity has to be\ncorrected for curvature effects. The typical magnitude of the interface or\ncollapse velocity is found to be given by the ratio of surface tension and\nfluid viscosity, $v\\simeq\\gamma/\\eta$, while the curvature correction\naccelerates collapse dynamics on length scales below the equilibrium crossover\nscales ($\\sim$1nm). The study offers a starting point for an efficient implicit\nmodeling of water dynamics in aqueous nanoassembly and protein systems in\nnonequilibrium.\n", "category": [3]}
{"abstract": "  We review recent progress in applying the AdS/CFT correspondence to\nfinite-temperature field theory. In particular, we show how the hydrodynamic\nbehavior of field theory is reflected in the low-momentum limit of correlation\nfunctions computed through a real-time AdS/CFT prescription, which we\nformulate. We also show how the hydrodynamic modes in field theory correspond\nto the low-lying quasinormal modes of the AdS black p-brane metric. We provide\na proof of the universality of the viscosity/entropy ratio within a class of\ntheories with gravity duals and formulate a viscosity bound conjecture.\nPossible implications for real systems are mentioned.\n", "category": [3]}
{"abstract": "  I investigate superconducting states in a quasi-2D Holstein model using the\ndynamical cluster approximation (DCA). The effects of spatial fluctuations\n(non-local corrections) are examined and approximations neglecting and\nincorporating lowest-order vertex corrections are computed. The approximation\nis expected to be valid for electron-phonon couplings of less than the\nbandwidth. The phase diagram and superconducting order parameter are\ncalculated. Effects which can only be attributed to theories beyond\nMigdal--Eliashberg theory are present. In particular, the order parameter shows\nmomentum dependence on the Fermi-surface with a modulated form and s-wave order\nis suppressed at half-filling. The results are discussed in relation to\nHohenberg's theorem and the BCS approximation.\n", "category": [3]}
{"abstract": "  We analyze the properties of the quasiparticle excitations of metallic\nantiferromagnetic states in a strongly correlated electron system. The study is\nbased on dynamical mean field theory (DMFT) for the infinite dimensional\nHubbard model with antiferromagnetic symmetry breaking. Self-consistent\nsolutions of the DMFT equations are calculated using the numerical\nrenormalization group (NRG). The low energy behavior in these results is then\nanalyzed in terms of renormalized quasiparticles. The parameters for these\nquasiparticles are calculated directly from the NRG derived self-energy, and\nalso from the low energy fixed point of the effective impurity. They are found\nto be in good agreement. We show that the main low energy features of the $\\bf\nk$-resolved spectral density can be understood in terms of the quasiparticle\npicture. We also find that Luttinger's theorem is satisfied for the total\nelectron number in the doped antiferromagnetic state.\n", "category": [3]}
{"abstract": "  The relative merits of current-spin-density- and spin-density-functional\ntheory are investigated for solids treated within the exact-exchange-only\napproximation. Spin-orbit splittings and orbital magnetic moments are\ndetermined at zero external magnetic field. We find that for magnetic (Fe, Co\nand Ni) and non-magnetic (Si and Ge) solids, the exact-exchange\ncurrent-spin-density functional approach does not significantly improve the\naccuracy of the corresponding spin-density functional results.\n", "category": [3]}
{"abstract": "  It has been known for some time that the standard MHV diagram formulation of\nperturbative Yang-Mills theory is incomplete, as it misses rational terms in\none-loop scattering amplitudes of pure Yang-Mills. We propose that certain\nLorentz violating counterterms, when expressed in the field variables which\ngive rise to standard MHV vertices, produce precisely these missing terms.\nThese counterterms appear when Yang-Mills is treated with a regulator,\nintroduced by Thorn and collaborators, which arises in worldsheet formulations\nof Yang-Mills theory in the lightcone gauge. As an illustration of our\nproposal, we show that a simple one-loop, two-point counterterm is the\ngenerating function for the infinite sequence of one-loop, all-plus helicity\namplitudes in pure Yang-Mills, in complete agreement with known expressions.\n", "category": [3, 3]}
{"abstract": "  There is currently a great deal of interest in the theoretical and practical\npossibility of cloaking objects from the observation by electromagnetic waves.\nThe basic idea of these invisibility devices \\cite{glu1, glu2, le},\\cite{pss1}\nis to use anisotropic {\\it transformation media} whose permittivity and\npermeability $\\var^{\\lambda\\nu}, \\mu^{\\lambda\\nu}$, are obtained from the ones,\n$\\var_0^{\\lambda\\nu}, \\mu^{\\lambda\\nu}_0$, of isotropic media, by singular\ntransformations of coordinates. In this paper we study electromagnetic cloaking\nin the time-domain using the formalism of time-dependent scattering theory.\nThis formalism allows us to settle in an unambiguous way the mathematical\nproblems posed by the singularities of the inverse of the permittivity and the\npermeability of the {\\it transformation media} on the boundary of the cloaked\nobjects. We write Maxwell's equations in Schr\\\"odinger form with the\nelectromagnetic propagator playing the role of the Hamiltonian. We prove that\nthe electromagnetic propagator outside of the cloaked objects is essentially\nself-adjoint. Moreover, the unique self-adjoint extension is unitarily\nequivalent to the electromagnetic propagator in the medium\n$\\var_0^{\\lambda\\nu}, \\mu^{\\lambda\\nu}_0$. Using this fact, and since the\ncoordinate transformation is the identity outside of a ball, we prove that the\nscattering operator is the identity. Our results give a rigorous proof that the\nconstruction of \\cite{glu1, glu2, le}, \\cite{pss1} perfectly cloaks passive and\nactive devices from observation by electromagnetic waves. Furthermore, we prove\ncloaking for general anisotropic materials. In particular, our results prove\nthat it is possible to cloak objects inside general crystals.\n", "category": [3, 2]}
{"abstract": "  These notes accompany a lecture about the topology of symplectic (and other)\nquotients. The aim is two-fold: first to advertise the ease of computation in\nthe symplectic category; and second to give an account of some new computations\nfor weighted projective spaces. We start with a brief exposition of how\norbifolds arise in the symplectic category, and discuss the techniques used to\nunderstand their topology. We then show how these results can be used to\ncompute the Chen-Ruan orbifold cohomology ring of abelian symplectic\nreductions. We conclude by comparing the several rings associated to a weighted\nprojective space. We make these computations directly, avoiding any mention of\na stacky fan or of a labeled moment polytope.\n", "category": [2, 2]}
{"abstract": "  In trigonometric series terms all polyharmonic functions inside the unit disk\nare described. For such functions it is proved the existence of their boundary\nvalues on the unit circle in the space of hyperfunctions. The necessary and\nsufficient conditions are presented for the boundary value to belong to certain\nsubspaces of the space of hyperfunctions.\n", "category": [2, 2]}
{"abstract": "  We present some numerical results obtained from a simple individual based\nmodel that describes clustering of organisms caused by competition. Our aim is\nto show how, even when a deterministic description developed for continuum\nmodels predicts no pattern formation, an individual based model displays well\ndefined patterns, as a consequence of fluctuations effects caused by the\ndiscrete nature of the interacting agents.\n", "category": [3, 4]}
{"abstract": "  In a previous paper, we showed how certain orientations of the edges of a\ngraph G embedded in a closed oriented surface S can be understood as discrete\nspin structures on S. We then used this correspondence to give a geometric\nproof of the Pfaffian formula for the partition function of the dimer model on\nG. In the present article, we generalize these results to the case of compact\noriented surfaces with boundary. We also show how the operations of cutting and\ngluing act on discrete spin structures and how they change the partition\nfunction. These operations allow to reformulate the dimer model as a quantum\nfield theory on surface graphs.\n", "category": [2, 2]}
{"abstract": "  It is known that every closed curve of length \\leq 4 in R^n (n>0) can be\nsurrounded by a sphere of radius 1, and that this is the best bound. Letting S\ndenote the circle of circumference 4, with the arc-length metric, we here\nexpress this fact by saying that the \"mapping radius\" of S in R^n is 1.\n  Tools are developed for estimating the mapping radius of a metric space X in\na metric space Y. In particular, it is shown that for X a bounded metric space,\nthe supremum of the mapping radii of X in all convex subsets of normed metric\nspaces is equal to the infimum of the sup norms of all convex linear\ncombinations of the functions d(x,-): X --> R (x\\in X).\n  Several explicit mapping radii are calculated, and open questions noted.\n", "category": [2]}
{"abstract": "  Let X be a simplicial complex on the vertex set V. The rational Leray number\nL(X) of X is the minimal d such that the rational reduced homology of any\ninduced subcomplex of X vanishes in dimensions d and above. Let \\pi be a\nsimplicial map from X to a simplex Y, such that the cardinality of the preimage\nof any point in |Y| is at most r. It is shown that L(\\pi(X)) \\leq r L(X)+r-1.\nOne consequence is a topological extension of a Helly type result of Amenta.\n", "category": [2]}
{"abstract": "  This paper considers the use of punctured convolutional codes to obtain\npragmatic space-time trellis codes over block-fading channel. We show that good\nperformance can be achieved even when puncturation is adopted and that we can\nstill employ the same Viterbi decoder of the convolutional mother code by using\napproximated metrics without increasing the complexity of the decoding\noperations.\n", "category": [0, 0, 2]}
{"abstract": "  We show that there is a unique Markov trace on the tower of Temperley--Lieb\ntype quotients of Hecke algebras of Coxeter type $E_n$ (for all $n \\geq 6$). We\nexplain in detail how this trace may be computed easily using tom Dieck's\ncalculus of diagrams. As applications, we show how to use the trace to show\nthat the diagram representation is faithful, and to compute leading\ncoefficients of certain Kazhdan--Lusztig polynomials.\n", "category": [2]}
{"abstract": "  The paper presents a survey of mathematical problems, techniques, and\nchallenges arising in the Thermoacoustic and Photoacoustic Tomography.\n", "category": [2, 2]}
{"abstract": "  Let ${\\mathcal S}(\\R)$ be an o-minimal structure over $\\R$, $T \\subset\n\\R^{k_1+k_2+\\ell}$ a closed definable set, and $$ \\displaylines{\\pi_1:\n\\R^{k_1+k_2+\\ell}\\to \\R^{k_1 + k_2}, \\pi_2: \\R^{k_1+k_2+\\ell}\\to \\R^{\\ell}, \\\n\\pi_3: \\R^{k_1 + k_2} \\to \\R^{k_2}} $$ the projection maps.\n  For any collection ${\\mathcal A} = \\{A_1,...,A_n\\}$ of subsets of\n$\\R^{k_1+k_2}$, and $\\z \\in \\R^{k_2}$, let $\\A_\\z$ denote the collection of\nsubsets of $\\R^{k_1}$, $\\{A_{1,\\z},..., A_{n,\\z}\\}$, where $A_{i,\\z} = A_i \\cap\n\\pi_3^{-1}(\\z), 1 \\leq i \\leq n$. We prove that there exists a constant $C =\nC(T) > 0,$ such that for any family ${\\mathcal A} = \\{A_1,...,A_n\\}$ of\ndefinable sets, where each $A_i = \\pi_1(T \\cap \\pi_2^{-1}(\\y_i))$, for some\n$\\y_i \\in \\R^{\\ell}$, the number of distinct stable homotopy types of $\\A_\\z,\n\\z \\in \\R^{k_2}$, is bounded by $ \\displaystyle{C \\cdot n^{(k_1+1)k_2},} $\nwhile the number of distinct homotopy types is bounded by $ \\displaystyle{C\n\\cdot n^{(k_1+3)k_2}.} $ This generalizes to the general o-minimal setting,\nbounds of the same type proved in \\cite{BV} for semi-algebraic and\nsemi-Pfaffian families. One main technical tool used in the proof of the above\nresults, is a topological comparison theorem which might be of independent\ninterest in the study of arrangements.\n", "category": [2, 2]}
{"abstract": "  For an arbitrary operator A on a Banach space X which is a generator of\nC_0-group with certain growth condition at the infinity, the direct theorems on\nconnection between the smoothness degree of a vector $x\\in X$ with respect to\nthe operator A, the order of convergence to zero of the best approximation of x\nby exponential type entire vectors for the operator A, and the k-module of\ncontinuity are given. Obtained results allows to acquire Jackson-type\ninequalities in many classic spaces of periodic functions and weighted $L_p$\nspaces.\n", "category": [2, 2]}
{"abstract": "  Moore introduced a class of real-valued \"recursive\" functions by analogy with\nKleene's formulation of the standard recursive functions. While his concise\ndefinition inspired a new line of research on analog computation, it contains\nsome technical inaccuracies. Focusing on his \"primitive recursive\" functions,\nwe pin down what is problematic and discuss possible attempts to remove the\nambiguity regarding the behavior of the differential recursion operator on\npartial functions. It turns out that in any case the purported relation to\ndifferentially algebraic functions, and hence to Shannon's model of analog\ncomputation, fails.\n", "category": [0]}
{"abstract": "  For the past two decades, single-index model, a special case of projection\npursuit regression, has proven to be an efficient way of coping with the high\ndimensional problem in nonparametric regression. In this paper, based on weakly\ndependent sample, we investigate the single-index prediction (SIP) model which\nis robust against deviation from the single-index model. The single-index is\nidentified by the best approximation to the multivariate prediction function of\nthe response variable, regardless of whether the prediction function is a\ngenuine single-index function. A polynomial spline estimator is proposed for\nthe single-index prediction coefficients, and is shown to be root-n consistent\nand asymptotically normal. An iterative optimization routine is used which is\nsufficiently fast for the user to analyze large data of high dimension within\nseconds. Simulation experiments have provided strong evidence that corroborates\nwith the asymptotic theory. Application of the proposed procedure to the rive\nflow data of Iceland has yielded superior out-of-sample rolling forecasts.\n", "category": [2, 6]}
{"abstract": "  This paper discusses the benefits of describing the world as information,\nespecially in the study of the evolution of life and cognition. Traditional\nstudies encounter problems because it is difficult to describe life and\ncognition in terms of matter and energy, since their laws are valid only at the\nphysical scale. However, if matter and energy, as well as life and cognition,\nare described in terms of information, evolution can be described consistently\nas information becoming more complex.\n  The paper presents eight tentative laws of information, valid at multiple\nscales, which are generalizations of Darwinian, cybernetic, thermodynamic,\npsychological, philosophical, and complexity principles. These are further used\nto discuss the notions of life, cognition and their evolution.\n", "category": [0, 0, 2, 4]}
{"abstract": "  The Hamiltonian cycle problem (HCP) in digraphs D with degree bound two is\nsolved by two mappings in this paper. The first bijection is between an\nincidence matrix C_{nm} of simple digraph and an incidence matrix F of balanced\nbipartite undirected graph G; The second mapping is from a perfect matching of\nG to a cycle of D. It proves that the complexity of HCP in D is polynomial, and\nfinding a second non-isomorphism Hamiltonian cycle from a given Hamiltonian\ndigraph with degree bound two is also polynomial. Lastly it deduces P=NP base\non the results.\n", "category": [0, 0]}
{"abstract": "  We lead the algorithm of expansion of sojourn probability of many-dimensional\ndiffusion processes in small domain. The principal member of this expansion\ndefines normalizing coefficient for special limit theorems.\n", "category": [2, 2]}
{"abstract": "  We show that if a complete Riemannian manifold supports a vector field such\nthat the Ricci tensor plus the Lie derivative of the metric with respect to the\nvector field has a positive lower bound, then the fundamental group is finite.\nIn particular, it follows that complete shrinking Ricci solitons and complete\nsmooth metric measure spaces with a positive lower bound on the Bakry-Emery\ntensor have finite fundamental group. The method of proof is to generalize\narguments of Garcia-Rio and Fernandez-Lopez in the compact case.\n", "category": [2]}
{"abstract": "  Alexander B. Medvinsky \\emph{et al} [A. B. Medvinsky, I. A. Tikhonova, R. R.\nAliev, B.-L. Li, Z.-S. Lin, and H. Malchow, Phys. Rev. E \\textbf{64}, 021915\n(2001)] and Marcus R. Garvie \\emph{et al} [M. R. Garvie and C. Trenchea, SIAM\nJ. Control. Optim. \\textbf{46}, 775-791 (2007)] shown that the minimal\nspatially extended reaction-diffusion model of phytoplankton-zooplankton can\nexhibit both regular, chaotic behavior, and spatiotemporal patterns in a patchy\nenvironment. Based on that, the spatial plankton model is furtherly\ninvestigated by means of computer simulations and theoretical analysis in the\npresent paper when its parameters would be expected in the case of mixed\nTuring-Hopf bifurcation region. Our results show that the spiral waves exist in\nthat region and the spatiotemporal chaos emerge, which arise from the far-field\nbreakup of the spiral waves over large ranges of diffusion coefficients of\nphytoplankton and zooplankton. Moreover, the spatiotemporal chaos arising from\nthe far-field breakup of spiral waves does not gradually involve the whole\nspace within that region. Our results are confirmed by means of computation\nspectra and nonlinear bifurcation of wave trains. Finally, we give some\nexplanations about the spatially structured patterns from the community level.\n", "category": [3, 3, 4]}
{"abstract": "  We study the pseudospectrum of a class of non-selfadjoint differential\noperators. Our work consists in a detailed study of the microlocal properties,\nwhich rule the spectral stability or instability phenomena appearing under\nsmall perturbations for elliptic quadratic differential operators. The class of\nelliptic quadratic differential operators stands for the class of operators\ndefined in the Weyl quantization by complex-valued elliptic quadratic symbols.\nWe establish in this paper a simple necessary and sufficient condition on the\nWeyl symbol of these operators, which ensures the stability of their spectra.\nWhen this condition is violated, we prove that it occurs some strong spectral\ninstabilities for the high energies of these operators, in some regions which\ncan be far away from their spectra. We give a precise geometrical description\nof them, which explains the results obtained for these operators in some\nnumerical simulations giving the computation of false eigenvalues far from\ntheir spectra by algorithms for eigenvalues computing.\n", "category": [2]}
{"abstract": "  Product probability property, known in the literature as statistical\nindependence, is examined first. Then generalized entropies are introduced, all\nof which give generalizations to Shannon entropy. It is shown that the nature\nof the recursivity postulate automatically determines the logarithmic\nfunctional form for Shannon entropy. Due to the logarithmic nature, Shannon\nentropy naturally gives rise to additivity, when applied to situations having\nproduct probability property. It is argued that the natural process is\nnon-additivity, important, for example, in statistical mechanics, even in\nproduct probability property situations and additivity can hold due to the\ninvolvement of a recursivity postulate leading to a logarithmic function.\nGeneralizations, including Mathai's generalized entropy are introduced and some\nof the properties are examined. Situations are examined where Mathai's entropy\nleads to pathway models, exponential and power law behavior and related\ndifferential equations. Connection of Mathai's entropy to Kerridge's measure of\n\"inaccuracy\" is also explored.\n", "category": [2, 3, 6]}
{"abstract": "  This paper deals with the investigation of the solution of an unified\nfractional reaction-diffusion equation associated with the Caputo derivative as\nthe time-derivative and Riesz-Feller fractional derivative as the\nspace-derivative. The solution is derived by the application of the Laplace and\nFourier transforms in closed form in terms of the H-function. The results\nderived are of general nature and include the results investigated earlier by\nmany authors, notably by Mainardi et al. (2001, 2005) for the fundamental\nsolution of the space-time fractional diffusion equation, and Saxena et al.\n(2006a, b) for fractional reaction- diffusion equations. The advantage of using\nRiesz-Feller derivative lies in the fact that the solution of the fractional\nreaction-diffusion equation containing this derivative includes the fundamental\nsolution for space-time fractional diffusion, which itself is a generalization\nof neutral fractional diffusion, space-fractional diffusion, and\ntime-fractional diffusion. These specialized types of diffusion can be\ninterpreted as spatial probability density functions evolving in time and are\nexpressible in terms of the H-functions in compact form.\n", "category": [2, 2, 2, 6]}
{"abstract": "  This letter reports complete sets of two-fold symmetries between partitions\nof the universal genetic code. By substituting bases at each position of the\ncodons according to a fixed rule, it happens that properties of the degeneracy\npattern or of tRNA aminoacylation specificity are exchanged.\n", "category": [4]}
{"abstract": "  We build a sequence of empirical measures on the space D(R_+,R^d) of\nR^d-valued c\\`adl\\`ag functions on R_+ in order to approximate the law of a\nstationary R^d-valued Markov and Feller process (X_t). We obtain some general\nresults of convergence of this sequence. Then, we apply them to Brownian\ndiffusions and solutions to L\\'evy driven SDE's under some Lyapunov-type\nstability assumptions. As a numerical application of this work, we show that\nthis procedure gives an efficient way of option pricing in stochastic\nvolatility models.\n", "category": [2, 5, 5]}
{"abstract": "  A class of three-dimensional initial data characterized by uniformly large\nvorticity is considered for the Euler equations of incompressible fluids. The\nfast singular oscillating limits of the Euler equations are studied for\nparametrically resonant cylinders. Resonances of fast swirling Beltrami waves\ndeplete the Euler nonlinearity. The resonant Euler equations are systems of\nthree-dimensional rigid body equations, coupled or not. Some cases of these\nresonant systems have homoclinic cycles, and orbits in the vicinity of these\nhomoclinic cycles lead to bursts of the Euler solution measured in Sobolev\nnorms of order higher than that corresponding to the enstrophy.\n", "category": [2]}
{"abstract": "  Cofibrations are defined in the category of Fr\\\"olicher spaces by weakening\nthe analog of the classical definition to enable smooth homotopy extensions to\nbe more easily constructed, using flattened unit intervals. We later relate\nsmooth cofibrations to smooth neighborhood deformation retracts. The notion of\nsmooth neighborhood deformation retract gives rise to an analogous result that\na closed Fr\\\"olicher subspace $A$ of the Fr\\\"olicher space $X$ is a smooth\nneighborhood deformation retract of $X$ if and only if the inclusion $i:\nA\\hookrightarrow X$ comes from a certain subclass of cofibrations. As an\napplication we construct the right Puppe sequence.\n", "category": [2]}
{"abstract": "  We present a simple proof of the resolvent estimates of elliptic Fourier\nmultipliers on the Euclidean space, and apply them to the analysis of\ntime-global and spatially-local smoothing estimates of a class of dispersive\nequations. For this purpose we study in detail the properties of the\nrestriction of Fourier transform on the unit cotangent sphere associated with\nthe symbols of multipliers.\n", "category": [2, 2]}
{"abstract": "  The Colin de Verdi\\`ere number $\\mu(G)$ of a graph $G$ is the maximum corank\nof a Colin de Verdi\\`ere matrix for $G$ (that is, of a Schr\\\"odinger operator\non $G$ with a single negative eigenvalue). In 2001, Lov\\'asz gave a\nconstruction that associated to every convex 3-polytope a Colin de Verdi\\`ere\nmatrix of corank 3 for its 1-skeleton.\n  We generalize the Lov\\'asz construction to higher dimensions by interpreting\nit as minus the Hessian matrix of the volume of the polar dual. As a corollary,\n$\\mu(G) \\ge d$ if $G$ is the 1-skeleton of a convex $d$-polytope.\n  Determination of the signature of the Hessian of the volume is based on the\nsecond Minkowski inequality for mixed volumes and on Bol's condition for\nequality.\n", "category": [2, 2]}
{"abstract": "  Population structure induced by both spatial embedding and more general\nnetworks of interaction, such as model social networks, have been shown to have\na fundamental effect on the dynamics and outcome of evolutionary games. These\neffects have, however, proved to be sensitive to the details of the underlying\ntopology and dynamics. Here we introduce a minimal population structure that is\ndescribed by two distinct hierarchical levels of interaction. We believe this\nmodel is able to identify effects of spatial structure that do not depend on\nthe details of the topology. We derive the dynamics governing the evolution of\na system starting from fundamental individual level stochastic processes\nthrough two successive meanfield approximations. In our model of population\nstructure the topology of interactions is described by only two parameters: the\neffective population size at the local scale and the relative strength of local\ndynamics to global mixing. We demonstrate, for example, the existence of a\ncontinuous transition leading to the dominance of cooperation in populations\nwith hierarchical levels of unstructured mixing as the benefit to cost ratio\nbecomes smaller then the local population size. Applying our model of spatial\nstructure to the repeated prisoner's dilemma we uncover a novel and\ncounterintuitive mechanism by which the constant influx of defectors sustains\ncooperation. Further exploring the phase space of the repeated prisoner's\ndilemma and also of the \"rock-paper-scissor\" game we find indications of rich\nstructure and are able to reproduce several effects observed in other models\nwith explicit spatial embedding, such as the maintenance of biodiversity and\nthe emergence of global oscillations.\n", "category": [4, 4]}
{"abstract": "  In this article we will first prove a result about convergence in capacity.\nUsing the achieved result we will obtain a general decompositon theorem for\ncomplex Monge-Ampere measues which will be used to prove a comparison principle\nfor the complex Monge-Ampere operator.\n", "category": [2]}
{"abstract": "  It has been observed that particular rate-1/2 partially systematic parallel\nconcatenated convolutional codes (PCCCs) can achieve a lower error floor than\nthat of their rate-1/3 parent codes. Nevertheless, good puncturing patterns can\nonly be identified by means of an exhaustive search, whilst convergence towards\nlow bit error probabilities can be problematic when the systematic output of a\nrate-1/2 partially systematic PCCC is heavily punctured. In this paper, we\npresent and study a family of rate-1/2 partially systematic PCCCs, which we\ncall pseudo-randomly punctured codes. We evaluate their bit error rate\nperformance and we show that they always yield a lower error floor than that of\ntheir rate-1/3 parent codes. Furthermore, we compare analytic results to\nsimulations and we demonstrate that their performance converges towards the\nerror floor region, owning to the moderate puncturing of their systematic\noutput. Consequently, we propose pseudo-random puncturing as a means of\nimproving the bandwidth efficiency of a PCCC and simultaneously lowering its\nerror floor.\n", "category": [0, 2]}
{"abstract": "  The problem of limit shapes in the six-vertex model with domain wall boundary\nconditions is addressed by considering a specially tailored bulk correlation\nfunction, the emptiness formation probability. A closed expression of this\ncorrelation function is given, both in terms of certain determinant and\nmultiple integral, which allows for a systematic treatment of the limit shapes\nof the model for full range of values of vertex weights. Specifically, we show\nthat for vertex weights corresponding to the free-fermion line on the phase\ndiagram, the emptiness formation probability is related to a one-matrix model\nwith a triple logarithmic singularity, or Triple Penner model. The saddle-point\nanalysis of this model leads to the Arctic Circle Theorem, and its\ngeneralization to the Arctic Ellipses, known previously from domino tilings.\n", "category": [3, 2]}
{"abstract": "  We study the limiting eigenvalue distribution of $n\\times n$ banded Toeplitz\nmatrices as $n\\to \\infty$. From classical results of Schmidt-Spitzer and\nHirschman it is known that the eigenvalues accumulate on a special curve in the\ncomplex plane and the normalized eigenvalue counting measure converges weakly\nto a measure on this curve as $n\\to\\infty$. In this paper, we characterize the\nlimiting measure in terms of an equilibrium problem. The limiting measure is\none component of the unique vector of measures that minimes an energy\nfunctional defined on admissible vectors of measures. In addition, we show that\neach of the other components is the limiting measure of the normalized counting\nmeasure on certain generalized eigenvalues.\n", "category": [2, 2]}
{"abstract": "  We study the high temperature phase of a family of typed branching diffusions\ninitially studied in [Ast\\'{e}risque 236 (1996) 133--154] and [Lecture Notes in\nMath. 1729 (2000) 239--256 Springer, Berlin]. The primary aim is to establish\nsome almost-sure limit results for the long-term behavior of this particle\nsystem, namely the speed at which the population of particles colonizes both\nspace and type dimensions, as well as the rate at which the population grows\nwithin this asymptotic shape. Our approach will include identification of an\nexplicit two-phase mechanism by which particles can build up in sufficient\nnumbers with spatial positions near $-\\gamma t$ and type positions near $\\kappa\n\\sqrt{t}$ at large times $t$. The proofs involve the application of a variety\nof martingale techniques--most importantly a ``spine'' construction involving a\nchange of measure with an additive martingale. In addition to the model's\nintrinsic interest, the methodologies presented contain ideas that will adapt\nto other branching settings. We also briefly discuss applications to traveling\nwave solutions of an associated reaction--diffusion equation.\n", "category": [2]}
{"abstract": "  A recent result of Balandraud shows that for every subset S of an abelian\ngroup G, there exists a non trivial subgroup H such that |TS| <= |T|+|S|-2\nholds only if the stabilizer of TS contains H. Notice that Kneser's Theorem\nsays only that the stabilizer of TS must be a non-zero subgroup.\n  This strong form of Kneser's theorem follows from some nice properties of a\ncertain poset investigated by Balandraud. We consider an analogous poset for\nnonabelian groups and, by using classical tools from Additive Number Theory,\nextend some of the above results. In particular we obtain short proofs of\nBalandraud's results in the abelian case.\n", "category": [2]}
{"abstract": "  We study the notion of Fagnano orbits for dual polygonal billiards. We used\nthem to characterize regular polygons and we study the iteration of the\ndeveloping map.\n", "category": [2]}
{"abstract": "  Structure entails function and thus a structural description of the brain\nwill help to understand its function and may provide insights into many\nproperties of brain systems, from their robustness and recovery from damage, to\ntheir dynamics and even their evolution. Advances in the analysis of complex\nnetworks provide useful new approaches to understanding structural and\nfunctional properties of brain networks. Structural properties of networks\nrecently described allow their characterization as small-world, random\n(exponential) and scale-free. They complement the set of other properties that\nhave been explored in the context of brain connectivity, such as topology,\nhodology, clustering, and hierarchical organization. Here we apply new network\nanalysis methods to cortical inter-areal connectivity networks for the cat and\nmacaque brains. We compare these corticocortical fibre networks to benchmark\nrewired, small-world, scale-free and random networks, using two analysis\nstrategies, in which we measure the effects of the removal of nodes and\nconnections on the structural properties of the cortical networks. The brain\nnetworks' structural decay is in most respects similar to that of scale-free\nnetworks. The results implicate highly connected hub-nodes and bottleneck\nconnections as structural basis for some of the conditional robustness of brain\nsystems. This informs the understanding of the development of brain networks'\nconnectivity.\n", "category": [4, 3, 3]}
{"abstract": "  This paper deals with discrete-time Markov control processes on a general\nstate space. A long-run risk-sensitive average cost criterion is used as a\nperformance measure. The one-step cost function is nonnegative and possibly\nunbounded. Using the vanishing discount factor approach, the optimality\ninequality and an optimal stationary strategy for the decision maker are\nestablished.\n", "category": [5, 2]}
{"abstract": "  We show that the number of renewals up to time $t$ exhibits distributional\nfluctuations as $t\\to\\infty$ if the underlying lifetimes increase at an\nexponential rate in a distributional sense. This provides a probabilistic\nexplanation for the asymptotics of insertion depth in random trees generated by\na bit-comparison strategy from uniform input; we also obtain a representation\nfor the resulting family of limit laws along subsequences. Our approach can\nalso be used to obtain rates of convergence.\n", "category": [2]}
{"abstract": "  In this paper we study the shape of least-energy solutions to a singularly\nperturbed quasilinear problem with homogeneous Neumann boundary condition. We\nuse an intrinsic variation method to show that at limit, the global maximum\npoint of least-energy solutions goes to a point on the boundary faster than the\nlinear rate and this point on the boundary approaches to a point where the mean\ncurvature of the boundary achieves its maximum. We also give a complete proof\nof exponential decay of least-energy solutions.\n", "category": [2]}
{"abstract": "  Semimartingale reflecting Brownian motions (SRBMs) living in the closures of\ndomains with piecewise smooth boundaries are of interest in applied probability\nbecause of their role as heavy traffic approximations for some stochastic\nnetworks. In this paper, assuming certain conditions on the domains and\ndirections of reflection, a perturbation result, or invariance principle, for\nSRBMs is proved. This provides sufficient conditions for a process that\nsatisfies the definition of an SRBM, except for small random perturbations in\nthe defining conditions, to be close in distribution to an SRBM. A crucial\ningredient in the proof of this result is an oscillation inequality for\nsolutions of a perturbed Skorokhod problem. We use the invariance principle to\nshow weak existence of SRBMs under mild conditions. We also use the invariance\nprinciple, in conjunction with known uniqueness results for SRBMs, to give some\nsufficient conditions for validating approximations involving (i) SRBMs in\nconvex polyhedrons with a constant reflection vector field on each face of the\npolyhedron, and (ii) SRBMs in bounded domains with piecewise smooth boundaries\nand possibly nonconstant reflection vector fields on the boundary surfaces.\n", "category": [2]}
{"abstract": "  It is shown that in the units of augmentation one of an integral group ring\n$\\mathbb{Z} G$ of a finite group $G$, a noncyclic subgroup of order $p^{2}$,\nfor some odd prime $p$, exists only if such a subgroup exists in $G$. The\ncorresponding statement for $p=2$ holds by the Brauer--Suzuki theorem, as\nrecently observed by W. Kimmerle.\n", "category": [2, 2]}
{"abstract": "  As main result we show that for each g > 1 there is some translation surface\nof genus g whose Veech group is a non congruence subgroup of SL(2,Z). We use\norigamis/square-tiled surfaces to produce our examples. The article is divided\ninto two parts: In the first part we introduce translation surfaces, origamis,\nVeech groups and Teichmueller curves and show for two origamis in genus 2 that\ntheir Veech groups are non congruence groups; in the second part we provide a\ntechnique that produces sequences of origamis whose Veech groups are\ndecreasing. This is used to prove the main result.\n", "category": [2, 2]}
{"abstract": "  Thermodynamic stable interaction pair potentials which are not of the form\n``positive function + real continuous function of positive type'' are presented\nin dimension one. Construction of such a potential in dimension two is\nsketched. These constructions use only elementary calculations. The\nmathematical background is discussed separately.\n", "category": [2]}
{"abstract": "  Is it possible to understand cancer? Or more specifically, is it possible to\nunderstand cancer from genetic side? There already many answers in literature.\nThe most optimistic one has claimed that it is mission-possible. Duesberg and\nhis colleagues reviewed the impressive amount of research results on cancer\naccumulated over 100 years. It confirms the a general opinion that considering\nall available experimental results and clinical observations there is no cancer\ntheory without major difficulties, including the prevailing gene-based cancer\ntheories. They have then listed 9 \"absolute discrepancies\" for such cancer\ntheory. In this letter the quantitative evidence against one of their major\nreasons for dismissing mutation cancer theory, by both in vivo experiment and a\nfirst principle computation, is explicitly pointed out.\n", "category": [4, 4]}
{"abstract": "  In this note we describe the natural coordinatizations of a Delzant space\ndefined as a reduced phase space (symplectic geometry view-point) and give\nexplicit formulas for the coordinate transformations. For each fixed point of\nthe torus action on the Delzant polytope, we have a maximal coordinatization of\nan open cell in the Delzant space which contains the fixed point. This cell is\nequal to the domain of definition of one of the natural coordinatizations of\nthe Delzant space as a toric variety (complex algebraic geometry view-point),\nand we give an explicit formula for the toric variety coordinates in terms of\nthe reduced phase space coordinates. We use considerations in the maximal\ncoordinate neighborhoods to give simple proofs of some of the basic facts about\nthe Delzant space, as a reduced phase space, and as a toric variety. These can\nbe viewed as a first application of the coordinatizations, and serve to make\nthe presentation more self-contained.\n", "category": [2, 2]}
{"abstract": "  These notes, based on the paper \"Formal Solution of the Master Equation via\nHPT and Deformation Theory\" by Huebschmann and Stasheff, were prepared for a\nseries of talks at Illinois State University with the intention of applying\nHomological Perturbation Theory to the derived bracket constructions of\nKosmann-Schwarzbach and T. Voronov, and eventually writing Part II of the paper\n\"Higher Derived Brackets and Deformation Theory I\" by the present authors.\n", "category": [2]}
{"abstract": "  We present a variational formulation of electrodynamics using de Rham even\nand odd differential forms. Our formulation relies on a variational principle\nmore complete than the Hamilton principle and thus leads to field equations\nwith external sources and permits the derivation of the constitutive relations.\nWe interpret a domain in space-time as an odd de Rham 4-current. This permits a\ntreatment of different types of boundary problems in an unified way. In\nparticular we obtain a smooth transition to the infinitesimal version by using\na current with a one point support.\n", "category": [2, 2]}
{"abstract": "  A projective surface S is said to be isogenous to a product if there exist\ntwo smooth curves C, F and a finite group G acting freely on C \\times F so that\nS=(C \\times F)/G. In this paper we classify all surfaces with p_g=q=1 which are\nisogenous to a product.\n", "category": [2, 2]}
{"abstract": "  Let $\\Gamma =(V,E)$ be a point-symmetric reflexive relation and let $v\\in V$\nsuch that\n  $|\\Gamma (v)|$ is finite (and hence $|\\Gamma (x)|$ is finite for all $x$, by\nthe transitive action of the group of automorphisms). Let $j\\in \\N$ be an\ninteger such that $\\Gamma ^j(v)\\cap \\Gamma ^{-}(v)=\\{v\\}$. Our main result\nstates that\n  $$ |\\Gamma ^{j} (v)|\\ge | \\Gamma ^{j-1} (v)| + |\\Gamma (v)|-1.$$\n  As an application we have $ |\\Gamma ^{j} (v)| \\ge 1+(|\\Gamma (v)|-1)j.$ The\nlast result confirms a recent conjecture of Seymour in the case of\nvertex-symmetric graphs. Also it gives a short proof for the validity of the\nCaccetta-H\\\"aggkvist conjecture for vertex-symmetric graphs and generalizes an\nadditive result of Shepherdson.\n", "category": [2]}
{"abstract": "  Annealed importance sampling is a means to assign equilibrium weights to a\nnonequilibrium sample that was generated by a simulated annealing protocol. The\nweights may then be used to calculate equilibrium averages, and also serve as\nan ``adiabatic signature'' of the chosen cooling schedule. In this paper we\ndemonstrate the method on the 50-atom dileucine peptide, showing that\nequilibrium distributions are attained for manageable cooling schedules. For\nthis system, as naively implemented here, the method is modestly more efficient\nthan constant temperature simulation. However, the method is worth considering\nwhenever any simulated heating or cooling is performed (as is often done at the\nbeginning of a simulation project, or during an NMR structure calculation), as\nit is simple to implement and requires minimal additional CPU expense.\nFurthermore, the naive implementation presented here can be improved.\n", "category": [4]}
{"abstract": "  Given a bipartite graph $G = (V_1,V_2,E)$ where edges take on {\\it both}\npositive and negative weights from set $\\mathcal{S}$, the {\\it maximum weighted\nedge biclique} problem, or $\\mathcal{S}$-MWEB for short, asks to find a\nbipartite subgraph whose sum of edge weights is maximized. This problem has\nvarious applications in bioinformatics, machine learning and databases and its\n(in)approximability remains open. In this paper, we show that for a wide range\nof choices of $\\mathcal{S}$, specifically when $| \\frac{\\min\\mathcal{S}} {\\max\n\\mathcal{S}} | \\in \\Omega(\\eta^{\\delta-1/2}) \\cap O(\\eta^{1/2-\\delta})$ (where\n$\\eta = \\max\\{|V_1|, |V_2|\\}$, and $\\delta \\in (0,1/2]$), no polynomial time\nalgorithm can approximate $\\mathcal{S}$-MWEB within a factor of $n^{\\epsilon}$\nfor some $\\epsilon > 0$ unless $\\mathsf{RP = NP}$. This hardness result gives\njustification of the heuristic approaches adopted for various applied problems\nin the aforementioned areas, and indicates that good approximation algorithms\nare unlikely to exist. Specifically, we give two applications by showing that:\n1) finding statistically significant biclusters in the SAMBA model, proposed in\n\\cite{Tan02} for the analysis of microarray data, is\n$n^{\\epsilon}$-inapproximable; and 2) no polynomial time algorithm exists for\nthe Minimum Description Length with Holes problem \\cite{Bu05} unless\n$\\mathsf{RP=NP}$.\n", "category": [0, 0]}
{"abstract": "  We show a one-to-one correspondence between arrangements of d lines in the\nprojective plane, and lines in P^{d-2}. We apply this correspondence to\nclassify (3,q)-nets over the complex numbers for all q<=6. When q=6, we have\ntwelve possible combinatorial cases, but we prove that only nine of them are\nrealizable. This new case shows several new properties for 3-nets: different\ndimensions for moduli, strict realization over certain fields, etc. We also\nconstruct a three dimensional family of (3,8)-nets corresponding to the\nQuaternion group.\n", "category": [2, 2]}
{"abstract": "  In this note we contrast two transformation-based methods to deduce absolute\nextrema and the corresponding extremizers. Unlike variation-based methods, the\ntransformation-based ones of Carlson and Leitmann and the recent one of Silva\nand Torres are direct in that they permit obtaining solutions by inspection.\n", "category": [2]}
{"abstract": "  We describe the maximal torus and maximal unipotent subgroup of the Picard\nvariety of a proper scheme over a perfect field.\n", "category": [2, 2]}
{"abstract": "  This paper is concerned with a shape sensitivity analysis of a viscous\nincompressible fluid driven by Stokes equations with nonhomogeneous boundary\ncondition. The structure of shape gradient with respect to the shape of the\nvariable domain for a given cost function is established by using the\ndifferentiability of a minimax formulation involving a Lagrangian functional\ncombining with function space parametrization technique or function space\nembedding technique. We apply an gradient type algorithm to our problem.\nNumerical examples show that our theory is useful for practical purpose and the\nproposed algorithm is feasible.\n", "category": [2]}
{"abstract": "  We illustrate through example 1 and 2 that the condition at theorem 1 in [8]\ndissatisfies necessity, and the converse proposition of fact 1.1 in [8] does\nnot hold, namely the condition Z/M - L/Ak < 1/(2 Ak^2) is not sufficient for\nf(i) + f(j) = f(k). Illuminate through an analysis and ex.3 that there is a\nlogic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4\nto be invalid. Demonstrate through ex.4 and 5 that each or the combination of\nqu+1 > qu * D at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) +\nf(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4\nand alg.2 based on table 1 are disordered and wrong logically. Further,\nmanifest through a repeated experiment and ex.5 that the data at table 2 is\nfalsified, and the example in [8] is woven elaborately. We explain why Cx = Ax\n* W^f(x) (% M) is changed to Cx = (Ax * W^f(x))^d (% M) in REESSE1+ v2.1. To\nthe signature fraud, we point out that [8] misunderstands the existence of T^-1\nand Q^-1 % (M-1), and forging of Q can be easily avoided through moving H.\nTherefore, the conclusion of [8] that REESSE1+ is not secure at all (which\nconnotes that [8] can extract a related private key from any public key in\nREESSE1+) is fully incorrect, and as long as the parameter Omega is fitly\nselected, REESSE1+ with Cx = Ax * W^f(x) (% M) is secure.\n", "category": [0]}
{"abstract": "  Given a remarkable representation of the generalized Pauli operators of\ntwo-qubits in terms of the points of the generalized quadrangle of order two,\nW(2), it is shown that specific subsets of these operators can also be\nassociated with the points and lines of the four-dimensional projective space\nover the Galois field with two elements - the so-called Veldkamp space of W(2).\nAn intriguing novelty is the recognition of (uni- and tri-centric) triads and\nspecific pentads of the Pauli operators in addition to the \"classical\" subsets\nanswering to geometric hyperplanes of W(2).\n", "category": [2]}
{"abstract": "  In this paper, we describe a new, systematic and explicit way of\napproximating solutions of mixed hyperbolic systems with constant coefficients\nsatisfying a Uniform Lopatinski Condition via different Penalization\napproaches.\n", "category": [2]}
{"abstract": "  We investigate cooperative wireless relay networks in which the nodes can\nhelp each other in data transmission. We study different coding strategies in\nthe single-source single-destination network with many relay nodes. Given the\nmyriad of ways in which nodes can cooperate, there is a natural routing\nproblem, i.e., determining an ordered set of nodes to relay the data from the\nsource to the destination. We find that for a given route, the\ndecode-and-forward strategy, which is an information theoretic cooperative\ncoding strategy, achieves rates significantly higher than that achievable by\nthe usual multi-hop coding strategy, which is a point-to-point non-cooperative\ncoding strategy. We construct an algorithm to find an optimal route (in terms\nof rate maximizing) for the decode-and-forward strategy. Since the algorithm\nruns in factorial time in the worst case, we propose a heuristic algorithm that\nruns in polynomial time. The heuristic algorithm outputs an optimal route when\nthe nodes transmit independent codewords. We implement these coding strategies\nusing practical low density parity check codes to compare the performance of\nthe strategies on different routes.\n", "category": [0, 2]}
{"abstract": "  We prove that if a group is nilpotent (resp. metabelian), then so is the\nsubgroup of its automorphism group generated by all polynomial automorphisms.\n", "category": [2]}
{"abstract": "  We argue that the critical behaviour near the point of ``gradient\ncatastrophe\" of the solution to the Cauchy problem for the focusing nonlinear\nSchr\\\"odinger equation $ i\\epsilon \\psi_t +\\frac{\\epsilon^2}2\\psi_{xx}+\n|\\psi|^2 \\psi =0$ with analytic initial data of the form $\\psi(x,0;\\epsilon)\n=A(x) e^{\\frac{i}{\\epsilon} S(x)}$ is approximately described by a particular\nsolution to the Painlev\\'e-I equation.\n", "category": [2, 2]}
{"abstract": "  In this paper, we continue the study of the category of functors Fquad,\nassociated to F_2-vector spaces equipped with a nondegenerate quadratic form,\ninitiated in two previous papers of the author. We define a filtration of the\nstandard projective objects in Fquad; this refines to give a decomposition into\nindecomposable factors of the two first standard projective objects in Fquad.\nAs an application of these two decompositions, we give a complete description\nof the polynomial functors of the category Fquad.\n", "category": [2, 2]}
{"abstract": "  We find a necessary and sufficient condition for a compact 7-manifold to\nadmit a $\\tilde G_2$-structure. As a result we find a sufficient condition for\nan open 7-manifold to admit a closed 3-form of $\\tilde G_2$-type.\n", "category": [2, 2]}
{"abstract": "  We consider a sequence of additive functionals {\\phi_n}, set on a sequence of\nMarkov chains {X_n} that weakly converges to a Markov process X. We give\nsufficient condition for such a sequence to converge in distribution,\nformulated in terms of the characteristics of the additive functionals, and\nrelated to the Dynkin's theorem on the convergence of W-functionals. As an\napplication of the main theorem, the general sufficient condition for\nconvergence of additive functionals in terms of transition probabilities of the\nchains X_n is proved.\n", "category": [2]}
{"abstract": "  In this paper we study a class of backward stochastic differential equations\n(BSDEs) of the form dY(t)= -AY(t)dt -f_0(t,Y(t))dt -f_1(t,Y(t),Z(t))dt +\nZ(t)dW(t) on the interval [0,T], with given final condition at time T, in an\ninfinite dimensional Hilbert space H. The unbounded operator A is sectorial and\ndissipative and the nonlinearity f_0(t,y) is dissipative and defined for y only\ntaking values in a subspace of H. A typical example is provided by the\nso-called polynomial nonlinearities. Applications are given to stochastic\npartial differential equations and spin systems.\n", "category": [2]}
{"abstract": "  A unified approach to (symmetric informationally complete) positive operator\nvalued measures and mutually unbiased bases is developed in this article. The\napproach is based on the use of operator equivalents expanded in the enveloping\nalgebra of SU(2). Emphasis is put on similarities and differences between\nSIC-POVMs and MUBs.\n", "category": [2]}
{"abstract": "  This article discusses completeness of Boolean Algebra as First Order Theory\nin Goedel's meaning. If Theory is complete then any possible transformation is\nequivalent to some transformation using axioms, predicates etc. defined for\nthis theory. If formula is to be proved (or disproved) then it has to be\nreduced to axioms. If every transformation is deducible then also optimal\ntransformation is deducible. If every transformation is exponential then\noptimal one is too, what allows to define lower bound for discussed problem to\nbe exponential (outside P). Then we show algorithm for NDTM solving the same\nproblem in O(n^c) (so problem is in NP), what proves that P \\neq NP.\n  Article proves also that result of relativisation of P=NP question and oracle\nshown by Baker-Gill-Solovay distinguish between deterministic and\nnon-deterministic calculation models. If there exists oracle A for which\nP^A=NP^A then A consists of infinite number of algorithms, DTMs, axioms and\npredicates, or like NDTM infinite number of simultaneous states.\n", "category": [2, 2]}
{"abstract": "  Foods naturally contain a number of contaminants that may have different and\nlong term toxic effects. This paper introduces a novel approach for the\nassessment of such chronic food risk that integrates the pharmacokinetic\nproperties of a given contaminant. The estimation of such a Kinetic Dietary\nExposure Model (KDEM) should be based on long term consumption data which, for\nthe moment, can only be provided by Household Budget Surveys such as the\nSECODIP panel in France. A semi parametric model is proposed to decompose a\nseries of household quantities into individual quantities which are then used\nas inputs of the KDEM. As an illustration, the risk assessment related to the\npresence of methyl mercury in seafood is revisited using this novel approach.\n", "category": [2, 6]}
{"abstract": "  As a continuation of Rabei et al. work [11], the Hamilton- Jacobi partial\ndifferential equation is generalized to be applicable for systems containing\nfractional derivatives. The Hamilton- Jacobi function in configuration space is\nobtained in a similar manner to the usual mechanics. Two problems are\nconsidered to demonstrate the application of the formalism. The result found to\nbe in exact agreement with Agrawal's formalism.\n", "category": [2]}
{"abstract": "  In this paper we investigate the optimal control problem for a class of\nstochastic Cauchy evolution problem with non standard boundary dynamic and\ncontrol. The model is composed by an infinite dimensional dynamical system\ncoupled with a finite dimensional dynamics, which describes the boundary\nconditions of the internal system. In other terms, we are concerned with non\nstandard boundary conditions, as the value at the boundary is governed by a\ndifferent stochastic differential equation.\n", "category": [2]}
{"abstract": "  Wentzel, Kramers, Brillouin (WKB) approximation for fractional systems is\ninvestigated in this paper using the fractional calculus. In the fractional\ncase the wave function is constructed such that the phase factor is the same as\nthe Hamilton's principle function \"S\". To demonstrate our proposed approach two\nexamples are investigated in details.\n", "category": [3, 2]}
{"abstract": "  This paper investigates the many-to-one throughput capacity (and by symmetry,\none-to-many throughput capacity) of IEEE 802.11 multi-hop networks. It has\ngenerally been assumed in prior studies that the many-to-one throughput\ncapacity is upper-bounded by the link capacity L. Throughput capacity L is not\nachievable under 802.11. This paper introduces the notion of \"canonical\nnetworks\", which is a class of regularly-structured networks whose capacities\ncan be analyzed more easily than unstructured networks. We show that the\nthroughput capacity of canonical networks under 802.11 has an analytical upper\nbound of 3L/4 when the source nodes are two or more hops away from the sink;\nand simulated throughputs of 0.690L (0.740L) when the source nodes are many\nhops away. We conjecture that 3L/4 is also the upper bound for general\nnetworks. When all links have equal length, 2L/3 can be shown to be the upper\nbound for general networks. Our simulations show that 802.11 networks with\nrandom topologies operated with AODV routing can only achieve throughputs far\nbelow the upper bounds. Fortunately, by properly selecting routes near the\ngateway (or by properly positioning the relay nodes leading to the gateway) to\nfashion after the structure of canonical networks, the throughput can be\nimproved significantly by more than 150%. Indeed, in a dense network, it is\nworthwhile to deactivate some of the relay nodes near the sink judiciously.\n", "category": [0, 0, 2]}
{"abstract": "  This article gives the proof of results announced in [J. Blanc, Finite\nAbelian subgroups of the Cremona group of the plane, C.R. Acad. Sci. Paris,\nS\\'er. I 344 (2007), 21-26.] and some description of automorphisms of rational\nsurfaces.\n  Given a finite Abelian subgroup of the Cremona group of the plane, we provide\na way to decide whether it is birationally conjugate to a group of\nautomorphisms of a minimal surface.\n  In particular, we prove that a finite cyclic group of birational\ntransformations of the plane is linearisable if and only if none of its\nnon-trivial elements fix a curve of positive genus. For finite Abelian groups,\nthere exists only one surprising exception, a group isomorphic to Z/2ZxZ/4Z,\nwhose non-trivial elements do not fix a curve of positive genus but which is\nnot conjugate to a group of automorphisms of a minimal rational surface.\n  We also give some descriptions of automorphisms (not necessarily of finite\norder) of del Pezzo surfaces and conic bundles.\n", "category": [2]}
{"abstract": "  Three types of integral representations for the cumulative distribution\nfunctions of convolutions of non-central p-variate gamma distributions are\ngiven by integration of elementary complex functions over the p-cube Cp =\n(-pi,pi]x...x(-pi,pi]. In particular, the joint distribution of the diagonal\nelements of a generalized quadratic form XAX' with n independent normally\ndistributed column vectors in X is obtained. For a single p-variate gamma\ndistribution function (p-1)-variate integrals over Cp-1 are derived. The\nintegrals are numerically more favourable than integrals obtained from the\nFourier or laplace inversion formula.\n", "category": [2, 6]}
{"abstract": "  The interference channel with degraded message sets (IC-DMS) refers to a\ncommunication model in which two senders attempt to communicate with their\nrespective receivers simultaneously through a common medium, and one of the\nsenders has complete and a priori (non-causal) knowledge about the message\nbeing transmitted by the other. A coding scheme that collectively has\nadvantages of cooperative coding, collaborative coding, and dirty paper coding,\nis developed for such a channel. With resorting to this coding scheme,\nachievable rate regions of the IC-DMS in both discrete memoryless and Gaussian\ncases are derived, which, in general, include several previously known rate\nregions. Numerical examples for the Gaussian case demonstrate that in the\nhigh-interference-gain regime, the derived achievable rate regions offer\nconsiderable improvements over these existing results.\n", "category": [0, 2]}
{"abstract": "  A subset $X$ of an abelian $G$ is said to be {\\em complete} if every element\nof the subgroup generated by $X$ can be expressed as a nonempty sum of distinct\nelements from $X$.\n  Let $A\\subset \\Z_n$ be such that all the elements of $A$ are coprime with\n$n$. Solving a conjecture of Erd\\H{o}s and Heilbronn, Olson proved that\n  $A$ is complete if $n$ is a prime and if $|A|>2\\sqrt{n}.$\n  Recently Vu proved that there is an absolute constant $c$, such that for an\narbitrary large $n$, $A$ is complete if $|A|\\ge c\\sqrt{n},$ and conjectured\nthat 2 is essentially the right value of $c$. We show that $A$ is complete if\n$|A|> 1+2\\sqrt{n-4}$, thus proving the last conjecture.\n", "category": [2]}
{"abstract": "  A solution is given to the following problem: how to compute the\nmultiplicity, or more generally the Hilbert function, at a point on a Schubert\nvariety in an orthogonal Grassmannian. Standard monomial theory is applied to\ntranslate the problem from geometry to combinatorics. The solution of the\nresulting combinatorial problem forms the bulk of the paper. This approach has\nbeen followed earlier to solve the same problem for the Grassmannian and the\nsymplectic Grassmannian.\n  As an application, we present an interpretation of the multiplicity as the\nnumber of non-intersecting lattice paths of a certain kind.\n  Taking the Schubert variety to be of a special kind and the point to be the\n\"identity coset,\" our problem specializes to a problem about Pfaffian ideals\ntreatments of which by different methods exist in the literature. Also\navailable in the literature is a geometric solution when the point is a\n\"generic singularity.\"\n", "category": [2, 2, 2]}
{"abstract": "  Erd\\\"{o}s and Tur\\'{a}n once conjectured that any set $A\\subset\\mathbb{N}$\nwith $\\sum_{a\\in A}{1}/{a}=\\infty$ should contain infinitely many progressions\nof arbitrary length $k\\geq3$. For the two-dimensional case Graham conjectured\nthat if $B\\subset \\mathbb{N}\\times\\mathbb{N}$ satisfies $$\\sum\\limits_{(x,y)\\in\nB}\\frac{1}{x^2+y^2}=\\infty,$$ then for any $s\\geq2$, $B$ contains an $s\\times\ns$ axes-parallel grid. In this paper it is shown that if the Graham conjecture\nis true for some $s\\geq2$, then the Erd\\\"{o}s-Tur\\'{a}n conjecture is true for\n$k=2s-1$.\n", "category": [2]}
{"abstract": "  Let $(A,\\Theta)$ be a principally polarised abelian variety, and let Y be a\nsubvariety. Pareschi and Popa conjectured that Y has minimal cohomology class\nif and only if the structure sheaf of Y satisfies a property that they call\nM-regularity.\n  Let now X be a smooth cubic threefold. By a classical result due to Clemens\nand Griffiths, its intermediate Jacobian J(X) is a principally polarised\nabelian variety; furthermore the Fano surface of lines on X can be embedded in\nJ(X) and has minimal cohomology class. In this short note we show that its\nstructure sheaf is M-regular.\n", "category": [2]}
{"abstract": "  The spectral action on noncommutative torus is obtained, using a\nChamseddine--Connes formula via computations of zeta functions. The importance\nof a Diophantine condition is outlined. Several results on holomorphic\ncontinuation of series of holomorphic functions are obtained in this context.\n", "category": [3, 2]}
{"abstract": "  We rigorously derive a weak form of the Lifshitz-Slyozov-Wagner equation as\nthe homogenization limit of a Stefan-type problem describing\nreaction-controlled coarsening of a large number of small spherical particles.\nMoreover, we deduce that the effective mean-field description holds true in the\nparticular limit of vanishing surface-area density of particles.\n", "category": [2]}
{"abstract": "  We introduce a new class of canonical AZD's (called the supercanonical AZD's)\non the canonical bundles of smooth projective varieties with pseudoeffective\ncanonical classes. We study the variation of the supercanonical AZD\n$\\hat{h}_{can}$ under projective deformations and give a new proof of the\ninvariance of plurigenera.\n", "category": [2, 2]}
{"abstract": "  We consider a model for interest rates, where the short rate is given by a\ntime-homogenous, one-dimensional affine process in the sense of Duffie,\nFilipovic and Schachermayer. We show that in such a model yield curves can only\nbe normal, inverse or humped (i.e. endowed with a single local maximum). Each\ncase can be characterized by simple conditions on the present short rate. We\ngive conditions under which the short rate process will converge to a limit\ndistribution and describe the limit distribution in terms of its cumulant\ngenerating function. We apply our results to the Vasicek model, the CIR model,\na CIR model with added jumps and a model of Ornstein-Uhlenbeck type.\n", "category": [5, 2]}
{"abstract": "  Let $G$ be a finite group and $V$ be a finite $G$--module. We present upper\nbounds for the cardinalities of certain subsets of $\\Irr(GV)$, such as the set\nof those $\\chi\\in\\Irr(GV)$ such that, for a fixed $v\\in V$, the restriction of\n$\\chi$ to $<v>$ is not a multiple of the regular character of $<v>$. These\nresults might be useful in attacking the non--coprime $k(GV)$--problem.\n", "category": [2, 2]}
{"abstract": "  We consider statistical mechanics models of continuous height effective\ninterfaces in the presence of a delta-pinning at height zero. There is a\ndetailed mathematical understanding of the depinning transition in 2 dimensions\nwithout disorder. Then the variance of the interface height w.r.t. the Gibbs\nmeasure stays bounded uniformly in the volume for any positive pinning force\nand diverges like the logarithm of the pinning force when it tends to zero.\n  How does the presence of a quenched disorder term in the Hamiltonian modify\nthis transition? We show that an arbitarily weak random field term is enough to\nbeat an arbitrarily strong delta-pinning in 2 dimensions and will cause\ndelocalization. The proof is based on a rigorous lower bound for the overlap\nbetween local magnetizations and random fields in finite volume. In 2\ndimensions it implies growth faster than the volume which is a contradiction to\nlocalization. We also derive a simple complementary inequality which shows that\nin higher dimensions the fraction of pinned sites converges to one when the\npinning force tends to infinity.\n", "category": [2, 2]}
{"abstract": "  Let $\\Dh$ and $A$ be unital and separable $C^{*}$-algebras; let $\\Dh$ be\nstrongly self-absorbing. It is known that any two unital $^*$-homomorphisms\nfrom $\\Dh$ to $A \\otimes \\Dh$ are approximately unitarily equivalent. We show\nthat, if $\\Dh$ is also $K_{1}$-injective, they are even asymptotically\nunitarily equivalent. This in particular implies that any unital endomorphism\nof $\\Dh$ is asymptotically inner. Moreover, the space of automorphisms of $\\Dh$\nis compactly-contractible (in the point-norm topology) in the sense that for\nany compact Hausdorff space $X$, the set of homotopy classes $[X,\\Aut(\\Dh)]$\nreduces to a point. The respective statement holds for the space of unital\nendomorphisms of $\\Dh$. As an application, we give a description of the\nKasparov group $KK(\\Dh, A\\ot \\Dh)$ in terms of $^*$-homomorphisms and\nasymptotic unitary equivalence. Along the way, we show that the Kasparov group\n$KK(\\Dh, A\\ot \\Dh)$ is isomorphic to $K_0(A\\ot \\Dh)$.\n", "category": [2, 2]}
{"abstract": "  A new expression as a certain asymptotic limit via \"discrete micro-states\" of\npermutations is provided to the mutual information of both continuous and\ndiscrete random variables.\n", "category": [2, 2, 6]}
{"abstract": "  We analyze 27 house price indexes of Las Vegas from Jun. 1983 to Mar. 2005,\ncorresponding to 27 different zip codes. These analyses confirm the existence\nof a real-estate bubble, defined as a price acceleration faster than\nexponential, which is found however to be confined to a rather limited time\ninterval in the recent past from approximately 2003 to mid-2004 and has\nprogressively transformed into a more normal growth rate comparable to\npre-bubble levels in 2005. There has been no bubble till 2002 except for a\nmedium-sized surge in 1990. In addition, we have identified a strong yearly\nperiodicity which provides a good potential for fine-tuned prediction from\nmonth to month. A monthly monitoring using a model that we have developed could\nconfirm, by testing the intra-year structure, if indeed the market has returned\nto ``normal'' or if more turbulence is expected ahead. We predict the evolution\nof the indexes one year ahead, which is validated with new data up to Sep.\n2006. The present analysis demonstrates the existence of very significant\nvariations at the local scale, in the sense that the bubble in Las Vegas seems\nto have preceded the more global USA bubble and has ended approximately two\nyears earlier (mid 2004 for Las Vegas compared with mid-2006 for the whole of\nthe USA).\n", "category": [5, 3, 5]}
{"abstract": "  We present an algorithm for systematic encoding of Hermitian codes. For a\nHermitian code defined over GF(q^2), the proposed algorithm achieves a run time\ncomplexity of O(q^2) and is suitable for VLSI implementation. The encoder\narchitecture uses as main blocks q varying-rate Reed-Solomon encoders and\nachieves a space complexity of O(q^2) in terms of finite field multipliers and\nmemory elements.\n", "category": [0, 2]}
{"abstract": "  We consider the curvature of a family of warped products of two\npseduo-Riemannian manifolds $(B,g_B)$ and $(F,g_F)$ furnished with metrics of\nthe form $c^{2}g_B \\oplus w^2 g_F$ and, in particular, of the type $w^{2\n\\mu}g_B \\oplus w^2 g_F$, where $c, w \\colon B \\to (0,\\infty)$ are smooth\nfunctions and $\\mu$ is a real parameter. We obtain suitable expressions for the\nRicci tensor and scalar curvature of such products that allow us to establish\nresults about the existence of Einstein or constant scalar curvature structures\nin these categories. If $(B,g_B)$ is Riemannian, the latter question involves\nnonlinear elliptic partial differential equations with concave-convex\nnonlinearities and singular partial differential equations of the\nLichnerowicz-York type among others.\n", "category": [2, 3, 3, 2, 2]}
{"abstract": "  This is a final step in a local classification of pseudo-Riemannian manifolds\nwith parallel Weyl tensor that are not conformally flat or locally symmetric.\n", "category": [2]}
{"abstract": "  Despite their claimed biological plausibility, most self organizing networks\nhave strict topological constraints and consequently they cannot take into\naccount a wide range of external stimuli. Furthermore their evolution is\nconditioned by deterministic laws which often are not correlated with the\nstructural parameters and the global status of the network, as it should happen\nin a real biological system. In nature the environmental inputs are noise\naffected and fuzzy. Which thing sets the problem to investigate the possibility\nof emergent behaviour in a not strictly constrained net and subjected to\ndifferent inputs. It is here presented a new model of Evolutionary Neural Gas\n(ENG) with any topological constraints, trained by probabilistic laws depending\non the local distortion errors and the network dimension. The network is\nconsidered as a population of nodes that coexist in an ecosystem sharing local\nand global resources. Those particular features allow the network to quickly\nadapt to the environment, according to its dimensions. The ENG model analysis\nshows that the net evolves as a scale-free graph, and justifies in a deeply\nphysical sense- the term gas here used.\n", "category": [3, 4]}
{"abstract": "  We introduce a new class of \"random\" subsets of natural numbers, WM sets.\nThis class contains normal sets (sets whose characteristic function is a normal\nbinary sequence). We establish necessary and sufficient conditions for\nsolvability of systems of linear equations within every WM set and within every\nnormal set. We also show that partition-regular system of linear equations with\ninteger coefficients is solvable in any WM set.\n", "category": [2, 2]}
{"abstract": "  We study birational maps with empty base locus defined by almost complete\nintersection ideals. Birationality is shown to be expressed by the equality of\ntwo Chern numbers. We provide a relatively effective method of their\ncalculation in terms of certain Hilbert coefficients. In dimension two the\nstructure of the irreducible ideals leads naturally to the calculation of\nSylvester determinants via a computer-assisted method. For degree at most 5 we\nproduce the full set of defining equations of the base ideal. The results\nanswer affirmatively some questions raised by D. Cox.\n", "category": [2, 2]}
{"abstract": "  We prove an Alexander type theorem for the spectral unit ball $\\Omega_n$\nshowing that there are no non-trivial proper holomorphic mappings in\n$\\Omega_n$, $n\\geq 2$.\n", "category": [2, 2]}
{"abstract": "  The parsimony score of a character on a tree equals the number of state\nchanges required to fit that character onto the tree. We show that for\nunordered, reversible characters this score equals the number of tree\nrearrangements required to fit the tree onto the character. We discuss\nimplications of this connection for the debate over the use of consensus trees\nor total evidence, and show how it provides a link between incongruence of\ncharacters and recombination.\n", "category": [4]}
{"abstract": "  A coarse-grained computational procedure based on the Finite Element Method\nis proposed to calculate the normal modes and mechanical response of proteins\nand their supramolecular assemblies. Motivated by the elastic network model,\nproteins are modeled as homogeneous isotropic elastic solids with volume\ndefined by their solvent-excluded surface. The discretized Finite Element\nrepresentation is obtained using a surface simplification algorithm that\nfacilitates the generation of models of arbitrary prescribed spatial\nresolution. The procedure is applied to compute the normal modes of a mutant of\nT4 phage lysozyme and of filamentous actin, as well as the critical Euler\nbuckling load of the latter when subject to axial compression. Results compare\nfavorably with all-atom normal mode analysis, the Rotation Translation Blocks\nprocedure, and experiment. The proposed methodology establishes a computational\nframework for the calculation of protein mechanical response that facilitates\nthe incorporation of specific atomic-level interactions into the model,\nincluding aqueous-electrolyte-mediated electrostatic effects. The procedure is\nequally applicable to proteins with known atomic coordinates as it is to\nelectron density maps of proteins, protein complexes, and supramolecular\nassemblies of unknown atomic structure.\n", "category": [4, 4]}
{"abstract": "  In this paper a theoretical model of functioning of a neural circuit during a\nbehavioral response has been proposed. A neural circuit can be thought of as a\ndirected multigraph whose each vertex is a neuron and each edge is a synapse.\nIt has been assumed in this paper that the behavior of such circuits is\nmanifested through the collective behavior of neurons belonging to that\ncircuit. Behavioral information of each neuron is contained in the coefficients\nof the fast Fourier transform (FFT) over the output spike train. Those\ncoefficients form a vector in a multidimensional vector space. Behavioral\ndynamics of a neuronal network in response to strong aversive stimuli has been\nstudied in a vector space in which a suitable pseudometric has been defined.\nThe neurodynamical model of network behavior has been formulated in terms of\nexisting memory, synaptic plasticity and feelings. The model has an analogy in\nclassical electrostatics, by which the notion of force and potential energy has\nbeen introduced. Since the model takes input from each neuron in a network and\nproduces a behavior as the output, it would be extremely difficult or may even\nbe impossible to implement. But with the help of the model a possible\nexplanation for an hitherto unexplained neurological observation in human brain\nhas been offered. The model is compatible with a recent model of sequential\nbehavioral dynamics. The model is based on electrophysiology, but its relevance\nto hemodynamics has been outlined.\n", "category": [4]}
{"abstract": "  We show that recent stock market fluctuations are characterized by the\ncumulative distributions whose tails on short, minute time scales exhibit power\nscaling with the scaling index alpha > 3 and this index tends to increase\nquickly with decreasing sampling frequency. Our study is based on\nhigh-frequency recordings of the S&P500, DAX and WIG20 indices over the\ninterval May 2004 - May 2006. Our findings suggest that dynamics of the\ncontemporary market may differ from the one observed in the past. This effect\nindicates a constantly increasing efficiency of world markets.\n", "category": [5, 3, 3]}
{"abstract": "  The problem of statistical learning is to construct a predictor of a random\nvariable $Y$ as a function of a related random variable $X$ on the basis of an\ni.i.d. training sample from the joint distribution of $(X,Y)$. Allowable\npredictors are drawn from some specified class, and the goal is to approach\nasymptotically the performance (expected loss) of the best predictor in the\nclass. We consider the setting in which one has perfect observation of the\n$X$-part of the sample, while the $Y$-part has to be communicated at some\nfinite bit rate. The encoding of the $Y$-values is allowed to depend on the\n$X$-values. Under suitable regularity conditions on the admissible predictors,\nthe underlying family of probability distributions and the loss function, we\ngive an information-theoretic characterization of achievable predictor\nperformance in terms of conditional distortion-rate functions. The ideas are\nillustrated on the example of nonparametric regression in Gaussian noise.\n", "category": [0, 0, 2]}
{"abstract": "  Suprathreshold stochastic resonance (SSR) is a form of noise enhanced signal\ntransmission that occurs in a parallel array of independently noisy identical\nthreshold nonlinearities, including model neurons. Unlike most forms of\nstochastic resonance, the output response to suprathreshold random input\nsignals of arbitrary magnitude is improved by the presence of even small\namounts of noise. In this paper the information transmission performance of SSR\nin the limit of a large array size is considered. Using a relationship between\nShannon's mutual information and Fisher information, a sufficient condition for\noptimality, i.e. channel capacity, is derived. It is shown that capacity is\nachieved when the signal distribution is Jeffrey's prior, as formed from the\nnoise distribution, or when the noise distribution depends on the signal\ndistribution via a cosine relationship. These results provide theoretical\nverification and justification for previous work in both computational\nneuroscience and electronics.\n", "category": [3, 4]}
{"abstract": "  The high volume of packets and packet rates of traffic on some router links\nmakes it exceedingly difficult for routers to examine every packet in order to\nkeep detailed statistics about the traffic which is traversing the router.\nSampling is commonly applied on routers in order to limit the load incurred by\nthe collection of information that the router has to undertake when evaluating\nflow information for monitoring purposes. The sampling process in nearly all\ncases is a deterministic process of choosing 1 in every N packets on a\nper-interface basis, and then forming the flow statistics based on the\ncollected sampled statistics. Even though this sampling may not be significant\nfor some statistics, such as packet rate, others can be severely distorted.\nHowever, it is important to consider the sampling techniques and their relative\naccuracy when applied to different traffic patterns. The main disadvantage of\nsampling is the loss of accuracy in the collected trace when compared to the\noriginal traffic stream. To date there has not been a detailed analysis of the\nimpact of sampling at a router in various traffic profiles and flow criteria.\nIn this paper, we assess the performance of the sampling process as used in\nNetFlow in detail, and we discuss some techniques for the compensation of loss\nof monitoring detail.\n", "category": [0, 0]}
{"abstract": "  We compute the loss of power in likelihood ratio tests when we test the\noriginal parameter of a probability density extended by the first Lehmann\nalternative.\n", "category": [2, 2, 6, 6]}
{"abstract": "  The rich set of interactions between individuals in the society results in\ncomplex community structure, capturing highly connected circles of friends,\nfamilies, or professional cliques in a social network. Thanks to frequent\nchanges in the activity and communication patterns of individuals, the\nassociated social and communication network is subject to constant evolution.\nOur knowledge of the mechanisms governing the underlying community dynamics is\nlimited, but is essential for a deeper understanding of the development and\nself-optimisation of the society as a whole. We have developed a new algorithm\nbased on clique percolation, that allows, for the first time, to investigate\nthe time dependence of overlapping communities on a large scale and as such, to\nuncover basic relationships characterising community evolution. Our focus is on\nnetworks capturing the collaboration between scientists and the calls between\nmobile phone users. We find that large groups persist longer if they are\ncapable of dynamically altering their membership, suggesting that an ability to\nchange the composition results in better adaptability. The behaviour of small\ngroups displays the opposite tendency, the condition for stability being that\ntheir composition remains unchanged. We also show that the knowledge of the\ntime commitment of the members to a given community can be used for estimating\nthe community's lifetime. These findings offer a new view on the fundamental\ndifferences between the dynamics of small groups and large institutions.\n", "category": [6, 3, 6]}
{"abstract": "  We apply results of Malliavin-Thalmaier-Watanabe for strong and weak Taylor\nexpansions of solutions of perturbed stochastic differential equations (SDEs).\nIn particular, we work out weight expressions for the Taylor coefficients of\nthe expansion. The results are applied to LIBOR market models in order to deal\nwith the typical stochastic drift and with stochastic volatility. In contrast\nto other accurate methods like numerical schemes for the full SDE, we obtain\neasily tractable expressions for accurate pricing. In particular, we present an\neasily tractable alternative to ``freezing the drift'' in LIBOR market models,\nwhich has an accuracy similar to the full numerical scheme. Numerical examples\nunderline the results.\n", "category": [5, 2]}
{"abstract": "  To investigate the universality of the structure of interactions in different\nmarkets, we analyze the cross-correlation matrix C of stock price fluctuations\nin the National Stock Exchange (NSE) of India. We find that this emerging\nmarket exhibits strong correlations in the movement of stock prices compared to\ndeveloped markets, such as the New York Stock Exchange (NYSE). This is shown to\nbe due to the dominant influence of a common market mode on the stock prices.\nBy comparison, interactions between related stocks, e.g., those belonging to\nthe same business sector, are much weaker. This lack of distinct sector\nidentity in emerging markets is explicitly shown by reconstructing the network\nof mutually interacting stocks. Spectral analysis of C for NSE reveals that,\nthe few largest eigenvalues deviate from the bulk of the spectrum predicted by\nrandom matrix theory, but they are far fewer in number compared to, e.g., NYSE.\nWe show this to be due to the relative weakness of intra-sector interactions\nbetween stocks, compared to the market mode, by modeling stock price dynamics\nwith a two-factor model. Our results suggest that the emergence of an internal\nstructure comprising multiple groups of strongly coupled components is a\nsignature of market development.\n", "category": [5, 3, 3, 3]}
{"abstract": "  In this paper we give a definition of \"algorithm,\" \"finite algorithm,\"\n\"equivalent algorithms,\" and what it means for a single algorithm to dominate a\nset of algorithms. We define a derived algorithm which may have a smaller mean\nexecution time than any of its component algorithms. We give an explicit\nexpression for the mean execution time (when it exists) of the derived\nalgorithm. We give several illustrative examples of derived algorithms with two\ncomponent algorithms. We include mean execution time solutions for\ntwo-algorithm processors whose joint density of execution times are of several\ngeneral forms. For the case in which the joint density for a two-algorithm\nprocessor is a step function, we give a maximum-likelihood estimation scheme\nwith which to analyze empirical processing time data.\n", "category": [0, 0]}
{"abstract": "  This paper develops a contention-based opportunistic feedback technique\ntowards relay selection in a dense wireless network. This technique enables the\nforwarding of additional parity information from the selected relay to the\ndestination. For a given network, the effects of varying key parameters such as\nthe feedback probability are presented and discussed. A primary advantage of\nthe proposed technique is that relay selection can be performed in a\ndistributed way. Simulation results find its performance to closely match that\nof centralized schemes that utilize GPS information, unlike the proposed\nmethod. The proposed relay selection method is also found to achieve throughput\ngains over a point-to-point transmission strategy.\n", "category": [0, 2]}
{"abstract": "  It has been shown that a decentralized relay selection protocol based on\nopportunistic feedback from the relays yields good throughput performance in\ndense wireless networks. This selection strategy supports a hybrid-ARQ\ntransmission approach where relays forward parity information to the\ndestination in the event of a decoding error. Such an approach, however,\nsuffers a loss compared to centralized strategies that select relays with the\nbest channel gain to the destination. This paper closes the performance gap by\nadding another level of channel feedback to the decentralized relay selection\nproblem. It is demonstrated that only one additional bit of feedback is\nnecessary for good throughput performance. The performance impact of varying\nkey parameters such as the number of relays and the channel feedback threshold\nis discussed. An accompanying bit error rate analysis demonstrates the\nimportance of relay selection.\n", "category": [0, 2]}
{"abstract": "  We assess the practicality of random network coding by illuminating the issue\nof overhead and considering it in conjunction with increasingly long packets\nsent over the erasure channel. We show that the transmission of increasingly\nlong packets, consisting of either of an increasing number of symbols per\npacket or an increasing symbol alphabet size, results in a data rate\napproaching zero over the erasure channel. This result is due to an erasure\nprobability that increases with packet length. Numerical results for a\nparticular modulation scheme demonstrate a data rate of approximately zero for\na large, but finite-length packet. Our results suggest a reduction in the\nperformance gains offered by random network coding.\n", "category": [0, 2]}
{"abstract": "  A new incremental algorithm for data compression is presented. For a sequence\nof input symbols algorithm incrementally constructs a p-adic integer number as\nan output. Decoding process starts with less significant part of a p-adic\ninteger and incrementally reconstructs a sequence of input symbols. Algorithm\nis based on certain features of p-adic numbers and p-adic norm. p-adic coding\nalgorithm may be considered as of generalization a popular compression\ntechnique - arithmetic coding algorithms. It is shown that for p = 2 the\nalgorithm works as integer variant of arithmetic coding; for a special class of\nmodels it gives exactly the same codes as Huffman's algorithm, for another\nspecial model and a specific alphabet it gives Golomb-Rice codes.\n", "category": [0]}
{"abstract": "  We study universal compression of sequences generated by monotonic\ndistributions. We show that for a monotonic distribution over an alphabet of\nsize $k$, each probability parameter costs essentially $0.5 \\log (n/k^3)$ bits,\nwhere $n$ is the coded sequence length, as long as $k = o(n^{1/3})$. Otherwise,\nfor $k = O(n)$, the total average sequence redundancy is $O(n^{1/3+\\epsilon})$\nbits overall. We then show that there exists a sub-class of monotonic\ndistributions over infinite alphabets for which redundancy of\n$O(n^{1/3+\\epsilon})$ bits overall is still achievable. This class contains\nfast decaying distributions, including many distributions over the integers and\ngeometric distributions. For some slower decays, including other distributions\nover the integers, redundancy of $o(n)$ bits overall is achievable, where a\nmethod to compute specific redundancy rates for such distributions is derived.\nThe results are specifically true for finite entropy monotonic distributions.\nFinally, we study individual sequence redundancy behavior assuming a sequence\nis governed by a monotonic distribution. We show that for sequences whose\nempirical distributions are monotonic, individual redundancy bounds similar to\nthose in the average case can be obtained. However, even if the monotonicity in\nthe empirical distribution is violated, diminishing per symbol individual\nsequence redundancies with respect to the monotonic maximum likelihood\ndescription length may still be achievable.\n", "category": [0, 2]}
{"abstract": "  This paper presents an experimental study and the lessons learned from the\nobservation of the attackers when logged on a compromised machine. The results\nare based on a six months period during which a controlled experiment has been\nrun with a high interaction honeypot. We correlate our findings with those\nobtained with a worldwide distributed system of lowinteraction honeypots.\n", "category": [0]}
{"abstract": "  This paper presents a measurement-based availability assessment study using\nfield data collected during a 4-year period from 373 SunOS/Solaris Unix\nworkstations and servers interconnected through a local area network. We focus\non the estimation of machine uptimes, downtimes and availability based on the\nidentification of failures that caused total service loss. Data corresponds to\nsyslogd event logs that contain a large amount of information about the normal\nactivity of the studied systems as well as their behavior in the presence of\nfailures. It is widely recognized that the information contained in such event\nlogs might be incomplete or imperfect. The solution investigated in this paper\nto address this problem is based on the use of auxiliary sources of data\nobtained from wtmpx files maintained by the SunOS/Solaris Unix operating\nsystem. The results obtained suggest that the combined use of wtmpx and syslogd\nlog files provides more complete information on the state of the target systems\nthat is useful to provide availability estimations that better reflect reality.\n", "category": [0]}
{"abstract": "  Honeypots are more and more used to collect data on malicious activities on\nthe Internet and to better understand the strategies and techniques used by\nattackers to compromise target systems. Analysis and modeling methodologies are\nneeded to support the characterization of attack processes based on the data\ncollected from the honeypots. This paper presents some empirical analyses based\non the data collected from the Leurr{\\'e}.com honeypot platforms deployed on\nthe Internet and presents some preliminary modeling studies aimed at fulfilling\nsuch objectives.\n", "category": [0, 0]}
{"abstract": "  For efficiency reasons, the software system designers' will is to use an\nintegrated set of methods and tools to describe specifications and designs, and\nalso to perform analyses such as dependability, schedulability and performance.\nAADL (Architecture Analysis and Design Language) has proved to be efficient for\nsoftware architecture modeling. In addition, AADL was designed to accommodate\nseveral types of analyses. This paper presents an iterative dependency-driven\napproach for dependability modeling using AADL. It is illustrated on a small\nexample. This approach is part of a complete framework that allows the\ngeneration of dependability analysis and evaluation models from AADL models to\nsupport the analysis of software and system architectures, in critical\napplication domains.\n", "category": [0, 0]}
{"abstract": "  We present a hierarchical simulation approach for the dependability analysis\nand evaluation of a highly available commercial cache-based RAID storage\nsystem. The archi-tecture is complex and includes several layers of\noverlap-ping error detection and recovery mechanisms. Three ab-straction levels\nhave been developed to model the cache architecture, cache operations, and\nerror detection and recovery mechanism. The impact of faults and errors\noc-curring in the cache and in the disks is analyzed at each level of the\nhierarchy. A simulation submodel is associated with each abstraction level. The\nmodels have been devel-oped using DEPEND, a simulation-based environment for\nsystem-level dependability analysis, which provides facili-ties to inject\nfaults into a functional behavior model, to simulate error detection and\nrecovery mechanisms, and to evaluate quantitative measures. Several fault\nmodels are defined for each submodel to simulate cache component failures, disk\nfailures, transmission errors, and data errors in the cache memory and in the\ndisks. Some of the parame-ters characterizing fault injection in a given\nsubmodel cor-respond to probabilities evaluated from the simulation of the\nlower-level submodel. Based on the proposed method-ology, we evaluate and\nanalyze 1) the system behavior un-der a real workload and high error rate\n(focusing on error bursts), 2) the coverage of the error detection mechanisms\nimplemented in the system and the error latency distribu-tions, and 3) the\naccumulation of errors in the cache and in the disks.\n", "category": [0]}
{"abstract": "  In this paper we study the Metropolis algorithm in connection with two\nmean--field spin systems, the so called mean--field Ising model and the\nBlume--Emery--Griffiths model. In both this examples the naive choice of\nproposal chain gives rise, for some parameters, to a slowly mixing Metropolis\nchain, that is a chain whose spectral gap decreases exponentially fast (in the\ndimension $N$ of the problem). Here we show how a slight variant in the\nproposal chain can avoid this problem, keeping the mean computational cost\nsimilar to the cost of the usual Metropolis. More precisely we prove that, with\na suitable variant in the proposal, the Metropolis chain has a spectral gap\nwhich decreases polynomially in 1/N. Using some symmetry structure of the\nenergy, the method rests on allowing appropriate jumps within the energy level\nof the starting state.\n", "category": [2, 2, 6, 6]}
{"abstract": "  Conditional independence models in the Gaussian case are algebraic varieties\nin the cone of positive definite covariance matrices. We study these varieties\nin the case of Bayesian networks, with a view towards generalizing the\nrecursive factorization theorem to situations with hidden variables. In the\ncase when the underlying graph is a tree, we show that the vanishing ideal of\nthe model is generated by the conditional independence statements implied by\ngraph. We also show that the ideal of any Bayesian network is homogeneous with\nrespect to a multigrading induced by a collection of upstream random variables.\nThis has a number of important consequences for hidden variable models.\nFinally, we relate the ideals of Bayesian networks to a number of classical\nconstructions in algebraic geometry including toric degenerations of the\nGrassmannian, matrix Schubert varieties, and secant varieties.\n", "category": [2, 2, 6]}
{"abstract": "  We investigate a one-parameter family of probability densities (related to\nthe Pareto distribution, which describes many natural phenomena) where the\nCramer-Rao inequality provides no information.\n", "category": [2, 6]}
{"abstract": "  We study fragmentation trees of Gibbs type. In the binary case, we identify\nthe most general Gibbs-type fragmentation tree with Aldous' beta-splitting\nmodel, which has an extended parameter range $\\beta>-2$ with respect to the\n${\\rm beta}(\\beta+1,\\beta+1)$ probability distributions on which it is based.\nIn the multifurcating case, we show that Gibbs fragmentation trees are\nassociated with the two-parameter Poisson--Dirichlet models for exchangeable\nrandom partitions of $\\mathbb {N}$, with an extended parameter range\n$0\\le\\alpha\\le1$, $\\theta\\ge-2\\alpha$ and $\\alpha<0$, $\\theta =-m\\alpha$, $m\\in\n\\mathbb {N}$.\n", "category": [2, 2, 6]}
{"abstract": "  In a sensor network, in practice, the communication among sensors is subject\nto:(1) errors or failures at random times; (3) costs; and(2) constraints since\nsensors and networks operate under scarce resources, such as power, data rate,\nor communication. The signal-to-noise ratio (SNR) is usually a main factor in\ndetermining the probability of error (or of communication failure) in a link.\nThese probabilities are then a proxy for the SNR under which the links operate.\nThe paper studies the problem of designing the topology, i.e., assigning the\nprobabilities of reliable communication among sensors (or of link failures) to\nmaximize the rate of convergence of average consensus, when the link\ncommunication costs are taken into account, and there is an overall\ncommunication budget constraint. To consider this problem, we address a number\nof preliminary issues: (1) model the network as a random topology; (2)\nestablish necessary and sufficient conditions for mean square sense (mss) and\nalmost sure (a.s.) convergence of average consensus when network links fail;\nand, in particular, (3) show that a necessary and sufficient condition for both\nmss and a.s. convergence is for the algebraic connectivity of the mean graph\ndescribing the network topology to be strictly positive. With these results, we\nformulate topology design, subject to random link failures and to a\ncommunication cost constraint, as a constrained convex optimization problem to\nwhich we apply semidefinite programming techniques. We show by an extensive\nnumerical study that the optimal design improves significantly the convergence\nspeed of the consensus algorithm and can achieve the asymptotic performance of\na non-random network at a fraction of the communication cost.\n", "category": [0, 0, 2]}
{"abstract": "  MIMO technology is one of the most significant advances in the past decade to\nincrease channel capacity and has a great potential to improve network capacity\nfor mesh networks. In a MIMO-based mesh network, the links outgoing from each\nnode sharing the common communication spectrum can be modeled as a Gaussian\nvector broadcast channel. Recently, researchers showed that ``dirty paper\ncoding'' (DPC) is the optimal transmission strategy for Gaussian vector\nbroadcast channels. So far, there has been little study on how this fundamental\nresult will impact the cross-layer design for MIMO-based mesh networks. To fill\nthis gap, we consider the problem of jointly optimizing DPC power allocation in\nthe link layer at each node and multihop/multipath routing in a MIMO-based mesh\nnetworks. It turns out that this optimization problem is a very challenging\nnon-convex problem. To address this difficulty, we transform the original\nproblem to an equivalent problem by exploiting the channel duality. For the\ntransformed problem, we develop an efficient solution procedure that integrates\nLagrangian dual decomposition method, conjugate gradient projection method\nbased on matrix differential calculus, cutting-plane method, and subgradient\nmethod. In our numerical example, it is shown that we can achieve a network\nperformance gain of 34.4% by using DPC.\n", "category": [0, 0, 2]}
{"abstract": "  Advances in semiconductor technology are contributing to the increasing\ncomplexity in the design of embedded systems. Architectures with novel\ntechniques such as evolvable nature and autonomous behavior have engrossed lot\nof attention. This paper demonstrates conceptually evolvable embedded systems\ncan be characterized basing on acausal nature. It is noted that in acausal\nsystems, future input needs to be known, here we make a mechanism such that the\nsystem predicts the future inputs and exhibits pseudo acausal nature. An\nembedded system that uses theoretical framework of acausality is proposed. Our\nmethod aims at a novel architecture that features the hardware evolability and\nautonomous behavior alongside pseudo acausality. Various aspects of this\narchitecture are discussed in detail along with the limitations.\n", "category": [0, 0]}
{"abstract": "  The on-line shortest path problem is considered under various models of\npartial monitoring. Given a weighted directed acyclic graph whose edge weights\ncan change in an arbitrary (adversarial) way, a decision maker has to choose in\neach round of a game a path between two distinguished vertices such that the\nloss of the chosen path (defined as the sum of the weights of its composing\nedges) be as small as possible. In a setting generalizing the multi-armed\nbandit problem, after choosing a path, the decision maker learns only the\nweights of those edges that belong to the chosen path. For this problem, an\nalgorithm is given whose average cumulative loss in n rounds exceeds that of\nthe best path, matched off-line to the entire sequence of the edge weights, by\na quantity that is proportional to 1/\\sqrt{n} and depends only polynomially on\nthe number of edges of the graph. The algorithm can be implemented with linear\ncomplexity in the number of rounds n and in the number of edges. An extension\nto the so-called label efficient setting is also given, in which the decision\nmaker is informed about the weights of the edges corresponding to the chosen\npath at a total of m << n time instances. Another extension is shown where the\ndecision maker competes against a time-varying path, a generalization of the\nproblem of tracking the best expert. A version of the multi-armed bandit\nsetting for shortest path is also discussed where the decision maker learns\nonly the total weight of the chosen path but not the weights of the individual\nedges on the path. Applications to routing in packet switched networks along\nwith simulation results are also presented.\n", "category": [0, 0]}
{"abstract": "  Ordinal regression is an important type of learning, which has properties of\nboth classification and regression. Here we describe a simple and effective\napproach to adapt a traditional neural network to learn ordinal categories. Our\napproach is a generalization of the perceptron method for ordinal regression.\nOn several benchmark datasets, our method (NNRank) outperforms a neural network\nclassification method. Compared with the ordinal regression methods using\nGaussian processes and support vector machines, NNRank achieves comparable\nperformance. Moreover, NNRank has the advantages of traditional neural\nnetworks: learning in both online and batch modes, handling very large training\ndatasets, and making rapid predictions. These features make NNRank a useful and\ncomplementary tool for large-scale data processing tasks such as information\nretrieval, web page ranking, collaborative filtering, and protein ranking in\nBioinformatics.\n", "category": [0, 0, 0]}
{"abstract": "  A drawback of Kolmogorov-Chaitin complexity (K) as a function from s to the\nshortest program producing s is its noncomputability which limits its range of\napplicability. Moreover, when strings are short, the dependence of K on a\nparticular universal Turing machine U can be arbitrary. In practice one can\napproximate it by computable compression methods. However, such compression\nmethods do not always provide meaningful approximations--for strings shorter,\nfor example, than typical compiler lengths. In this paper we suggest an\nempirical approach to overcome this difficulty and to obtain a stable\ndefinition of the Kolmogorov-Chaitin complexity for short sequences.\nAdditionally, a correlation in terms of distribution frequencies was found\nacross the output of two models of abstract machines, namely unidimensional\ncellular automata and deterministic Turing machine.\n", "category": [0, 0, 2]}
{"abstract": "  Efficiently computing fast paths in large scale dynamic road networks (where\ndynamic traffic information is known over a part of the network) is a practical\nproblem faced by several traffic information service providers who wish to\noffer a realistic fast path computation to GPS terminal enabled vehicles. The\nheuristic solution method we propose is based on a highway hierarchy-based\nshortest path algorithm for static large-scale networks; we maintain a static\nhighway hierarchy and perform each query on the dynamically evaluated network.\n", "category": [0, 0]}
{"abstract": "  This paper is concerned with optimum diversity receiver structure and its\nperformance analysis of differential phase shift keying (DPSK) with\ndifferential detection over nonselective, independent, nonidentically\ndistributed, Rayleigh fading channels. The fading process in each branch is\nassumed to have an arbitrary Doppler spectrum with arbitrary Doppler bandwidth,\nbut to have distinct, asymmetric fading power spectral density characteristic.\nUsing 8-DPSK as an example, the average bit error probability (BEP) of the\noptimum diversity receiver is obtained by calculating the BEP for each of the\nthree individual bits. The BEP results derived are given in exact, explicit,\nclosed-form expressions which show clearly the behavior of the performance as a\nfunction of various system parameters.\n", "category": [0, 0, 2]}
{"abstract": "  We consider testing independence in group-wise selections with some\nrestrictions on combinations of choices. We present models for frequency data\nof selections for which it is easy to perform conditional tests by Markov chain\nMonte Carlo (MCMC) methods. When the restrictions on the combinations can be\ndescribed in terms of a Segre-Veronese configuration, an explicit form of a\nGr\\\"obner basis consisting of moves of degree two is readily available for\nperforming a Markov chain. We illustrate our setting with the National Center\nTest for university entrance examinations in Japan. We also apply our method to\ntesting independence hypotheses involving genotypes at more than one locus or\nhaplotypes of alleles on the same chromosome.\n", "category": [2, 2, 6, 6]}
{"abstract": "  We analyse the dependence of stock return cross-correlations on the sampling\nfrequency of the data known as the Epps effect: For high resolution data the\ncross-correlations are significantly smaller than their asymptotic value as\nobserved on daily data. The former description implies that changing trading\nfrequency should alter the characteristic time of the phenomenon. This is not\ntrue for the empirical data: The Epps curves do not scale with market activity.\nThe latter result indicates that the time scale of the phenomenon is connected\nto the reaction time of market participants (this we denote as human time\nscale), independent of market activity. In this paper we give a new description\nof the Epps effect through the decomposition of cross-correlations. After\ntesting our method on a model of generated random walk price changes we justify\nour analytical results by fitting the Epps curves of real world data.\n", "category": [5, 3, 3]}
{"abstract": "  This paper explores the following question: what kind of statistical\nguarantees can be given when doing variable selection in high-dimensional\nmodels? In particular, we look at the error rates and power of some multi-stage\nregression methods. In the first stage we fit a set of candidate models. In the\nsecond stage we select one model by cross-validation. In the third stage we use\nhypothesis testing to eliminate some variables. We refer to the first two\nstages as \"screening\" and the last stage as \"cleaning.\" We consider three\nscreening methods: the lasso, marginal regression, and forward stepwise\nregression. Our method gives consistent variable selection under certain\nconditions.\n", "category": [2, 6, 6]}
{"abstract": "  The subject of collective attention is central to an information age where\nmillions of people are inundated with daily messages. It is thus of interest to\nunderstand how attention to novel items propagates and eventually fades among\nlarge populations. We have analyzed the dynamics of collective attention among\none million users of an interactive website -- \\texttt{digg.com} -- devoted to\nthousands of novel news stories. The observations can be described by a\ndynamical model characterized by a single novelty factor. Our measurements\nindicate that novelty within groups decays with a stretched-exponential law,\nsuggesting the existence of a natural time scale over which attention fades.\n", "category": [0, 0, 3]}
{"abstract": "  The holographic bound in physics constrains the complexity of life. The\nfinite storage capability of information in the observable universe requires\nthe protein linguistics in the evolution of life. We find that the evolution of\ngenetic code determines the variance of amino acid frequencies and genomic GC\ncontent among species. The elegant linguistic mechanism is confirmed by the\nexperimental observations based on all known entire proteomes.\n", "category": [4, 3, 4]}
{"abstract": "  Hypervolume indicator is a commonly accepted quality measure for comparing\nPareto approximation set generated by multi-objective optimizers. The best\nknown algorithm to calculate it for $n$ points in $d$-dimensional space has a\nrun time of $O(n^{d/2})$ with special data structures. This paper presents a\nrecursive, vertex-splitting algorithm for calculating the hypervolume indicator\nof a set of $n$ non-comparable points in $d>2$ dimensions. It splits out\nmultiple child hyper-cuboids which can not be dominated by a splitting\nreference point. In special, the splitting reference point is carefully chosen\nto minimize the number of points in the child hyper-cuboids. The complexity\nanalysis shows that the proposed algorithm achieves $O((\\frac{d}{2})^n)$ time\nand $O(dn^2)$ space complexity in the worst case.\n", "category": [0, 0]}
{"abstract": "  We present a genetic algorithm which is distributed in two novel ways: along\ngenotype and temporal axes. Our algorithm first distributes, for every member\nof the population, a subset of the genotype to each network node, rather than a\nsubset of the population to each. This genotype distribution is shown to offer\na significant gain in running time. Then, for efficient use of the\ncomputational resources in the network, our algorithm divides the candidate\nsolutions into pipelined sets and thus the distribution is in the temporal\ndomain, rather that in the spatial domain. This temporal distribution may lead\nto temporal inconsistency in selection and replacement, however our experiments\nyield better efficiency in terms of the time to convergence without incurring\nsignificant penalties.\n", "category": [0, 0]}
{"abstract": "  The large-scale organization of the world economies is exhibiting\nincreasingly levels of local heterogeneity and global interdependency.\nUnderstanding the relation between local and global features calls for\nanalytical tools able to uncover the global emerging organization of the\ninternational trade network. Here we analyze the world network of bilateral\ntrade imbalances and characterize its overall flux organization, unraveling\nlocal and global high-flux pathways that define the backbone of the trade\nsystem. We develop a general procedure capable to progressively filter out in a\nconsistent and quantitative way the dominant trade channels. This procedure is\ncompletely general and can be applied to any weighted network to detect the\nunderlying structure of transport flows. The trade fluxes properties of the\nworld trade web determines a ranking of trade partnerships that highlights\nglobal interdependencies, providing information not accessible by simple local\nanalysis. The present work provides new quantitative tools for a dynamical\napproach to the propagation of economic crises.\n", "category": [5, 3, 3]}
{"abstract": "  There is a huge amount of historical documents in libraries and in various\nNational Archives that have not been exploited electronically. Although\nautomatic reading of complete pages remains, in most cases, a long-term\nobjective, tasks such as word spotting, text/image alignment, authentication\nand extraction of specific fields are in use today. For all these tasks, a\nmajor step is document segmentation into text lines. Because of the low quality\nand the complexity of these documents (background noise, artifacts due to\naging, interfering lines),automatic text line segmentation remains an open\nresearch field. The objective of this paper is to present a survey of existing\nmethods, developed during the last decade, and dedicated to documents of\nhistorical interest.\n", "category": [0]}
{"abstract": "  We consider the problem of coloring the vertices of a large sparse random\ngraph with a given number of colors so that no adjacent vertices have the same\ncolor. Using the cavity method, we present a detailed and systematic analytical\nstudy of the space of proper colorings (solutions).\n  We show that for a fixed number of colors and as the average vertex degree\n(number of constraints) increases, the set of solutions undergoes several phase\ntransitions similar to those observed in the mean field theory of glasses.\nFirst, at the clustering transition, the entropically dominant part of the\nphase space decomposes into an exponential number of pure states so that beyond\nthis transition a uniform sampling of solutions becomes hard. Afterward, the\nspace of solutions condenses over a finite number of the largest states and\nconsequently the total entropy of solutions becomes smaller than the annealed\none. Another transition takes place when in all the entropically dominant\nstates a finite fraction of nodes freezes so that each of these nodes is\nallowed a single color in all the solutions inside the state. Eventually, above\nthe coloring threshold, no more solutions are available. We compute all the\ncritical connectivities for Erdos-Renyi and regular random graphs and determine\ntheir asymptotic values for large number of colors.\n  Finally, we discuss the algorithmic consequences of our findings. We argue\nthat the onset of computational hardness is not associated with the clustering\ntransition and we suggest instead that the freezing transition might be the\nrelevant phenomenon. We also discuss the performance of a simple local Walk-COL\nalgorithm and of the belief propagation algorithm in the light of our results.\n", "category": [3, 3, 0]}
{"abstract": "  This paper uncovers and explores the close relationship between Monte Carlo\nOptimization of a parametrized integral (MCO), Parametric machine-Learning\n(PL), and `blackbox' or `oracle'-based optimization (BO). We make four\ncontributions. First, we prove that MCO is mathematically identical to a broad\nclass of PL problems. This identity potentially provides a new application\ndomain for all broadly applicable PL techniques: MCO. Second, we introduce\nimmediate sampling, a new version of the Probability Collectives (PC) algorithm\nfor blackbox optimization. Immediate sampling transforms the original BO\nproblem into an MCO problem. Accordingly, by combining these first two\ncontributions, we can apply all PL techniques to BO. In our third contribution\nwe validate this way of improving BO by demonstrating that cross-validation and\nbagging improve immediate sampling. Finally, conventional MC and MCO procedures\nignore the relationship between the sample point locations and the associated\nvalues of the integrand; only the values of the integrand at those locations\nare considered. We demonstrate that one can exploit the sample location\ninformation using PL techniques, for example by forming a fit of the sample\nlocations to the associated values of the integrand. This provides an\nadditional way to apply PL techniques to improve MCO.\n", "category": [0]}
{"abstract": "  Many organizations aspire to adopt agile processes to take advantage of the\nnumerous benefits that it offers to an organization. Those benefits include,\nbut are not limited to, quicker return on investment, better software quality,\nand higher customer satisfaction. To date however, there is no structured\nprocess (at least in the public domain) that guides organizations in adopting\nagile practices. To address this problem we present the Agile Adoption\nFramework. The framework consists of two components: an agile measurement\nindex, and a 4-Stage process, that together guide and assist the agile adoption\nefforts of organizations. More specifically, the agile measurement index is\nused to identify the agile potential of projects and organizations. The 4-Stage\nprocess, on the other hand, helps determine (a) whether or not organizations\nare ready for agile adoption, and (b) guided by their potential, what set of\nagile practices can and should be introduced.\n", "category": [0]}
{"abstract": "  A multiple antenna downlink channel where limited channel feedback is\navailable to the transmitter is considered. In a vector downlink channel\n(single antenna at each receiver), the transmit antenna array can be used to\ntransmit separate data streams to multiple receivers only if the transmitter\nhas very accurate channel knowledge, i.e., if there is high-rate channel\nfeedback from each receiver. In this work it is shown that channel feedback\nrequirements can be significantly reduced if each receiver has a small number\nof antennas and appropriately combines its antenna outputs. A combining method\nthat minimizes channel quantization error at each receiver, and thereby\nminimizes multi-user interference, is proposed and analyzed. This technique is\nshown to outperform traditional techniques such as maximum-ratio combining\nbecause minimization of interference power is more critical than maximization\nof signal power in the multiple antenna downlink. Analysis is provided to\nquantify the feedback savings, and the technique is seen to work well with user\nselection and is also robust to receiver estimation error.\n", "category": [0, 2]}
{"abstract": "  Low density lattice codes (LDLC) are novel lattice codes that can be decoded\nefficiently and approach the capacity of the additive white Gaussian noise\n(AWGN) channel. In LDLC a codeword x is generated directly at the n-dimensional\nEuclidean space as a linear transformation of a corresponding integer message\nvector b, i.e., x = Gb, where H, the inverse of G, is restricted to be sparse.\nThe fact that H is sparse is utilized to develop a linear-time iterative\ndecoding scheme which attains, as demonstrated by simulations, good error\nperformance within ~0.5dB from capacity at block length of n = 100,000 symbols.\nThe paper also discusses convergence results and implementation considerations.\n", "category": [0, 2]}
{"abstract": "  In this paper, we consider daily financial data of a collection of different\nstock market indices, exchange rates, and interest rates, and we analyze their\nmulti-scaling properties by estimating a simple specification of the\nMarkov-switching multifractal model (MSM). In order to see how well the\nestimated models capture the temporal dependence of the data, we estimate and\ncompare the scaling exponents $H(q)$ (for $q = 1, 2$) for both empirical data\nand simulated data of the estimated MSM models. In most cases the multifractal\nmodel appears to generate `apparent' long memory in agreement with the\nempirical scaling laws.\n", "category": [5, 3]}
{"abstract": "  Using particle system methodologies we study the propagation of financial\ndistress in a network of firms facing credit risk. We investigate the\nphenomenon of a credit crisis and quantify the losses that a bank may suffer in\na large credit portfolio. Applying a large deviation principle we compute the\nlimiting distributions of the system and determine the time evolution of the\ncredit quality indicators of the firms, deriving moreover the dynamics of a\nglobal financial health indicator. We finally describe a suitable version of\nthe \"Central Limit Theorem\" useful to study large portfolio losses. Simulation\nresults are provided as well as applications to portfolio loss distribution\nanalysis.\n", "category": [5, 2]}
{"abstract": "  This paper reports on work aimed at supporting knowledge and expertise\nfinding within a large Research and Development (R&D) organisation. The paper\nfirst discusses the nature of knowledge important to R&D organisations and\npresents a prototype information system developed to support knowledge and\nexpertise finding. The paper then discusses a trial of the system within an R&D\norganisation, the implications and limitations of the trial, and discusses\nfuture research questions.\n", "category": [0, 0, 0, 0]}
{"abstract": "  Distance-preserving mappings (DPMs) are mappings from the set of all q-ary\nvectors of a fixed length to the set of permutations of the same or longer\nlength such that every two distinct vectors are mapped to permutations with the\nsame or even larger Hamming distance than that of the vectors. In this paper,\nwe propose a construction of DPMs from ternary vectors. The constructed DPMs\nimprove the lower bounds on the maximal size of permutation arrays.\n", "category": [0, 0, 2]}
{"abstract": "  We study an efficient dynamic blind source separation algorithm of\nconvolutive sound mixtures based on updating statistical information in the\nfrequency domain, andminimizing the support of time domain demixing filters by\na weighted least square method. The permutation and scaling indeterminacies of\nseparation, and concatenations of signals in adjacent time frames are resolved\nwith optimization of $l^1 \\times l^\\infty$ norm on cross-correlation\ncoefficients at multiple time lags. The algorithm is a direct method without\niterations, and is adaptive to the environment. Computations on recorded and\nsynthetic mixtures of speech and music signals show excellent performance.\n", "category": [2, 2, 6, 6]}
{"abstract": "  A fast and accurate computational scheme for simulating nonlinear dynamic\nsystems is presented. The scheme assumes that the system can be represented by\na combination of components of only two different types: first-order low-pass\nfilters and static nonlinearities. The parameters of these filters and\nnonlinearities may depend on system variables, and the topology of the system\nmay be complex, including feedback. Several examples taken from neuroscience\nare given: phototransduction, photopigment bleaching, and spike generation\naccording to the Hodgkin-Huxley equations. The scheme uses two slightly\ndifferent forms of autoregressive filters, with an implicit delay of zero for\nfeedforward control and an implicit delay of half a sample distance for\nfeedback control. On a fairly complex model of the macaque retinal horizontal\ncell it computes, for a given level of accuracy, 1-2 orders of magnitude faster\nthan 4th-order Runge-Kutta. The computational scheme has minimal memory\nrequirements, and is also suited for computation on a stream processor, such as\na GPU (Graphical Processing Unit).\n", "category": [4, 4]}
{"abstract": "  The secure and robust functioning of a network relies on the defect-free\nimplementation of network applications. As network protocols have become\nincreasingly complex, however, hand-writing network message processing code has\nbecome increasingly error-prone. In this paper, we present a domain-specific\nlanguage, Zebu, for describing protocol message formats and related processing\nconstraints. From a Zebu specification, a compiler automatically generates\nstubs to be used by an application to parse network messages. Zebu is easy to\nuse, as it builds on notations used in RFCs to describe protocol grammars. Zebu\nis also efficient, as the memory usage is tailored to application needs and\nmessage fragments can be specified to be processed on demand. Finally,\nZebu-based applications are robust, as the Zebu compiler automatically checks\nspecification consistency and generates parsing stubs that include validation\nof the message structure. Using a mutation analysis in the context of SIP and\nRTSP, we show that Zebu significantly improves application robustness.\n", "category": [0]}
{"abstract": "  In 1948, W. Hoeffding introduced a large class of unbiased estimators called\nU-statistics, defined as the average value of a real-valued k-variate function\nh calculated at all possible sets of k points from a random sample. In the\npresent paper we investigate the corresponding extreme value analogue, which we\nshall call U-max-statistics. We are concerned with the behavior of the largest\nvalue of such function h instead of its average. Examples of U-max-statistics\nare the diameter or the largest scalar product within a random sample.\nU-max-statistics of higher degrees are given by triameters and other metric\ninvariants.\n", "category": [2, 2, 6]}
{"abstract": "  We present a simple and generic theoretical description of actin-based\nmotility, where polymerization of filaments maintains propulsion. The dynamics\nis driven by polymerization kinetics at the filaments' free ends, crosslinking\nof the actin network, attachment and detachment of filaments to the obstacle\ninterfaces and entropic forces. We show that spontaneous oscillations in the\nvelocity emerge in a broad range of parameter values, and compare our findings\nwith experiments.\n", "category": [4]}
{"abstract": "  In these notes we formally describe the functionality of Calculating Valid\nDomains from the BDD representing the solution space of valid configurations.\nThe formalization is largely based on the CLab configuration framework.\n", "category": [0]}
{"abstract": "  This paper has been withdrawn by the author. This draft is withdrawn for its\npoor quality in english, unfortunately produced by the author when he was just\nstarting his science route. Look at the ICML version instead:\nhttp://icml2008.cs.helsinki.fi/papers/111.pdf\n", "category": [0, 0]}
{"abstract": "  Most design approaches for trellis-coded quantization take advantage of the\nduality of trellis-coded quantization with trellis-coded modulation, and use\nthe same empirically-found convolutional codes to label the trellis branches.\nThis letter presents an alternative approach that instead takes advantage of\nmaximum-Hamming-distance convolutional codes. The proposed source codes are\nshown to be competitive with the best in the literature for the same\ncomputational complexity.\n", "category": [0, 2]}
{"abstract": "  Several representations of the exact cdf of the sum of squares of n\nindependent gamma-distributed random variables Xi are given, in particular by a\nseries of gamma distribution functions. Using a characterization of the gamma\ndistribution by Laha, an expansion of the exact distribution of the sample\nvariance is derived by a Taylor series approach with the former distribution as\nits leading term. In particular for integer orders alpha some further series\nare provided, including a convex combination of gamma distributions for alpha =\n1 and nearly of this type for alpha > 1. Furthermore, some representations of\nthe distribution of the angle Phi between (X1,...,Xn) and (1,...,1) are given\nby orthogonal series. All these series are based on the same sequence of easily\ncomputed moments of cos(Phi).\n", "category": [2, 6]}
{"abstract": "  Taking advantage of the recent litterature on exact simulation algorithms\n(Beskos, Papaspiliopoulos and Roberts) and unbiased estimation of the\nexpectation of certain fonctional integrals (Wagner, Beskos et al. and\nFearnhead et al.), we apply an exact simulation based technique for pricing\ncontinuous arithmetic average Asian options in the Black and Scholes framework.\nUnlike existing Monte Carlo methods, we are no longer prone to the\ndiscretization bias resulting from the approximation of continuous time\nprocesses through discrete sampling. Numerical results of simulation studies\nare presented and variance reduction problems are considered.\n", "category": [5, 2]}
{"abstract": "  We consider the problem of estimating the probability of an observed string\ndrawn i.i.d. from an unknown distribution. The key feature of our study is that\nthe length of the observed string is assumed to be of the same order as the\nsize of the underlying alphabet. In this setting, many letters are unseen and\nthe empirical distribution tends to overestimate the probability of the\nobserved letters. To overcome this problem, the traditional approach to\nprobability estimation is to use the classical Good-Turing estimator. We\nintroduce a natural scaling model and use it to show that the Good-Turing\nsequence probability estimator is not consistent. We then introduce a novel\nsequence probability estimator that is indeed consistent under the natural\nscaling model.\n", "category": [0, 2]}
{"abstract": "  We point out some pitfalls related to the concept of an oracle property as\nused in Fan and Li (2001, 2002, 2004) which are reminiscent of the well-known\npitfalls related to Hodges' estimator. The oracle property is often a\nconsequence of sparsity of an estimator. We show that any estimator satisfying\na sparsity property has maximal risk that converges to the supremum of the loss\nfunction; in particular, the maximal risk diverges to infinity whenever the\nloss function is unbounded. For ease of presentation the result is set in the\nframework of a linear regression model, but generalizes far beyond that\nsetting. In a Monte Carlo study we also assess the extent of the problem in\nfinite samples for the smoothly clipped absolute deviation (SCAD) estimator\nintroduced in Fan and Li (2001). We find that this estimator can perform rather\npoorly in finite samples and that its worst-case performance relative to\nmaximum likelihood deteriorates with increasing sample size when the estimator\nis tuned to sparsity.\n", "category": [2, 6, 6]}
{"abstract": "  This paper presents new low-complexity lattice-decoding algorithms for\nnoncoherent block detection of QAM and PAM signals over complex-valued fading\nchannels. The algorithms are optimal in terms of the generalized likelihood\nratio test (GLRT). The computational complexity is polynomial in the block\nlength; making GLRT-optimal noncoherent detection feasible for implementation.\nWe also provide even lower complexity suboptimal algorithms. Simulations show\nthat the suboptimal algorithms have performance indistinguishable from the\noptimal algorithms. Finally, we consider block based transmission, and propose\nto use noncoherent detection as an alternative to pilot assisted transmission\n(PAT). The new technique is shown to outperform PAT.\n", "category": [0, 2]}
{"abstract": "  Conformational transitions in macromolecular complexes often involve the\nreorientation of lever-like structures. Using a simple theoretical model, we\nshow that the rate of such transitions is drastically enhanced if the lever is\nbendable, e.g. at a localized \"hinge''. Surprisingly, the transition is fastest\nwith an intermediate flexibility of the hinge. In this intermediate regime, the\ntransition rate is also least sensitive to the amount of \"cargo'' attached to\nthe lever arm, which could be exploited by molecular motors. To explain this\neffect, we generalize the Kramers-Langer theory for multi-dimensional barrier\ncrossing to configuration dependent mobility matrices.\n", "category": [4]}
{"abstract": "  We propose a combined experimental (Atomic Force Microscopy) and theoretical\nstudy of the structural and dynamical properties of nucleosomes. In contrast to\nbiochemical approaches, this method allows to determine simultaneously the DNA\ncomplexed length distribution and nucleosome position in various contexts.\nFirst, we show that differences in the nucleo-proteic structure observed\nbetween conventional H2A and H2A.Bbd variant nucleosomes induce quantitative\nchanges in the in the length distribution of DNA complexed with histones. Then,\nthe sliding action of remodeling complex SWI/SNF is characterized through the\nevolution of the nucleosome position and wrapped DNA length mapping. Using a\nlinear energetic model for the distribution of DNA complexed length, we extract\nthe net wrapping energy of DNA onto the histone octamer, and compare it to\nprevious studies.\n", "category": [3, 3, 4]}
{"abstract": "  The class of 2-interval graphs has been introduced for modelling scheduling\nand allocation problems, and more recently for specific bioinformatic problems.\nSome of those applications imply restrictions on the 2-interval graphs, and\njustify the introduction of a hierarchy of subclasses of 2-interval graphs that\ngeneralize line graphs: balanced 2-interval graphs, unit 2-interval graphs, and\n(x,x)-interval graphs. We provide instances that show that all the inclusions\nare strict. We extend the NP-completeness proof of recognizing 2-interval\ngraphs to the recognition of balanced 2-interval graphs. Finally we give hints\non the complexity of unit 2-interval graphs recognition, by studying\nrelationships with other graph classes: proper circular-arc, quasi-line graphs,\nK_{1,5}-free graphs, ...\n", "category": [0, 4]}
{"abstract": "  We consider the problem of estimating the unconditional distribution of a\npost-model-selection estimator. The notion of a post-model-selection estimator\nhere refers to the combined procedure resulting from first selecting a model\n(e.g., by a model selection criterion like AIC or by a hypothesis testing\nprocedure) and then estimating the parameters in the selected model (e.g., by\nleast-squares or maximum likelihood), all based on the same data set. We show\nthat it is impossible to estimate the unconditional distribution with\nreasonable accuracy even asymptotically. In particular, we show that no\nestimator for this distribution can be uniformly consistent (not even locally).\nThis follows as a corollary to (local) minimax lower bounds on the performance\nof estimators for the distribution; performance is here measured by the\nprobability that the estimation error exceeds a given threshold. These lower\nbounds are shown to approach 1/2 or even 1 in large samples, depending on the\nsituation considered. Similar impossibility results are also obtained for the\ndistribution of linear functions (e.g., predictors) of the post-model-selection\nestimator.\n", "category": [2, 6, 6]}
{"abstract": "  Fluctuations in the abundance of molecules in the living cell may affect its\ngrowth and well being. For regulatory molecules (e.g., signaling proteins or\ntranscription factors), fluctuations in their expression can affect the levels\nof downstream targets in a network. Here, we develop an analytic framework to\ninvestigate the phenomenon of noise correlation in molecular networks.\nSpecifically, we focus on the metabolic network, which is highly inter-linked,\nand noise properties may constrain its structure and function. Motivated by the\nanalogy between the dynamics of a linear metabolic pathway and that of the\nexactly soluable linear queueing network or, alternatively, a mass transfer\nsystem, we derive a plethora of results concerning fluctuations in the\nabundance of intermediate metabolites in various common motifs of the metabolic\nnetwork. For all but one case examined, we find the steady-state fluctuation in\ndifferent nodes of the pathways to be effectively uncorrelated. Consequently,\nfluctuations in enzyme levels only affect local properties and do not propagate\nelsewhere into metabolic networks, and intermediate metabolites can be freely\nshared by different reactions. Our approach may be applicable to study\nmetabolic networks with more complex topologies, or protein signaling networks\nwhich are governed by similar biochemical reactions. Possible implications for\nbioinformatic analysis of metabolimic data are discussed.\n", "category": [4, 3]}
{"abstract": "  Information integration applications, such as mediators or mashups, that\nrequire access to information resources currently rely on users manually\ndiscovering and integrating them in the application. Manual resource discovery\nis a slow process, requiring the user to sift through results obtained via\nkeyword-based search. Although search methods have advanced to include evidence\nfrom document contents, its metadata and the contents and link structure of the\nreferring pages, they still do not adequately cover information sources --\noften called ``the hidden Web''-- that dynamically generate documents in\nresponse to a query. The recently popular social bookmarking sites, which allow\nusers to annotate and share metadata about various information sources, provide\nrich evidence for resource discovery. In this paper, we describe a\nprobabilistic model of the user annotation process in a social bookmarking\nsystem del.icio.us. We then use the model to automatically find resources\nrelevant to a particular information domain. Our experimental results on data\nobtained from \\emph{del.icio.us} show this approach as a promising method for\nhelping automate the resource discovery task.\n", "category": [0, 0, 0]}
{"abstract": "  The social media site Flickr allows users to upload their photos, annotate\nthem with tags, submit them to groups, and also to form social networks by\nadding other users as contacts. Flickr offers multiple ways of browsing or\nsearching it. One option is tag search, which returns all images tagged with a\nspecific keyword. If the keyword is ambiguous, e.g., ``beetle'' could mean an\ninsect or a car, tag search results will include many images that are not\nrelevant to the sense the user had in mind when executing the query. We claim\nthat users express their photography interests through the metadata they add in\nthe form of contacts and image annotations. We show how to exploit this\nmetadata to personalize search results for the user, thereby improving search\nperformance. First, we show that we can significantly improve search precision\nby filtering tag search results by user's contacts or a larger social network\nthat includes those contact's contacts. Secondly, we describe a probabilistic\nmodel that takes advantage of tag information to discover latent topics\ncontained in the search results. The users' interests can similarly be\ndescribed by the tags they used for annotating their images. The latent topics\nfound by the model are then used to personalize search results by finding\nimages on topics that are of interest to the user.\n", "category": [0, 0, 0, 0, 0]}
{"abstract": "  We settle a long-standing open question in algorithmic game theory. We prove\nthat Bimatrix, the problem of finding a Nash equilibrium in a two-player game,\nis complete for the complexity class PPAD Polynomial Parity Argument, Directed\nversion) introduced by Papadimitriou in 1991.\n  This is the first of a series of results concerning the complexity of Nash\nequilibria. In particular, we prove the following theorems:\n  Bimatrix does not have a fully polynomial-time approximation scheme unless\nevery problem in PPAD is solvable in polynomial time. The smoothed complexity\nof the classic Lemke-Howson algorithm and, in fact, of any algorithm for\nBimatrix is not polynomial unless every problem in PPAD is solvable in\nrandomized polynomial time. Our results demonstrate that, even in the simplest\nform of non-cooperative games, equilibrium computation and approximation are\npolynomial-time equivalent to fixed point computation. Our results also have\ntwo broad complexity implications in mathematical economics and operations\nresearch: Arrow-Debreu market equilibria are PPAD-hard to compute. The P-Matrix\nLinear Complementary Problem is computationally harder than convex programming\nunless every problem in PPAD is solvable in polynomial time.\n", "category": [0, 0]}
{"abstract": "  A k-query Locally Decodable Code (LDC) encodes an n-bit message x as an N-bit\ncodeword C(x), such that one can probabilistically recover any bit x_i of the\nmessage by querying only k bits of the codeword C(x), even after some constant\nfraction of codeword bits has been corrupted. The major goal of LDC related\nresearch is to establish the optimal trade-off between length and query\ncomplexity of such codes.\n  Recently [Y] introduced a novel technique for constructing locally decodable\ncodes and vastly improved the upper bounds for code length. The technique is\nbased on Mersenne primes. In this paper we extend the work of [Y] and argue\nthat further progress via these methods is tied to progress on an old number\ntheory question regarding the size of the largest prime factors of Mersenne\nnumbers.\n  Specifically, we show that every Mersenne number m=2^t-1 that has a prime\nfactor p>m^\\gamma yields a family of k(\\gamma)-query locally decodable codes of\nlength Exp(n^{1/t}). Conversely, if for some fixed k and all \\epsilon > 0 one\ncan use the technique of [Y] to obtain a family of k-query LDCs of length\nExp(n^\\epsilon); then infinitely many Mersenne numbers have prime factors arger\nthan known currently.\n", "category": [0, 2]}
{"abstract": "  The SOM algorithm is very astonishing. On the one hand, it is very simple to\nwrite down and to simulate, its practical properties are clear and easy to\nobserve. But, on the other hand, its theoretical properties still remain\nwithout proof in the general case, despite the great efforts of several\nauthors. In this paper, we pass in review the last results and provide some\nconjectures for the future work.\n", "category": [2, 6]}
{"abstract": "  Bi-intuitionistic logic is the extension of intuitionistic logic with a\nconnective dual to implication. Bi-intuitionistic logic was introduced by\nRauszer as a Hilbert calculus with algebraic and Kripke semantics. But her\nsubsequent ``cut-free'' sequent calculus for BiInt has recently been shown by\nUustalu to fail cut-elimination. We present a new cut-free sequent calculus for\nBiInt, and prove it sound and complete with respect to its Kripke semantics.\nEnsuring completeness is complicated by the interaction between implication and\nits dual, similarly to future and past modalities in tense logic. Our calculus\nhandles this interaction using extended sequents which pass information from\npremises to conclusions using variables instantiated at the leaves of failed\nderivation trees. Our simple termination argument allows our calculus to be\nused for automated deduction, although this is not its main purpose.\n", "category": [0]}
{"abstract": "  Nous montrons comment il est possible d'utiliser l'algorithme d'auto\norganisation de Kohonen pour traiter des donn\\'ees avec valeurs manquantes et\nestimer ces derni\\`eres. Apr\\`es un rappel m\\'ethodologique, nous illustrons\nnotre propos \\`a partir de trois applications \\`a des donn\\'ees r\\'eelles.\n  -----\n  We show how it is possible to use the Kohonen self-organizing algorithm to\ndeal with data which contain missing values and to estimate them. After a\nmethodological recall, we illustrate our purpose from three real databases\napplications.\n", "category": [6, 0]}
{"abstract": "  The paper deals with the study of labor market dynamics, and aims to\ncharacterize its equilibriums and possible trajectories. The theoretical\nbackground is the theory of the segmented labor market. The main idea is that\nthis theory is well adapted to interpret the observed trajectories, due to the\nheterogeneity of the work situations.\n", "category": [6]}
{"abstract": "  Prices of commodities or assets produce what is called time-series. Different\nkinds of financial time-series have been recorded and studied for decades.\nNowadays, all transactions on a financial market are recorded, leading to a\nhuge amount of data available, either for free in the Internet or commercially.\nFinancial time-series analysis is of great interest to practitioners as well as\nto theoreticians, for making inferences and predictions. Furthermore, the\nstochastic uncertainties inherent in financial time-series and the theory\nneeded to deal with them make the subject especially interesting not only to\neconomists, but also to statisticians and physicists. While it would be a\nformidable task to make an exhaustive review on the topic, with this review we\ntry to give a flavor of some of its aspects.\n", "category": [5, 3]}
{"abstract": "  As networks and their structure have become a major field of research, a\nstrong demand for network visualization has emerged. We address this challenge\nby formalizing the well established spring layout in terms of dynamic\nequations. We thus open up the design space for new algorithms. Drawing from\nthe knowledge of systems design, we derive a layout algorithm that remedies\nseveral drawbacks of the original spring layout. This new algorithm relies on\nthe balancing of two antagonistic forces. We thus call it {\\em arf} for\n\"attractive and repulsive forces\". It is, as we claim, particularly suited for\na dynamic layout of smaller networks ($n < 10^3$). We back this claim with\nseveral application examples from on going complex systems research.\n", "category": [3, 0, 3]}
{"abstract": "  While most useful information theoretic inequalities can be deduced from the\nbasic properties of entropy or mutual information, up to now Shannon's entropy\npower inequality (EPI) is an exception: Existing information theoretic proofs\nof the EPI hinge on representations of differential entropy using either Fisher\ninformation or minimum mean-square error (MMSE), which are derived from de\nBruijn's identity. In this paper, we first present an unified view of these\nproofs, showing that they share two essential ingredients: 1) a data processing\nargument applied to a covariance-preserving linear transformation; 2) an\nintegration over a path of a continuous Gaussian perturbation. Using these\ningredients, we develop a new and brief proof of the EPI through a mutual\ninformation inequality, which replaces Stam and Blachman's Fisher information\ninequality (FII) and an inequality for MMSE by Guo, Shamai and Verd\\'u used in\nearlier proofs. The result has the advantage of being very simple in that it\nrelies only on the basic properties of mutual information. These ideas are then\ngeneralized to various extended versions of the EPI: Zamir and Feder's\ngeneralized EPI for linear transformations of the random variables, Takano and\nJohnson's EPI for dependent variables, Liu and Viswanath's\ncovariance-constrained EPI, and Costa's concavity inequality for the entropy\npower.\n", "category": [0, 2]}
{"abstract": "  The Invar package is introduced, a fast manipulator of generic scalar\npolynomial expressions formed from the Riemann tensor of a four-dimensional\nmetric-compatible connection. The package can maximally simplify any polynomial\ncontaining tensor products of up to seven Riemann tensors within seconds. It\nhas been implemented both in Mathematica and Maple algebraic systems.\n", "category": [0, 3, 3]}
{"abstract": "  This paper aims to provide a practical example on the assessment and\npropagation of input uncertainty for option pricing when using tree-based\nmethods. Input uncertainty is propagated into output uncertainty, reflecting\nthat option prices are as unknown as the inputs they are based on. Option\npricing formulas are tools whose validity is conditional not only on how close\nthe model represents reality, but also on the quality of the inputs they use,\nand those inputs are usually not observable. We provide three alternative\nframeworks to calibrate option pricing tree models, propagating parameter\nuncertainty into the resulting option prices. We finally compare our methods\nwith classical calibration-based results assuming that there is no options\nmarket established. These methods can be applied to pricing of instruments for\nwhich there is not an options market, as well as a methodological tool to\naccount for parameter and model uncertainty in theoretical option pricing.\n", "category": [0, 0]}
{"abstract": "  We present a formal model to represent and solve the unicast/multicast\nrouting problem in networks with Quality of Service (QoS) requirements. To\nattain this, first we translate the network adapting it to a weighted graph\n(unicast) or and-or graph (multicast), where the weight on a connector\ncorresponds to the multidimensional cost of sending a packet on the related\nnetwork link: each component of the weights vector represents a different QoS\nmetric value (e.g. bandwidth, cost, delay, packet loss). The second step\nconsists in writing this graph as a program in Soft Constraint Logic\nProgramming (SCLP): the engine of this framework is then able to find the best\npaths/trees by optimizing their costs and solving the constraints imposed on\nthem (e.g. delay < 40msec), thus finding a solution to QoS routing problems.\nMoreover, c-semiring structures are a convenient tool to model QoS metrics. At\nlast, we provide an implementation of the framework over scale-free networks\nand we suggest how the performance can be improved.\n", "category": [0, 0, 0]}
{"abstract": "  Many important real-world networks manifest \"small-world\" properties such as\nscale-free degree distributions, small diameters, and clustering. The most\ncommon model of growth for these networks is \"preferential attachment\", where\nnodes acquire new links with probability proportional to the number of links\nthey already have. We show that preferential attachment is a special case of\nthe process of molecular evolution. We present a new single-parameter model of\nnetwork growth that unifies varieties of preferential attachment with the\nquasispecies equation (which models molecular evolution), and also with the\nErdos-Renyi random graph model. We suggest some properties of evolutionary\nmodels that might be applied to the study of networks. We also derive the form\nof the degree distribution resulting from our algorithm, and we show through\nsimulations that the process also models aspects of network growth. The\nunification allows mathematical machinery developed for evolutionary dynamics\nto be applied in the study of network dynamics, and vice versa.\n", "category": [4, 4]}
{"abstract": "  We describe and analyze the joint source/channel coding properties of a class\nof sparse graphical codes based on compounding a low-density generator matrix\n(LDGM) code with a low-density parity check (LDPC) code. Our first pair of\ntheorems establish that there exist codes from this ensemble, with all degrees\nremaining bounded independently of block length, that are simultaneously\noptimal as both source and channel codes when encoding and decoding are\nperformed optimally. More precisely, in the context of lossy compression, we\nprove that finite degree constructions can achieve any pair $(R, D)$ on the\nrate-distortion curve of the binary symmetric source. In the context of channel\ncoding, we prove that finite degree codes can achieve any pair $(C, p)$ on the\ncapacity-noise curve of the binary symmetric channel. Next, we show that our\ncompound construction has a nested structure that can be exploited to achieve\nthe Wyner-Ziv bound for source coding with side information (SCSI), as well as\nthe Gelfand-Pinsker bound for channel coding with side information (CCSI).\nAlthough the current results are based on optimal encoding and decoding, the\nproposed graphical codes have sparse structure and high girth that renders them\nwell-suited to message-passing and other efficient decoding procedures.\n", "category": [0, 2]}
{"abstract": "  This paper analyses the possibilities of performing parallel\ntransaction-oriented simulations with a special focus on the space-parallel\napproach and discrete event simulation synchronisation algorithms that are\nsuitable for transaction-oriented simulation and the target environment of Ad\nHoc Grids. To demonstrate the findings a Java-based parallel\ntransaction-oriented simulator for the simulation language GPSS/H is\nimplemented on the basis of the promising Shock Resistant Time Warp\nsynchronisation algorithm and using the Grid framework ProActive. The\nvalidation of this parallel simulator shows that the Shock Resistant Time Warp\nalgorithm can successfully reduce the number of rolled back Transaction moves\nbut it also reveals circumstances in which the Shock Resistant Time Warp\nalgorithm can be outperformed by the normal Time Warp algorithm. The conclusion\nof this paper suggests possible improvements to the Shock Resistant Time Warp\nalgorithm to avoid such problems.\n", "category": [0]}
{"abstract": "  On-line chain partition is a two-player game between Spoiler and Algorithm.\nSpoiler presents a partially ordered set, point by point. Algorithm assigns\nincoming points (immediately and irrevocably) to the chains which constitute a\nchain partition of the order. The value of the game for orders of width $w$ is\na minimum number $\\fVal(w)$ such that Algorithm has a strategy using at most\n$\\fVal(w)$ chains on orders of width at most $w$. We analyze the chain\npartition game for up-growing semi-orders. Surprisingly, the golden ratio comes\ninto play and the value of the game is $\\lfloor\\frac{1+\\sqrt{5}}{2}\\; w\n\\rfloor$.\n", "category": [0]}
{"abstract": "  The IEEE 802.11e standard revises the Medium Access Control (MAC) layer of\nthe former IEEE 802.11 standard for Quality-of-Service (QoS) provision in the\nWireless Local Area Networks (WLANs). The Enhanced Distributed Channel Access\n(EDCA) function of 802.11e defines multiple Access Categories (AC) with\nAC-specific Contention Window (CW) sizes, Arbitration Interframe Space (AIFS)\nvalues, and Transmit Opportunity (TXOP) limits to support MAC-level QoS and\nprioritization. We propose an analytical model for the EDCA function which\nincorporates an accurate CW, AIFS, and TXOP differentiation at any traffic\nload. The proposed model is also shown to capture the effect of MAC layer\nbuffer size on the performance. Analytical and simulation results are compared\nto demonstrate the accuracy of the proposed approach for varying traffic loads,\nEDCA parameters, and MAC layer buffer space.\n", "category": [0]}
{"abstract": "  The recently ratified IEEE 802.11e standard defines the Enhanced Distributed\nChannel Access (EDCA) function for Quality-of-Service (QoS) provisioning in the\nWireless Local Area Networks (WLANs). The EDCA uses Carrier Sense Multiple\nAccess with Collision Avoidance (CSMA/CA) and slotted Binary Exponential\nBackoff (BEB) mechanism. We present a simple mathematical analysis framework\nfor the EDCA function. Our analysis considers the fact that the distributed\nrandom access systems exhibit cyclic behavior where each station successfully\ntransmits a packet in a cycle. Our analysis shows that an AC-specific cycle\ntime exists for the EDCA function. Validating the theoretical results via\nsimulations, we show that the proposed analysis accurately captures EDCA\nsaturation performance in terms of average throughput, medium access delay, and\npacket loss ratio. The cycle time analysis is a simple and insightful\nsubstitute for previously proposed more complex EDCA models.\n", "category": [0]}
{"abstract": "  Most of the deployed IEEE 802.11e Wireless Local Area Networks (WLANs) use\ninfrastructure Basic Service Set (BSS) in which an Access Point (AP) serves as\na gateway between wired and wireless domains. We present the unfairness problem\nbetween the uplink and the downlink flows of any Access Category (AC) in the\n802.11e Enhanced Distributed Channel Access (EDCA) when the default settings of\nthe EDCA parameters are used. We propose a simple analytical model to calculate\nthe EDCA parameter settings that achieve weighted fair resource allocation for\nall uplink and downlink flows. We also propose a simple model-assisted\nmeasurement-based dynamic EDCA parameter adaptation algorithm. Moreover, our\ndynamic solution addresses the differences in the transport layer and the\nMedium Access Control (MAC) layer interactions of User Datagram Protocol (UDP)\nand Transmission Control Protocol (TCP). We show that proposed Contention\nWindow (CW) and Transmit Opportunity (TXOP) limit adaptation at the AP provides\nfair UDP and TCP access between uplink and downlink flows of the same AC while\npreserving prioritization among ACs.\n", "category": [0]}
{"abstract": "  In this paper, we propose an achievable rate region for discrete memoryless\ninterference channels with conferencing at the transmitter side. We employ\nsuperposition block Markov encoding, combined with simultaneous superposition\ncoding, dirty paper coding, and random binning to obtain the achievable rate\nregion. We show that, under respective conditions, the proposed achievable\nregion reduces to Han and Kobayashi achievable region for interference\nchannels, the capacity region for degraded relay channels, and the capacity\nregion for the Gaussian vector broadcast channel. Numerical examples for the\nGaussian case are given.\n", "category": [0, 2]}
{"abstract": "  BACKGROUND: An important question is whether evolution favors properties such\nas mutational robustness or evolvability that do not directly benefit any\nindividual, but can influence the course of future evolution. Functionally\nsimilar proteins can differ substantially in their robustness to mutations and\ncapacity to evolve new functions, but it has remained unclear whether any of\nthese differences might be due to evolutionary selection for these properties.\n  RESULTS: Here we use laboratory experiments to demonstrate that evolution\nfavors protein mutational robustness if the evolving population is sufficiently\nlarge. We neutrally evolve cytochrome P450 proteins under identical selection\npressures and mutation rates in populations of different sizes, and show that\nproteins from the larger and thus more polymorphic population tend towards\nhigher mutational robustness. Proteins from the larger population also evolve\ngreater stability, a biophysical property that is known to enhance both\nmutational robustness and evolvability. The excess mutational robustness and\nstability is well described by existing mathematical theories, and can be\nquantitatively related to the way that the proteins occupy their neutral\nnetwork.\n  CONCLUSIONS: Our work is the first experimental demonstration of the general\ntendency of evolution to favor mutational robustness and protein stability in\nhighly polymorphic populations. We suggest that this phenomenon may contribute\nto the mutational robustness and evolvability of viruses and bacteria that\nexist in large populations.\n", "category": [4, 4]}
{"abstract": "  The Kripke semantics of classical propositional normal modal logic is made\nalgebraic via an embedding of Kripke structures into the larger class of\npointed stably supported quantales. This algebraic semantics subsumes the\ntraditional algebraic semantics based on lattices with unary operators, and it\nsuggests natural interpretations of modal logic, of possible interest in the\napplications, in structures that arise in geometry and analysis, such as\nfoliated manifolds and operator algebras, via topological groupoids and inverse\nsemigroups. We study completeness properties of the quantale based semantics\nfor the systems K, T, K4, S4, and S5, in particular obtaining an axiomatization\nfor S5 which does not use negation or the modal necessity operator. As\nadditional examples we describe intuitionistic propositional modal logic, the\nlogic of programs PDL, and the ramified temporal logic CTL.\n", "category": [2, 0, 2]}
{"abstract": "  A practical introduction to stochastic modelling of reaction-diffusion\nprocesses is presented. No prior knowledge of stochastic simulations is\nassumed. The methods are explained using illustrative examples. The article\nstarts with the classical Gillespie algorithm for the stochastic modelling of\nchemical reactions. Then stochastic algorithms for modelling molecular\ndiffusion are given. Finally, basic stochastic reaction-diffusion methods are\npresented. The connections between stochastic simulations and deterministic\nmodels are explained and basic mathematical tools (e.g. chemical master\nequation) are presented. The article concludes with an overview of more\nadvanced methods and problems.\n", "category": [4, 3, 4]}
{"abstract": "  Data on the prevalence of bone cancer in dinosaurs is available from past\nradiological examination of preserved bones. We statistically test this data\nfor consistency with rates extrapolated from information on bone cancer in\nmodern vertebrates, and find that there is no evidence of a different rate.\nThus, this test provides no support for a possible role of ionizing radiation\nin the K-T extinction event.\n", "category": [4, 3]}
{"abstract": "  In spatially distributed multiuser antenna systems, the received signal\ncontains multiple carrier-frequency offsets (CFOs) arising from mismatch\nbetween the oscillators of transmitters and receivers. This results in a\ntime-varying rotation of the data constellation, which needs to be compensated\nat the receiver before symbol recovery. In this paper, a new approach for blind\nCFO estimation and symbol recovery is proposed. The received base-band signal\nis over-sampled, and its polyphase components are used to formulate a virtual\nMultiple-Input Multiple-Output (MIMO) problem. By applying blind MIMO system\nestimation techniques, the system response can be estimated and decoupled\nversions of the user symbols can be recovered, each one of which contains a\ndistinct CFO. By applying a decision feedback Phase Lock Loop (PLL), the CFO\ncan be mitigated and the transmitted symbols can be recovered. The estimated\nMIMO system response provides information about the CFOs that can be used to\ninitialize the PLL, speed up its convergence, and avoid ambiguities usually\nlinked with PLL.\n", "category": [0, 2]}
{"abstract": "  A new framework for asset price dynamics is introduced in which the concept\nof noisy information about future cash flows is used to derive the price\nprocesses. In this framework an asset is defined by its cash-flow structure.\nEach cash flow is modelled by a random variable that can be expressed as a\nfunction of a collection of independent random variables called market factors.\nWith each such \"X-factor\" we associate a market information process, the values\nof which are accessible to market agents. Each information process is a sum of\ntwo terms; one contains true information about the value of the market factor;\nthe other represents \"noise\". The noise term is modelled by an independent\nBrownian bridge. The market filtration is assumed to be that generated by the\naggregate of the independent information processes. The price of an asset is\ngiven by the expectation of the discounted cash flows in the risk-neutral\nmeasure, conditional on the information provided by the market filtration. When\nthe cash flows are the dividend payments associated with equities, an explicit\nmodel is obtained for the share-price, and the prices of options on\ndividend-paying assets are derived. Remarkably, the resulting formula for the\nprice of a European call option is of the Black-Scholes-Merton type. The\ninformation-based framework also generates a natural explanation for the origin\nof stochastic volatility.\n", "category": [5, 2, 2, 6]}
{"abstract": "  The dynamics of many socioeconomic systems is determined by the decision\nmaking process of agents. The decision process depends on agent's\ncharacteristics, such as preferences, risk aversion, behavioral biases, etc..\nIn addition, in some systems the size of agents can be highly heterogeneous\nleading to very different impacts of agents on the system dynamics. The large\nsize of some agents poses challenging problems to agents who want to control\ntheir impact, either by forcing the system in a given direction or by hiding\ntheir intentionality. Here we consider the financial market as a model system,\nand we study empirically how agents strategically adjust the properties of\nlarge orders in order to meet their preference and minimize their impact. We\nquantify this strategic behavior by detecting scaling relations of allometric\nnature between the variables characterizing the trading activity of different\ninstitutions. We observe power law distributions in the investment time\nhorizon, in the number of transactions needed to execute a large order and in\nthe traded value exchanged by large institutions and we show that heterogeneity\nof agents is a key ingredient for the emergence of some aggregate properties\ncharacterizing this complex system.\n", "category": [5, 3]}
{"abstract": "  Motivation: Profile hidden Markov Models (pHMMs) are a popular and very\nuseful tool in the detection of the remote homologue protein families.\nUnfortunately, their performance is not always satisfactory when proteins are\nin the 'twilight zone'. We present HMMER-STRUCT, a model construction algorithm\nand tool that tries to improve pHMM performance by using structural information\nwhile training pHMMs. As a first step, HMMER-STRUCT constructs a set of pHMMs.\nEach pHMM is constructed by weighting each residue in an aligned protein\naccording to a specific structural property of the residue. Properties used\nwere primary, secondary and tertiary structures, accessibility and packing.\nHMMER-STRUCT then prioritizes the results by voting. Results: We used the SCOP\ndatabase to perform our experiments. Throughout, we apply leave-one-family-out\ncross-validation over protein superfamilies. First, we used the MAMMOTH-mult\nstructural aligner to align the training set proteins. Then, we performed two\nsets of experiments. In a first experiment, we compared structure weighted\nmodels against standard pHMMs and against each other. In a second experiment,\nwe compared the voting model against individual pHMMs. We compare method\nperformance through ROC curves and through Precision/Recall curves, and assess\nsignificance through the paired two tailed t-test. Our results show significant\nperformance improvements of all structurally weighted models over default\nHMMER, and a significant improvement in sensitivity of the combined models over\nboth the original model and the structurally weighted models.\n", "category": [0]}
{"abstract": "  Standard game theory assumes that the structure of the game is common\nknowledge among players. We relax this assumption by considering extensive\ngames where agents may be unaware of the complete structure of the game. In\nparticular, they may not be aware of moves that they and other agents can make.\nWe show how such games can be represented; the key idea is to describe the game\nfrom the point of view of every agent at every node of the game tree. We\nprovide a generalization of Nash equilibrium and show that every game with\nawareness has a generalized Nash equilibrium. Finally, we extend these results\nto games with awareness of unawareness, where a player i may be aware that a\nplayer j can make moves that i is not aware of, and to subjective games, where\npayers may have no common knowledge regarding the actual game and their beliefs\nare incompatible with a common prior.\n", "category": [0, 0]}
{"abstract": "  This paper studies the performance of partial-Rake (PRake) receivers in\nimpulse-radio ultrawideband wireless networks when an energy-efficient power\ncontrol scheme is adopted. Due to the large bandwidth of the system, the\nmultipath channel is assumed to be frequency-selective. By using noncooperative\ngame-theoretic models and large system analysis, explicit expressions are\nderived in terms of network parameters to measure the effects of self- and\nmultiple-access interference at a receiving access point. Performance of the\nPRake is compared in terms of achieved utilities and loss to that of the\nall-Rake receiver.\n", "category": [0, 0, 2]}
{"abstract": "  In this paper Arabic was investigated from the speech recognition problem\npoint of view. We propose a novel approach to build an Arabic Automated Speech\nRecognition System (ASR). This system is based on the open source CMU Sphinx-4,\nfrom the Carnegie Mellon University. CMU Sphinx is a large-vocabulary;\nspeaker-independent, continuous speech recognition system based on discrete\nHidden Markov Models (HMMs). We build a model using utilities from the\nOpenSource CMU Sphinx. We will demonstrate the possible adaptability of this\nsystem to Arabic voice recognition.\n", "category": [0, 0]}
{"abstract": "  We consider inapproximability of the correlation clustering problem defined\nas follows: Given a graph $G = (V,E)$ where each edge is labeled either \"+\"\n(similar) or \"-\" (dissimilar), correlation clustering seeks to partition the\nvertices into clusters so that the number of pairs correctly (resp.\nincorrectly) classified with respect to the labels is maximized (resp.\nminimized). The two complementary problems are called MaxAgree and MinDisagree,\nrespectively, and have been studied on complete graphs, where every edge is\nlabeled, and general graphs, where some edge might not have been labeled.\nNatural edge-weighted versions of both problems have been studied as well. Let\nS-MaxAgree denote the weighted problem where all weights are taken from set S,\nwe show that S-MaxAgree with weights bounded by $O(|V|^{1/2-\\delta})$\nessentially belongs to the same hardness class in the following sense: if there\nis a polynomial time algorithm that approximates S-MaxAgree within a factor of\n$\\lambda = O(\\log{|V|})$ with high probability, then for any choice of S',\nS'-MaxAgree can be approximated in polynomial time within a factor of $(\\lambda\n+ \\epsilon)$, where $\\epsilon > 0$ can be arbitrarily small, with high\nprobability. A similar statement also holds for $S-MinDisagree. This result\nimplies it is hard (assuming $NP \\neq RP$) to approximate unweighted MaxAgree\nwithin a factor of $80/79-\\epsilon$, improving upon a previous known factor of\n$116/115-\\epsilon$ by Charikar et. al. \\cite{Chari05}.\n", "category": [0, 0]}
{"abstract": "  The cross-correlations between price fluctuations of 201 frequently traded\nstocks in the National Stock Exchange (NSE) of India are analyzed in this\npaper. We use daily closing prices for the period 1996-2006, which coincides\nwith the period of rapid transformation of the market following liberalization.\nThe eigenvalue distribution of the cross-correlation matrix, $\\mathbf{C}$, of\nNSE is found to be similar to that of developed markets, such as the New York\nStock Exchange (NYSE): the majority of eigenvalues fall within the bounds\nexpected for a random matrix constructed from mutually uncorrelated time\nseries. Of the few largest eigenvalues that deviate from the bulk, the largest\nis identified with market-wide movements. The intermediate eigenvalues that\noccur between the largest and the bulk have been associated in NYSE with\nspecific business sectors with strong intra-group interactions. However, in the\nIndian market, these deviating eigenvalues are comparatively very few and lie\nmuch closer to the bulk. We propose that this is because of the relative lack\nof distinct sector identity in the market, with the movement of stocks\ndominantly influenced by the overall market trend. This is shown by explicit\nconstruction of the interaction network in the market, first by generating the\nminimum spanning tree from the unfiltered correlation matrix, and later, using\nan improved method of generating the graph after filtering out the market mode\nand random effects from the data. Both methods show, compared to developed\nmarkets, the relative absence of clusters of co-moving stocks that belong to\nthe same business sector. This is consistent with the general belief that\nemerging markets tend to be more correlated than developed markets.\n", "category": [5, 3, 3, 3]}
{"abstract": "  Objectives: In this study, we quantify the growth variability of tumour cell\nclones from a human leukemia cell line. Materials and methods: We have used\nmicroplate spectrophotometry to measure the growth kinetics of hundreds of\nindividual cell clones from the Molt3 cell line. The growth rate of each clonal\npopulation has been estimated by fitting experimental data with the logistic\nequation. Results: The growth rates were observed to vary among different\nclones. Up to six clones with a growth rate above or below the mean growth rate\nof the parent population were further cloned and the growth rates of their\noffsprings were measured. The distribution of the growth rates of the subclones\ndid not significantly differ from that of the parent population thus suggesting\nthat growth variability has an epigenetic origin. To explain the observed\ndistributions of clonal growth rates we have developed a probabilistic model\nassuming that the fluctuations in the number of mitochondria through successive\ncell cycles are the leading cause of growth variability. For fitting purposes,\nwe have estimated experimentally by flow cytometry the maximum average number\nof mitochondria in Molt3 cells. The model fits nicely the observed\ndistributions of growth rates, however, cells in which the mitochondria were\nrendered non functional (rho-0 cells) showed only a 30% reduction in the clonal\ngrowth variability with respect to normal cells. Conclusions: A tumor cell\npopulation is a dynamic ensemble of clones with highly variable growth rate. At\nleast part of this variability is due to fluctuations in the number of\nmitochondria.\n", "category": [4, 4]}
{"abstract": "  In many professons employees are rewarded according to their relative\nperformance. Corresponding economy can be modeled by taking $N$ independent\nagents who gain from the market with a rate which depends on their current\ngain. We argue that this simple realistic rate generates a scale free\ndistribution even though intrinsic ability of agents are marginally different\nfrom each other. As an evidence we provide distribution of scores for two\ndifferent systems (a) the global stock game where players invest in real stock\nmarket and (b) the international cricket.\n", "category": [3, 3, 5]}
{"abstract": "  In this paper we examine the implications of the statistical large sample\ntheory for the computational complexity of Bayesian and quasi-Bayesian\nestimation carried out using Metropolis random walks. Our analysis is motivated\nby the Laplace-Bernstein-Von Mises central limit theorem, which states that in\nlarge samples the posterior or quasi-posterior approaches a normal density.\nUsing the conditions required for the central limit theorem to hold, we\nestablish polynomial bounds on the computational complexity of general\nMetropolis random walks methods in large samples. Our analysis covers cases\nwhere the underlying log-likelihood or extremum criterion function is possibly\nnon-concave, discontinuous, and with increasing parameter dimension. However,\nthe central limit theorem restricts the deviations from continuity and\nlog-concavity of the log-likelihood or extremum criterion function in a very\nspecific manner.\n  Under minimal assumptions required for the central limit theorem to hold\nunder the increasing parameter dimension, we show that the Metropolis algorithm\nis theoretically efficient even for the canonical Gaussian walk which is\nstudied in detail. Specifically, we show that the running time of the algorithm\nin large samples is bounded in probability by a polynomial in the parameter\ndimension $d$, and, in particular, is of stochastic order $d^2$ in the leading\ncases after the burn-in period. We then give applications to exponential\nfamilies, curved exponential families, and Z-estimation of increasing\ndimension.\n", "category": [2, 2, 6, 6]}
{"abstract": "  Chargaff's second parity rule holds empirically for most types of DNA that\nalong single strands of DNA the base contents are equal for complimentary\nbases, A = T, G = C. A Markov chain model is constructed to track the evolution\nof any single base position along single strands of genomes whose organisms are\nequipped with replication mismatch repair. Under the key assumptions that\nmismatch error rates primarily depend the number of hydrogen bonds of\nnucleotides and that the mismatch repairing process itself makes strand\nrecognition error, the model shows that the steady state probabilities for any\nbase position to take on one of the 4 nucleotide bases are equal for\ncomplimentary bases. As a result, Chargaff's second parity rule is the\nmanifestation of the Law of Large Number acting on the steady state\nprobabilities. More importantly, because the model pinpoints mismatch repair as\na basis of the rule, it is suitable for experimental verification.\n", "category": [4]}
{"abstract": "  A Boolean network model of the cell-cycle regulatory network of fission yeast\n(Schizosaccharomyces Pombe) is constructed solely on the basis of the known\nbiochemical interaction topology. Simulating the model in the computer,\nfaithfully reproduces the known sequence of regulatory activity patterns along\nthe cell cycle of the living cell. Contrary to existing differential equation\nmodels, no parameters enter the model except the structure of the regulatory\ncircuitry. The dynamical properties of the model indicate that the biological\ndynamical sequence is robustly implemented in the regulatory network, with the\nbiological stationary state G1 corresponding to the dominant attractor in state\nspace, and with the biological regulatory sequence being a strongly attractive\ntrajectory. Comparing the fission yeast cell-cycle model to a similar model of\nthe corresponding network in S. cerevisiae, a remarkable difference in\ncircuitry, as well as dynamics is observed. While the latter operates in a\nstrongly damped mode, driven by external excitation, the S. pombe network\nrepresents an auto-excited system with external damping.\n", "category": [4]}
{"abstract": "  In this paper we present the creation of an Arabic version of Automated\nSpeech Recognition System (ASR). This system is based on the open source\nSphinx-4, from the Carnegie Mellon University. Which is a speech recognition\nsystem based on discrete hidden Markov models (HMMs). We investigate the\nchanges that must be made to the model to adapt Arabic voice recognition.\n  Keywords: Speech recognition, Acoustic model, Arabic language, HMMs,\nCMUSphinx-4, Artificial intelligence.\n", "category": [0, 0]}
{"abstract": "  We reveal an interesting convex duality relationship between two problems:\n(a) minimizing the probability of lifetime ruin when the rate of consumption is\nstochastic and when the individual can invest in a Black-Scholes financial\nmarket; (b) a controller-and-stopper problem, in which the controller controls\nthe drift and volatility of a process in order to maximize a running reward\nbased on that process, and the stopper chooses the time to stop the running\nreward and rewards the controller a final amount at that time. Our primary goal\nis to show that the minimal probability of ruin, whose stochastic\nrepresentation does not have a classical form as does the utility maximization\nproblem (i.e., the objective's dependence on the initial values of the state\nvariables is implicit), is the unique classical solution of its\nHamilton-Jacobi-Bellman (HJB) equation, which is a non-linear boundary-value\nproblem. We establish our goal by exploiting the convex duality relationship\nbetween (a) and (b).\n", "category": [5, 2, 2, 5]}
{"abstract": "  We prove that approximating the size of stopping and trapping sets in Tanner\ngraphs of linear block codes, and more restrictively, the class of low-density\nparity-check (LDPC) codes, is NP-hard. The ramifications of our findings are\nthat methods used for estimating the height of the error-floor of moderate- and\nlong-length LDPC codes based on stopping and trapping set enumeration cannot\nprovide accurate worst-case performance predictions.\n", "category": [0, 2]}
{"abstract": "  In this work, the critical role of noisy feedback in enhancing the secrecy\ncapacity of the wiretap channel is established. Unlike previous works, where a\nnoiseless public discussion channel is used for feedback, the feed-forward and\nfeedback signals share the same noisy channel in the present model. Quite\ninterestingly, this noisy feedback model is shown to be more advantageous in\nthe current setting. More specifically, the discrete memoryless modulo-additive\nchannel with a full-duplex destination node is considered first, and it is\nshown that the judicious use of feedback increases the perfect secrecy capacity\nto the capacity of the source-destination channel in the absence of the\nwiretapper. In the achievability scheme, the feedback signal corresponds to a\nprivate key, known only to the destination. In the half-duplex scheme, a novel\nfeedback technique that always achieves a positive perfect secrecy rate (even\nwhen the source-wiretapper channel is less noisy than the source-destination\nchannel) is proposed. These results hinge on the modulo-additive property of\nthe channel, which is exploited by the destination to perform encryption over\nthe channel without revealing its key to the source. Finally, this scheme is\nextended to the continuous real valued modulo-$\\Lambda$ channel where it is\nshown that the perfect secrecy capacity with feedback is also equal to the\ncapacity in the absence of the wiretapper.\n", "category": [0, 0, 2]}
{"abstract": "  Phylogenetic mixtures model the inhomogeneous molecular evolution commonly\nobserved in data. The performance of phylogenetic reconstruction methods where\nthe underlying data is generated by a mixture model has stimulated considerable\nrecent debate. Much of the controversy stems from simulations of mixture model\ndata on a given tree topology for which reconstruction algorithms output a tree\nof a different topology; these findings were held up to show the shortcomings\nof particular tree reconstruction methods. In so doing, the underlying\nassumption was that mixture model data on one topology can be distinguished\nfrom data evolved on an unmixed tree of another topology given enough data and\nthe ``correct'' method. Here we show that this assumption can be false. For\nbiologists our results imply that, for example, the combined data from two\ngenes whose phylogenetic trees differ only in terms of branch lengths can\nperfectly fit a tree of a different topology.\n", "category": [4]}
{"abstract": "  In this paper we derive some new and practical results on testing and\ninterval estimation problems for the population eigenvalues of a Wishart matrix\nbased on the asymptotic theory for block-wise infinite dispersion of the\npopulation eigenvalues. This new type of asymptotic theory has been developed\nby the present authors in Takemura and Sheena (2005) and Sheena and Takemura\n(2007a,b) and in these papers it was applied to point estimation problem of\npopulation covariance matrix in a decision theoretic framework. In this paper\nwe apply it to some testing and interval estimation problems. We show that the\napproximation based on this type of asymptotics is generally much better than\nthe traditional large-sample asymptotics for the problems.\n", "category": [2, 6]}
{"abstract": "  The configurations of single and double bonds in polycyclic hydrocarbons are\nabstracted as Kekul\\'e states of graphs. Sending a so-called soliton over an\nopen channel between ports (external nodes) of the graph changes the Kekul\\'e\nstate and therewith the set of open channels in the graph. This switching\nbehaviour is proposed as a basis for molecular computation. The proposal is\nhighly speculative but may have tremendous impact.\n  Kekul\\'e states with the same boundary behaviour (port assignment) can be\nregarded as equivalent. This gives rise to the abstraction of Kekul\\'e cells.\nThe basic theory of Kekul\\'e states and Kekul\\'e cells is developed here, up to\nthe classification of Kekul\\'e cells with $\\leq 4$ ports. To put the theory in\ncontext, we generalize Kekul\\'e states to semi-Kekul\\'e states, which form the\nsolutions of a linear system of equations over the field of the bits 0 and 1.\nWe briefly study so-called omniconjugated graphs, in which every port\nassignment of the right signature has a Kekul\\'e state. Omniconjugated graphs\nmay be useful as connectors between computational elements. We finally\ninvestigate some examples with potentially useful switching behaviour.\n", "category": [0, 0]}
{"abstract": "  A secure human identification protocol aims at authenticating human users to\na remote server when even the users' inputs are not hidden from an adversary.\nRecently, the authors proposed a human identification protocol in the RSA\nConference 2007, which is loosely based on the ability of humans to efficiently\nprocess an image. The advantage being that an automated adversary is not\neffective in attacking the protocol without human assistance. This paper\nextends that work by trying to solve some of the open problems. First, we\nanalyze the complexity of defeating the proposed protocols by quantifying the\nworkload of a human adversary. Secondly, we propose a new construction based on\ntextual CAPTCHAs (Reverse Turing Tests) in order to make the generation of\nautomated challenges easier. We also present a brief experiment involving real\nhuman users to find out the number of possible attributes in a given image and\ngive some guidelines for the selection of challenge questions based on the\nresults. Finally, we analyze the previously proposed protocol in detail for the\nrelationship between the secrets. Our results show that we can construct human\nidentification protocols based on image evaluation with reasonably\n``quantified'' security guarantees based on our model.\n", "category": [0]}
{"abstract": "  A finite element method is presented to compute time harmonic microwave\nfields in three dimensional configurations. Nodal-based finite elements have\nbeen coupled with an absorbing boundary condition to solve open boundary\nproblems. This paper describes how the modeling of large devices has been made\npossible using parallel computation, New algorithms are then proposed to\nimplement this formulation on a cluster of workstations (10 DEC ALPHA 300X) and\non a CRAY C98. Analysis of the computation efficiency is performed using simple\nproblems. The electromagnetic scattering of a plane wave by a perfect electric\nconducting airplane is finally given as example.\n", "category": [0]}
{"abstract": "  This paper deals with the computation of the rank and of some integer Smith\nforms of a series of sparse matrices arising in algebraic K-theory. The number\nof non zero entries in the considered matrices ranges from 8 to 37 millions.\nThe largest rank computation took more than 35 days on 50 processors. We report\non the actual algorithms we used to build the matrices, their link to the\nmotivic cohomology and the linear algebra and parallelizations required to\nperform such huge computations. In particular, these results are part of the\nfirst computation of the cohomology of the linear group GL_7(Z).\n", "category": [2, 0, 0, 2]}
{"abstract": "  We consider a cognitive network consisting of n random pairs of cognitive\ntransmitters and receivers communicating simultaneously in the presence of\nmultiple primary users. Of interest is how the maximum throughput achieved by\nthe cognitive users scales with n. Furthermore, how far these users must be\nfrom a primary user to guarantee a given primary outage. Two scenarios are\nconsidered for the network scaling law: (i) when each cognitive transmitter\nuses constant power to communicate with a cognitive receiver at a bounded\ndistance away, and (ii) when each cognitive transmitter scales its power\naccording to the distance to a considered primary user, allowing the cognitive\ntransmitter-receiver distances to grow. Using single-hop transmission, suitable\nfor cognitive devices of opportunistic nature, we show that, in both scenarios,\nwith path loss larger than 2, the cognitive network throughput scales linearly\nwith the number of cognitive users. We then explore the radius of a primary\nexclusive region void of cognitive transmitters. We obtain bounds on this\nradius for a given primary outage constraint. These bounds can help in the\ndesign of a primary network with exclusive regions, outside of which cognitive\nusers may transmit freely. Our results show that opportunistic secondary\nspectrum access using single-hop transmission is promising.\n", "category": [0, 2]}
{"abstract": "  We address the problem of &#64257;nding nice labellings for event structures\nof degree 3. We develop a minimum theory by which we prove that the labelling\nnumber of an event structure of degree 3 is bounded by a linear function of the\nheight. The main theorem we present in this paper states that event structures\nof degree 3 whose causality order is a tree have a nice labelling with 3\ncolors. Finally, we exemplify how to use this theorem to construct upper bounds\nfor the labelling number of other event structures of degree 3.\n", "category": [0]}
{"abstract": "  Power control is a fundamental task accomplished in any wireless cellular\nnetwork; its aim is to set the transmit power of any mobile terminal, so that\neach user is able to achieve its own target SINR. While conventional power\ncontrol algorithms require knowledge of a number of parameters of the signal of\ninterest and of the multiaccess interference, in this paper it is shown that in\na large CDMA system much of this information can be dispensed with, and\neffective distributed power control algorithms may be implemented with very\nlittle information on the user of interest. An uplink CDMA system subject to\nflat fading is considered with a focus on the cases in which a linear MMSE\nreceiver and a non-linear MMSE serial interference cancellation receiver are\nadopted; for the latter case new formulas are also given for the system SINR in\nthe large system asymptote. Experimental results show an excellent agreement\nbetween the performance and the power profile of the proposed distributed\nalgorithms and that of conventional ones that require much greater prior\nknowledge.\n", "category": [0, 2]}
{"abstract": "  This paper is focused on the cross-layer design problem of joint multiuser\ndetection and power control for energy-efficiency optimization in a wireless\ndata network through a game-theoretic approach. Building on work of Meshkati,\net al., wherein the tools of game-theory are used in order to achieve\nenergy-efficiency in a simple synchronous code division multiple access system,\nsystem asynchronism, the use of bandlimited chip-pulses, and the multipath\ndistortion induced by the wireless channel are explicitly incorporated into the\nanalysis. Several non-cooperative games are proposed wherein users may vary\ntheir transmit power and their uplink receiver in order to maximize their\nutility, which is defined here as the ratio of data throughput to transmit\npower. In particular, the case in which a linear multiuser detector is adopted\nat the receiver is considered first, and then, the more challenging case in\nwhich non-linear decision feedback multiuser detectors are employed is\nconsidered. The proposed games are shown to admit a unique Nash equilibrium\npoint, while simulation results show the effectiveness of the proposed\nsolutions, as well as that the use of a decision-feedback multiuser receiver\nbrings remarkable performance improvements.\n", "category": [0, 2]}
{"abstract": "  In this paper we introduce a variant of pushdown dimension called bounded\npushdown (BPD) dimension, that measures the density of information contained in\na sequence, relative to a BPD automata, i.e. a finite state machine equipped\nwith an extra infinite memory stack, with the additional requirement that every\ninput symbol only allows a bounded number of stack movements. BPD automata are\na natural real-time restriction of pushdown automata. We show that BPD\ndimension is a robust notion by giving an equivalent characterization of BPD\ndimension in terms of BPD compressors. We then study the relationships between\nBPD compression, and the standard Lempel-Ziv (LZ) compression algorithm, and\nshow that in contrast to the finite-state compressor case, LZ is not universal\nfor bounded pushdown compressors in a strong sense: we construct a sequence\nthat LZ fails to compress signicantly, but that is compressed by at least a\nfactor 2 by a BPD compressor. As a corollary we obtain a strong separation\nbetween finite-state and BPD dimension.\n", "category": [0, 0, 2]}
{"abstract": "  Protein-DNA complexes with loops play a fundamental role in a wide variety of\ncellular processes, ranging from the regulation of DNA transcription to\ntelomere maintenance. As ubiquitous as they are, their precise in vivo\nproperties and their integration into the cellular function still remain\nlargely unexplored. Here, we present a multilevel approach that efficiently\nconnects in both directions molecular properties with cell physiology and use\nit to characterize the molecular properties of the looped DNA-lac repressor\ncomplex while functioning in vivo. The properties we uncover include the\npresence of two representative conformations of the complex, the stabilization\nof one conformation by DNA architectural proteins, and precise values of the\nunderlying twisting elastic constants and bending free energies. Incorporation\nof all this molecular information into gene-regulation models reveals an\nunprecedented versatility of looped DNA-protein complexes at shaping the\nproperties of gene expression.\n", "category": [4, 4]}
{"abstract": "  Typing of lambda-terms in Elementary and Light Affine Logic (EAL, LAL, resp.)\nhas been studied for two different reasons: on the one hand the evaluation of\ntyped terms using LAL (EAL, resp.) proof-nets admits a guaranteed polynomial\n(elementary, resp.) bound; on the other hand these terms can also be evaluated\nby optimal reduction using the abstract version of Lamping's algorithm. The\nfirst reduction is global while the second one is local and asynchronous. We\nprove that for LAL (EAL, resp.) typed terms, Lamping's abstract algorithm also\nadmits a polynomial (elementary, resp.) bound. We also show its soundness and\ncompleteness (for EAL and LAL with type fixpoints), by using a simple geometry\nof interaction model (context semantics).\n", "category": [0, 0]}
{"abstract": "  On a fading channel with no channel state information at the receiver,\ncalculating true log-likelihood ratios (LLR) is complicated. Existing work\nassume that the power of the additive noise is known and use the expected value\nof the fading gain in a linear function of the channel output to find\napproximate LLRs. In this work, we first assume that the power of the additive\nnoise is known and we find the optimum linear approximation of LLRs in the\nsense of maximum achievable transmission rate on the channel. The maximum\nachievable rate under this linear LLR calculation is almost equal to the\nmaximum achievable rate under true LLR calculation. We also observe that this\nmethod appears to be the optimum in the sense of bit error rate performance\ntoo. These results are then extended to the case that the noise power is\nunknown at the receiver and a performance almost identical to the case that the\nnoise power is perfectly known is obtained.\n", "category": [0, 2]}
{"abstract": "  Surviving in a diverse environment requires corresponding organism responses.\nAt the cellular level, such adjustment relies on the transcription factors\n(TFs) which must rapidly find their target sequences amidst a vast amount of\nnon-relevant sequences on DNA molecules. Whether these transcription factors\nlocate their target sites through a 1D or 3D pathway is still a matter of\nspeculation. It has been suggested that the optimum search time is when the\nprotein equally shares its search time between 1D and 3D diffusions. In this\npaper, we study the above problem using a Monte Carlo simulation by considering\na very simple physical model. A 1D strip, representing a DNA, with a number of\nlow affinity sites, corresponding to non-target sites, and high affinity sites,\ncorresponding to target sites, is considered and later extended to a 2D strip.\nWe study the 1D and 3D exploration pathways, and combinations of the two modes\nby considering three different types of molecules: a walker that randomly walks\nalong the strip with no dissociation; a jumper that represents dissociation and\nthen re-association of a TF with the strip at later time at a distant site; and\na hopper that is similar to the jumper but it dissociates and then\nre-associates at a faster rate than the jumper. We analyze the final\nprobability distribution of molecules for each case and find that TFs can\nlocate their targets fast enough even if they spend 15% of their search time\ndiffusing freely in the solution. This indeed agrees with recent experimental\nresults obtained by Elf et al. 2007 and is in contrast with theoretical\nexpectation.\n", "category": [4, 4]}
{"abstract": "  We report 10 successfully folding events of trpzip2 by molecular dynamics\nsimulation. It is found that the trizip2 can fold into its native state through\ndifferent zipper pathways, depending on the ways of forming hydrophobic core.\nWe also find a very fast non-zipper pathway. This indicates that there may be\nno inconsistencies in the current pictures of beta-hairpin folding mechanisms.\nThese pathways occur with different probabilities. zip-out is the most probable\none. This may explain the recent experiment that the turn formation is the\nrate-limiting step for beta-hairpin folding.\n", "category": [4]}
{"abstract": "  A main distinguishing feature of a wireless network compared with a wired\nnetwork is its broadcast nature, in which the signal transmitted by a node may\nreach several other nodes, and a node may receive signals from several other\nnodes simultaneously. Rather than a blessing, this feature is treated more as\nan interference-inducing nuisance in most wireless networks today (e.g., IEEE\n802.11). This paper shows that the concept of network coding can be applied at\nthe physical layer to turn the broadcast property into a capacity-boosting\nadvantage in wireless ad hoc networks. Specifically, we propose a\nphysical-layer network coding (PNC) scheme to coordinate transmissions among\nnodes. In contrast to straightforward network coding which performs coding\narithmetic on digital bit streams after they have been received, PNC makes use\nof the additive nature of simultaneously arriving electromagnetic (EM) waves\nfor equivalent coding operation. And in doing so, PNC can potentially achieve\n100% and 50% throughput increases compared with traditional transmission and\nstraightforward network coding, respectively, in multi-hop networks. More\nspecifically, the information-theoretic capacity of PNC is almost double that\nof traditional transmission in the SNR region of practical interest (higher\nthan 0dB). We believe this is a first paper that ventures into EM-wave-based\nnetwork coding at the physical layer and demonstrates its potential for\nboosting network capacity.\n", "category": [0, 2]}
{"abstract": "  In this paper we study the problem of adaptive estimation of a multivariate\nfunction satisfying some structural assumption. We propose a novel estimation\nprocedure that adapts simultaneously to unknown structure and smoothness of the\nunderlying function. The problem of structural adaptation is stated as the\nproblem of selection from a given collection of estimators. We develop a\ngeneral selection rule and establish for it global oracle inequalities under\narbitrary $\\rL_p$--losses. These results are applied for adaptive estimation in\nthe additive multi--index model.\n", "category": [2, 2, 6]}
{"abstract": "  In this paper we study the aggregation problem that can be formulated as\nfollows. Assume that we have a family of estimators $\\mathcal{F}$ built on the\nbasis of available observations. The goal is to construct a new estimator whose\nrisk is as close as possible to that of the best estimator in the family. We\npropose a general aggregation scheme that is universal in the following sense:\nit applies for families of arbitrary estimators and a wide variety of models\nand global risk measures. The procedure is based on comparison of empirical\nestimates of certain linear functionals with estimates induced by the family\n$\\mathcal{F}$. We derive oracle inequalities and show that they are\nunimprovable in some sense. Numerical results demonstrate good practical\nbehavior of the procedure.\n", "category": [2, 6]}
{"abstract": "  \"Extended Clifford algebras\" are introduced as a means to obtain low ML\ndecoding complexity space-time block codes. Using left regular matrix\nrepresentations of two specific classes of extended Clifford algebras, two\nsystematic algebraic constructions of full diversity Distributed Space-Time\nCodes (DSTCs) are provided for any power of two number of relays. The left\nregular matrix representation has been shown to naturally result in space-time\ncodes meeting the additional constraints required for DSTCs. The DSTCs so\nconstructed have the salient feature of reduced Maximum Likelihood (ML)\ndecoding complexity. In particular, the ML decoding of these codes can be\nperformed by applying the lattice decoder algorithm on a lattice of four times\nlesser dimension than what is required in general. Moreover these codes have a\nuniform distribution of power among the relays and in time, thus leading to a\nlow Peak to Average Power Ratio at the relays.\n", "category": [0, 0, 2]}
{"abstract": "  A set of sufficient conditions to construct $\\lambda$-real symbol Maximum\nLikelihood (ML) decodable STBCs have recently been provided by Karmakar et al.\nSTBCs satisfying these sufficient conditions were named as Clifford Unitary\nWeight (CUW) codes. In this paper, the maximal rate (as measured in complex\nsymbols per channel use) of CUW codes for $\\lambda=2^a,a\\in\\mathbb{N}$ is\nobtained using tools from representation theory. Two algebraic constructions of\ncodes achieving this maximal rate are also provided. One of the constructions\nis obtained using linear representation of finite groups whereas the other\nconstruction is based on the concept of right module algebra over\nnon-commutative rings. To the knowledge of the authors, this is the first paper\nin which matrices over non-commutative rings is used to construct STBCs. An\nalgebraic explanation is provided for the 'ABBA' construction first proposed by\nTirkkonen et al and the tensor product construction proposed by Karmakar et al.\nFurthermore, it is established that the 4 transmit antenna STBC originally\nproposed by Tirkkonen et al based on the ABBA construction is actually a single\ncomplex symbol ML decodable code if the design variables are permuted and\nsignal sets of appropriate dimensions are chosen.\n", "category": [0, 0, 2]}
{"abstract": "  The problem of designing high rate, full diversity noncoherent space-time\nblock codes (STBCs) with low encoding and decoding complexity is addressed.\nFirst, the notion of $g$-group encodable and $g$-group decodable linear STBCs\nis introduced. Then for a known class of rate-1 linear designs, an explicit\nconstruction of fully-diverse signal sets that lead to four-group encodable and\nfour-group decodable differential scaled unitary STBCs for any power of two\nnumber of antennas is provided. Previous works on differential STBCs either\nsacrifice decoding complexity for higher rate or sacrifice rate for lower\ndecoding complexity.\n", "category": [0, 2]}
{"abstract": "  The differential encoding/decoding setup introduced by Kiran et al, Oggier et\nal and Jing et al for wireless relay networks that use codebooks consisting of\nunitary matrices is extended to allow codebooks consisting of scaled unitary\nmatrices. For such codebooks to be used in the Jing-Hassibi protocol for\ncooperative diversity, the conditions that need to be satisfied by the relay\nmatrices and the codebook are identified. A class of previously known rate one,\nfull diversity, four-group encodable and four-group decodable Differential\nSpace-Time Codes (DSTCs) is proposed for use as Distributed DSTCs (DDSTCs) in\nthe proposed set up. To the best of our knowledge, this is the first known low\ndecoding complexity DDSTC scheme for cooperative wireless networks.\n", "category": [0, 2]}
{"abstract": "  Is it possible to link a set of nodes without using preexisting positional\ninformation or any kind of long-range attraction of the nodes? Can the process\nof generating positional information, i.e. the detection of ``unknown'' nodes\nand the estabishment of chemical gradients, \\emph{and} the process of network\nformation, i.e. the establishment of links between nodes, occur in parallel, on\na comparable time scale, as a process of co-evolution?\n  The paper discusses a model where the generation of relevant information for\nestablishing the links between nodes results from the interaction of many\n\\emph{agents}, i.e. subunits of the system that are capable of performing some\nactivities. Their collective interaction is based on (indirect) communication,\nwhich also includes memory effects and the dissemination of information in the\nsystem. The relevant (``pragmatic'') information that leads to the\nestablishment of the links then emerges from an evolutionary interplay of\nselection and reamplification.\n", "category": [3, 3, 4]}
{"abstract": "  The main goal of this project is to research technical advances in order to\nenhance the possibility to develop narratives within immersive mediated\nenvironments. An important part of the research is concerned with the question\nof how a script can be written, annotated and realized for an immersive\ncontext. A first description of the main theoretical framework and the ongoing\nwork and a first script example is provided. This project is part of the\nprogram for presence research, and it will exploit physiological feedback and\nComputational Intelligence within virtual reality.\n", "category": [0]}
{"abstract": "  The Extended BP (EBP) Generalized EXIT (GEXIT) function introduced in\n\\cite{MMRU05} plays a fundamental role in the asymptotic analysis of sparse\ngraph codes. For transmission over the binary erasure channel (BEC) the\nanalytic properties of the EBP GEXIT function are relatively simple and well\nunderstood. The general case is much harder and even the existence of the curve\nis not known in general. We introduce some tools from non-linear analysis which\ncan be useful to prove the existence of EXIT like curves in some cases. The\nmain tool is the Krasnoselskii-Rabinowitz (KR) bifurcation theorem.\n", "category": [0, 2]}
{"abstract": "  The complementary strands of DNA molecules can be separated when stretched\napart by a force; the unzipping signal is correlated to the base content of the\nsequence but is affected by thermal and instrumental noise. We consider here\nthe ideal case where opening events are known to a very good time resolution\n(very large bandwidth), and study how the sequence can be reconstructed from\nthe unzipping data. Our approach relies on the use of statistical Bayesian\ninference and of Viterbi decoding algorithm. Performances are studied\nnumerically on Monte Carlo generated data, and analytically. We show how\nmultiple unzippings of the same molecule may be exploited to improve the\nquality of the prediction, and calculate analytically the number of required\nunzippings as a function of the bandwidth, the sequence content, the elasticity\nparameters of the unzipped strands.\n", "category": [4, 3]}
{"abstract": "  In this paper, we propose a novel inference method for dynamic genetic\nnetworks which makes it possible to face with a number of time measurements n\nmuch smaller than the number of genes p. The approach is based on the concept\nof low order conditional dependence graph that we extend here in the case of\nDynamic Bayesian Networks. Most of our results are based on the theory of\ngraphical models associated with the Directed Acyclic Graphs (DAGs). In this\nway, we define a minimal DAG G which describes exactly the full order\nconditional dependencies given the past of the process. Then, to face with the\nlarge p and small n estimation case, we propose to approximate DAG G by\nconsidering low order conditional independencies. We introduce partial qth\norder conditional dependence DAGs G(q) and analyze their probabilistic\nproperties. In general, DAGs G(q) differ from DAG G but still reflect relevant\ndependence facts for sparse networks such as genetic networks. By using this\napproximation, we set out a non-bayesian inference method and demonstrate the\neffectiveness of this approach on both simulated and real data analysis. The\ninference procedure is implemented in the R package 'G1DBN' freely available\nfrom the CRAN archive.\n", "category": [2, 4, 6]}
{"abstract": "  The puzzle presented by the famous stumps of Gilboa, New York, finds a\nsolution in the discovery of two fossil specimens that allow the entire\nstructure of these early trees to be reconstructed.\n", "category": [4]}
{"abstract": "  This paper deals with the problem of increasing the minimum distance of a\nlinear code by adding one or more columns to the generator matrix. Several\nmethods to compute extensions of linear codes are presented. Many codes\nimproving the previously known lower bounds on the minimum distance have been\nfound.\n", "category": [0, 0, 2]}
{"abstract": "  A discrete (finite-difference) analogue of differential forms is considered,\ndefined on simplicial complexes, including triangulations of continuous\nmanifolds. Various operations are explicitly defined on these forms, including\nexterior derivative and exterior product. The latter one is non-associative.\nInstead, as anticipated, it is a part of non-trivial A-infinity structure,\ninvolving a chain of poly-linear operations, constrained by nilpotency\nrelation: (d + \\wedge + m + ...)^n = 0 with n=2.\n", "category": [2, 0, 3]}
{"abstract": "  The problem of joint universal source coding and modeling, treated in the\ncontext of lossless codes by Rissanen, was recently generalized to fixed-rate\nlossy coding of finitely parametrized continuous-alphabet i.i.d. sources. We\nextend these results to variable-rate lossy block coding of stationary ergodic\nsources and show that, for bounded metric distortion measures, any finitely\nparametrized family of stationary sources satisfying suitable mixing,\nsmoothness and Vapnik-Chervonenkis learnability conditions admits universal\nschemes for joint lossy source coding and identification. We also give several\nexplicit examples of parametric sources satisfying the regularity conditions.\n", "category": [0, 0, 2]}
{"abstract": "  If predictions for species extinctions hold, then the `tree of life' today\nmay be quite different to that in (say) 100 years. We describe a technique to\nquantify how much each species is likely to contribute to future biodiversity,\nas measured by its expected contribution to phylogenetic diversity. Our\napproach considers all possible scenarios for the set of species that will be\nextant at some future time, and weights them according to their likelihood\nunder an independent (but not identical) distribution on species extinctions.\nAlthough the number of extinction scenarios can typically be very large, we\nshow that there is a simple algorithm that will quickly compute this index. The\nmethod is implemented and applied to the prosimian primates as a test case, and\nthe associated species ranking is compared to a related measure (the `Shapley\nindex'). We describe indices for rooted and unrooted trees, and a modification\nthat also includes the focal taxon's probability of extinction, making it\ndirectly comparable to some new conservation metrics.\n", "category": [4]}
{"abstract": "  The problem of resource allocation is studied for a two-user fading\northogonal multiaccess relay channel (MARC) where both users (sources)\ncommunicate with a destination in the presence of a relay. A half-duplex relay\nis considered that transmits on a channel orthogonal to that used by the\nsources. The instantaneous fading state between every transmit-receive pair in\nthis network is assumed to be known at both the transmitter and receiver. Under\nan average power constraint at each source and the relay, the sum-rate for the\nachievable strategy of decode-and-forward (DF) is maximized over all power\nallocations (policies) at the sources and relay. It is shown that the sum-rate\nmaximizing policy exploits the multiuser fading diversity to reveal the\noptimality of opportunistic channel use by each user. A geometric\ninterpretation of the optimal power policy is also presented.\n", "category": [0, 2]}
{"abstract": "  A transmitter without channel state information (CSI) wishes to send a\ndelay-limited Gaussian source over a slowly fading channel. The source is coded\nin superimposed layers, with each layer successively refining the description\nin the previous one. The receiver decodes the layers that are supported by the\nchannel realization and reconstructs the source up to a distortion. In the\nlimit of a continuum of infinite layers, the optimal power distribution that\nminimizes the expected distortion is given by the solution to a set of linear\ndifferential equations in terms of the density of the fading distribution. In\nthe optimal power distribution, as SNR increases, the allocation over the\nhigher layers remains unchanged; rather the extra power is allocated towards\nthe lower layers. On the other hand, as the bandwidth ratio b (channel uses per\nsource symbol) tends to zero, the power distribution that minimizes expected\ndistortion converges to the power distribution that maximizes expected\ncapacity. While expected distortion can be improved by acquiring CSI at the\ntransmitter (CSIT) or by increasing diversity from the realization of\nindependent fading paths, at high SNR the performance benefit from diversity\nexceeds that from CSIT, especially when b is large.\n", "category": [0, 2]}
{"abstract": "  We introduce a framework for filtering features that employs the\nHilbert-Schmidt Independence Criterion (HSIC) as a measure of dependence\nbetween the features and the labels. The key idea is that good features should\nmaximise such dependence. Feature selection for various supervised learning\nproblems (including classification and regression) is unified under this\nframework, and the solutions can be approximated using a backward-elimination\nalgorithm. We demonstrate the usefulness of our method on both artificial and\nreal world datasets.\n", "category": [0]}
{"abstract": "  Motivated by on-chip communication, a channel model is proposed where the\nvariance of the additive noise depends on the weighted sum of the past channel\ninput powers. For this channel, an expression for the capacity per unit cost is\nderived, and it is shown that the expression holds also in the presence of\nfeedback.\n", "category": [0, 2]}
{"abstract": "  The random initialization of weights of a multilayer perceptron makes it\npossible to model its training process as a Las Vegas algorithm, i.e. a\nrandomized algorithm which stops when some required training error is obtained,\nand whose execution time is a random variable. This modeling is used to perform\na case study on a well-known pattern recognition benchmark: the UCI Thyroid\nDisease Database. Empirical evidence is presented of the training time\nprobability distribution exhibiting a heavy tail behavior, meaning a big\nprobability mass of long executions. This fact is exploited to reduce the\ntraining time cost by applying two simple restart strategies. The first assumes\nfull knowledge of the distribution yielding a 40% cut down in expected time\nwith respect to the training without restarts. The second, assumes null\nknowledge, yielding a reduction ranging from 9% to 23%.\n", "category": [0]}
{"abstract": "  A wireless network in which packets are broadcast to a group of receivers\nthrough use of a random access protocol is considered in this work. The\nrelation to previous work on networks of interacting queues is discussed and\nsubsequently, the stability and throughput regions of the system are analyzed\nand presented. A simple network of two source nodes and two destination nodes\nis considered first. The broadcast service process is analyzed assuming a\nchannel that allows for packet capture and multipacket reception. In this small\nnetwork, the stability and throughput regions are observed to coincide. The\nsame problem for a network with N sources and M destinations is considered\nnext. The channel model is simplified in that multipacket reception is no\nlonger permitted. Bounds on the stability region are developed using the\nconcept of stability rank and the throughput region of the system is compared\nto the bounds. Our results show that as the number of destination nodes\nincreases, the stability and throughput regions diminish. Additionally, a\nprevious conjecture that the stability and throughput regions coincide for a\nnetwork of arbitrarily many sources is supported for a broadcast scenario by\nthe results presented in this work.\n", "category": [0, 2]}
{"abstract": "  In this paper we survey the computational time complexity of assorted simple\nstochastic game problems, and we give an overview of the best known algorithms\nassociated with each problem.\n", "category": [0, 0]}
{"abstract": "  This paper studies a variant of the classical problem of ``writing on dirty\npaper'' in which the sum of the input and the interference, or dirt, is\nmultiplied by a random variable that models resizing, known to the decoder but\nnot to the encoder. The achievable rate of Costa's dirty paper coding (DPC)\nscheme is calculated and compared to the case of the decoder's also knowing the\ndirt. In the ergodic case, the corresponding rate loss vanishes asymptotically\nin the limits of both high and low signal-to-noise ratio (SNR), and is small at\nall finite SNR for typical distributions like Rayleigh, Rician, and Nakagami.\nIn the quasi-static case, the DPC scheme is lossless at all SNR in terms of\noutage probability. Quasi-static fading broadcast channels (BC) without\ntransmit channel state information (CSI) are investigated as an application of\nthe robustness properties. It is shown that the DPC scheme leads to an outage\nachievable rate region that strictly dominates that of time division.\n", "category": [0, 2]}
{"abstract": "  For cellular biochemical reaction systems where the numbers of molecules is\nsmall, significant noise is associated with chemical reaction events. This\nmolecular noise can give rise to behavior that is very different from the\npredictions of deterministic rate equation models. Unfortunately, there are few\nanalytic methods for examining the qualitative behavior of stochastic systems.\nHere we describe such a method that extends deterministic analysis to include\nleading-order corrections due to the molecular noise. The method allows the\nsteady-state behavior of the stochastic model to be easily computed,\nfacilitates the mapping of stability phase diagrams that include stochastic\neffects and reveals how model parameters affect noise susceptibility, in a\nmanner not accessible to numerical simulation. By way of illustration we\nconsider two genetic circuits: a bistable positive-feedback loop and a\nnegative-feedback oscillator. We find in the positive feedback circuit that\ntranslational activation leads to a far more stable system than transcriptional\ncontrol. Conversely, in a negative-feedback loop triggered by a\npositive-feedback switch, the stochasticity of transcriptional control is\nharnessed to generate reproducible oscillations.\n", "category": [4, 4]}
{"abstract": "  This work considers the problem of transmitting multiple compressible sources\nover a network at minimum cost. The aim is to find the optimal rates at which\nthe sources should be compressed and the network flows using which they should\nbe transmitted so that the cost of the transmission is minimal. We consider\nnetworks with capacity constraints and linear cost functions. The problem is\ncomplicated by the fact that the description of the feasible rate region of\ndistributed source coding problems typically has a number of constraints that\nis exponential in the number of sources. This renders general purpose solvers\ninefficient. We present a framework in which these problems can be solved\nefficiently by exploiting the structure of the feasible rate regions coupled\nwith dual decomposition and optimization techniques such as the subgradient\nmethod and the proximal bundle method.\n", "category": [0, 0, 2]}
{"abstract": "  We consider a list decoding algorithm recently proposed by Pellikaan-Wu\n\\cite{PW2005} for $q$-ary Reed-Muller codes $\\mathcal{RM}_q(\\ell, m, n)$ of\nlength $n \\leq q^m$ when $\\ell \\leq q$. A simple and easily accessible\ncorrectness proof is given which shows that this algorithm achieves a relative\nerror-correction radius of $\\tau \\leq (1 - \\sqrt{{\\ell q^{m-1}}/{n}})$. This is\nan improvement over the proof using one-point Algebraic-Geometric codes given\nin \\cite{PW2005}. The described algorithm can be adapted to decode\nProduct-Reed-Solomon codes.\n  We then propose a new low complexity recursive algebraic decoding algorithm\nfor Reed-Muller and Product-Reed-Solomon codes. Our algorithm achieves a\nrelative error correction radius of $\\tau \\leq \\prod_{i=1}^m (1 -\n\\sqrt{k_i/q})$. This technique is then proved to outperform the Pellikaan-Wu\nmethod in both complexity and error correction radius over a wide range of code\nrates.\n", "category": [0, 0, 2]}
{"abstract": "  We calculate crossing probabilities and one-sided last exit time densities\nfor a class of moving barriers on an interval $[0,T]$ via Schwartz\ndistributions. We derive crossing probabilities and first hitting time\ndensities for another class of barriers on $[0,T]$ by proving a Schwartz\ndistribution version of the method of images. Analytic expressions for crossing\nprobabilities and related densities are given for new explicit and\nsemi-explicit barriers.\n", "category": [2, 2, 2, 6]}
{"abstract": "  In wireless ad hoc networks, distributed nodes can collaboratively form an\nantenna array for long-distance communications to achieve high energy\nefficiency. In recent work, Ochiai, et al., have shown that such collaborative\nbeamforming can achieve a statistically nice beampattern with a narrow main\nlobe and low sidelobes. However, the process of collaboration introduces\nsignificant delay, since all collaborating nodes need access to the same\ninformation. In this paper, a technique that significantly reduces the\ncollaboration overhead is proposed. It consists of two phases. In the first\nphase, nodes transmit locally in a random access fashion. Collisions, when they\noccur, are viewed as linear mixtures of the collided packets. In the second\nphase, a set of cooperating nodes acts as a distributed antenna system and\nbeamform the received analog waveform to one or more faraway destinations. This\nstep requires multiplication of the received analog waveform by a complex\nnumber, which is independently computed by each cooperating node, and which\nenables separation of the collided packets based on their final destination.\nThe scheme requires that each node has global knowledge of the network\ncoordinates. The proposed scheme can achieve high throughput, which in certain\ncases exceeds one.\n", "category": [0, 2]}
{"abstract": "  Future nano-scale electronics built up from an Avogadro number of components\nneeds efficient, highly scalable, and robust means of communication in order to\nbe competitive with traditional silicon approaches. In recent years, the\nNetworks-on-Chip (NoC) paradigm emerged as a promising solution to interconnect\nchallenges in silicon-based electronics. Current NoC architectures are either\nhighly regular or fully customized, both of which represent implausible\nassumptions for emerging bottom-up self-assembled molecular electronics that\nare generally assumed to have a high degree of irregularity and imperfection.\nHere, we pragmatically and experimentally investigate important design\ntrade-offs and properties of an irregular, abstract, yet physically plausible\n3D small-world interconnect fabric that is inspired by modern network-on-chip\nparadigms. We vary the framework's key parameters, such as the connectivity,\nthe number of switch nodes, the distribution of long- versus short-range\nconnections, and measure the network's relevant communication characteristics.\nWe further explore the robustness against link failures and the ability and\nefficiency to solve a simple toy problem, the synchronization task. The results\nconfirm that (1) computation in irregular assemblies is a promising and\ndisruptive computing paradigm for self-assembled nano-scale electronics and (2)\nthat 3D small-world interconnect fabrics with a power-law decaying distribution\nof shortcut lengths are physically plausible and have major advantages over\nlocal 2D and 3D regular topologies.\n", "category": [0, 3, 3]}
{"abstract": "  These are the notes for a set of lectures delivered by the two authors at the\nLes Houches Summer School on `Complex Systems' in July 2006. They provide an\nintroduction to the basic concepts in modern (probabilistic) coding theory,\nhighlighting connections with statistical mechanics. We also stress common\nconcepts with other disciplines dealing with similar problems that can be\ngenerically referred to as `large graphical models'.\n  While most of the lectures are devoted to the classical channel coding\nproblem over simple memoryless channels, we present a discussion of more\ncomplex channel models. We conclude with an overview of the main open\nchallenges in the field.\n", "category": [0, 3, 2]}
{"abstract": "  We analyze complexity of financial (and general economic) processes by\ncomparing classical and quantum-like models for randomness. Our analysis\nimplies that it might be that a quantum-like probabilistic description is more\nnatural for financial market than the classical one. A part of our analysis is\ndevoted to study the possibility of application of the quantum probabilistic\nmodel to agents of financial market. We show that, although the direct quantum\n(physical) reduction (based on using the scales of quantum mechanics) is\nmeaningless, one may apply so called quantum-like models. In our approach\nquantum-like probabilistic behaviour is a consequence of contextualy of\nstatistical data in finances (and economics in general). However, our\nhypothesis on \"quantumness\" of financial data should be tested experimentally\n(as opposed to the conventional description based on the noncontextual\nclassical probabilistic approach). We present a new statistical test based on a\ngeneralization of the well known in quantum physics Bell's inequality.\n", "category": [5, 2, 2]}
{"abstract": "  Medvedev and Melott (2007) have suggested that periodicity in fossil\nbiodiversity may be induced by cosmic rays which vary as the Solar System\noscillates normal to the galactic disk. We re-examine the evidence for a 62\nmillion year (Myr) periodicity in biodiversity throughout the Phanerozoic\nhistory of animal life reported by Rohde & Mueller (2005), as well as related\nquestions of periodicity in origination and extinction. We find that the signal\nis robust against variations in methods of analysis, and is based on\nfluctuations in the Paleozoic and a substantial part of the Mesozoic.\nExamination of origination and extinction is somewhat ambiguous, with results\ndepending upon procedure. Origination and extinction intensity as defined by RM\nmay be affected by an artifact at 27 Myr in the duration of stratigraphic\nintervals. Nevertheless, when a procedure free of this artifact is implemented,\nthe 27 Myr periodicity appears in origination, suggesting that the artifact may\nultimately be based on a signal in the data. A 62 Myr feature appears in\nextinction, when this same procedure is used. We conclude that evidence for a\nperiodicity at 62 Myr is robust, and evidence for periodicity at approximately\n27 Myr is also present, albeit more ambiguous.\n", "category": [4, 3]}
{"abstract": "  We extend our approach to abstract syntax (with binding constructions)\nthrough modules and linearity. First we give a new general definition of arity,\nyielding the companion notion of signature. Then we obtain a modularity result\nas requested by Ghani and Uustalu (2003): in our setting, merging two\nextensions of syntax corresponds to building an amalgamated sum. Finally we\ndefine a natural notion of equation concerning a signature and prove the\nexistence of an initial semantics for a so-called representable signature\nequipped with a set of equations.\n", "category": [0]}
{"abstract": "  An important goal for digital libraries is to enable researchers to more\neasily explore related work. While citation data is often used as an indicator\nof relatedness, in this paper we demonstrate that digital access records (e.g.\nhttp-server logs) can be used as indicators as well. In particular, we show\nthat measures based on co-access provide better coverage than co-citation, that\nthey are available much sooner, and that they are more accurate for recent\npapers.\n", "category": [0, 0]}
{"abstract": "  Current techniques for generating a knowledge space, such as QUERY,\nguarantees that the resulting structure is closed under union, but not that it\nsatisfies wellgradedness, which is one of the defining conditions for a\nlearning space. We give necessary and sufficient conditions on the base of a\nunion-closed set family that ensures that the family is well-graded. We\nconsider two cases, depending on whether or not the family contains the empty\nset. We also provide algorithms for efficiently testing these conditions, and\nfor augmenting a set family in a minimal way to one that satisfies these\nconditions.\n", "category": [2, 0, 0]}
{"abstract": "  In this paper, we study a routing problem on the Gaussian multiple relay\nchannel, in which nodes employ a decode-and-forward coding strategy. We are\ninterested in routes for the information flow through the relays that achieve\nthe highest DF rate. We first construct an algorithm that provably finds\noptimal DF routes. As the algorithm runs in factorial time in the worst case,\nwe propose a polynomial time heuristic algorithm that finds an optimal route\nwith high probability. We demonstrate that that the optimal (and near optimal)\nDF routes are good in practice by simulating a distributed DF coding scheme\nusing low density parity check codes with puncturing and incremental\nredundancy.\n", "category": [0, 2]}
{"abstract": "  This thesis investigates in the use of access log data as a source of\ninformation for identifying related scientific papers. This is done for\narXiv.org, the authority for publication of e-prints in several fields of\nphysics.\n  Compared to citation information, access logs have the advantage of being\nimmediately available, without manual or automatic extraction of the citation\ngraph. Because of that, a main focus is on the question, how far user behavior\ncan serve as a replacement for explicit meta-data, which potentially might be\nexpensive or completely unavailable. Therefore, we compare access, content, and\ncitation-based measures of relatedness on different recommendation tasks. As a\nfinal result, an online recommendation system has been built that can help\nscientists to find further relevant literature, without having to search for\nthem actively.\n", "category": [0, 0]}
{"abstract": "  Secondary structure elements of many protein families exhibit differential\nconservation on their opposing faces. Amphipathic helices and beta-sheets by\ndefinition possess this property, and play crucial functional roles. This type\nof evolutionary trajectory of a protein family is usually critical to the\nfunctions of the protein family, as well as in creating functions within\nsubfamilies. That is, differential conservation maintains properties of a\nprotein structure related to its orientation, and that are important in\npacking, recognition, and catalysis. Here I define and formulate a new concept,\ncalled the selection moment, that detects this evolutionary process in protein\nsequences. A treatment of its various applications is detailed.\n", "category": [4, 4]}
{"abstract": "  A fundamental problem in neuroscience is understanding how working memory --\nthe ability to store information at intermediate timescales, like 10s of\nseconds -- is implemented in realistic neuronal networks. The most likely\ncandidate mechanism is the attractor network, and a great deal of effort has\ngone toward investigating it theoretically. Yet, despite almost a quarter\ncentury of intense work, attractor networks are not fully understood. In\nparticular, there are still two unanswered questions. First, how is it that\nattractor networks exhibit irregular firing, as is observed experimentally\nduring working memory tasks? And second, how many memories can be stored under\nbiologically realistic conditions? Here we answer both questions by studying an\nattractor neural network in which inhibition and excitation balance each other.\nUsing mean field analysis, we derive a three-variable description of attractor\nnetworks. From this description it follows that irregular firing can exist only\nif the number of neurons involved in a memory is large. The same mean field\nanalysis also shows that the number of memories that can be stored in a network\nscales with the number of excitatory connections, a result that has been\nsuggested for simple models but never shown for realistic ones. Both of these\npredictions are verified using simulations with large networks of spiking\nneurons.\n", "category": [4, 3]}
{"abstract": "  In this paper non-group permutation modulated sequences for the Gaussian\nchannel are considered. Without the restriction to group codes rather than\nsubsets of group codes, arbitrary rates are achievable. The code construction\nutilizes the known optimal group constellations to ensure at least the same\nperformance but exploit the Gray code ordering structure of multiset\npermutations as a selection criterion at the decoder. The decoder achieves near\nmaximum likelihood performance at low computational cost and low additional\nmemory requirements at the receiver.\n", "category": [0, 2]}
{"abstract": "  We consider two-way wire-tap channels, where two users are communicating with\neach other in the presence of an eavesdropper, who has access to the\ncommunications through a multiple-access channel. We find achievable rates for\ntwo different scenarios, the Gaussian two-way wire-tap channel, (GTW-WT), and\nthe binary additive two-way wire-tap channel, (BATW-WT). It is shown that the\ntwo-way channels inherently provide a unique advantage for wire-tapped\nscenarios, as the users know their own transmitted signals and in effect help\nencrypt the other user's messages, similar to a one-time pad. We compare the\nachievable rates to that of the Gaussian multiple-access wire-tap channel\n(GMAC-WT) to illustrate this advantage.\n", "category": [0, 0, 2]}
{"abstract": "  Successful predictions are among the most compelling validations of any\nmodel. Extracting falsifiable predictions from nonlinear multiparameter models\nis complicated by the fact that such models are commonly sloppy, possessing\nsensitivities to different parameter combinations that range over many decades.\nHere we discuss how sloppiness affects the sorts of data that best constrain\nmodel predictions, makes linear uncertainty approximations dangerous, and\nintroduces computational difficulties in Monte-Carlo uncertainty analysis. We\nalso present a useful test problem and suggest refinements to the standards by\nwhich models are communicated.\n", "category": [4]}
{"abstract": "  Cooperative diversity systems are wireless communication systems designed to\nexploit cooperation among users to mitigate the effects of multipath fading. In\nfairly general conditions, it has been shown that these systems can achieve the\ndiversity order of an equivalent MISO channel and, if the node geometry\npermits, virtually the same outage probability can be achieved as that of the\nequivalent MISO channel for a wide range of applicable SNR. However, much of\nthe prior analysis has been performed under the assumption of perfect timing\nand frequency offset synchronization. In this paper, we derive the estimation\nbounds and associated maximum likelihood estimators for frequency offset\nestimation in a cooperative communication system. We show the benefit of\nadaptively tuning the frequency of the relay node in order to reduce estimation\nerror at the destination. We also derive an efficient estimation algorithm,\nbased on the correlation sequence of the data, which has mean squared error\nclose to the Cramer-Rao Bound.\n", "category": [6, 2, 6]}
{"abstract": "  In most vertebrate species, the body axis is generated by the formation of\nrepeated transient structures called somites. This spatial periodicity in\nsomitogenesis has been related to the temporally sustained oscillations in\ncertain mRNAs and their associated gene products in the cells forming the\npresomatic mesoderm. The mechanism underlying these oscillations have been\nidentified as due to the delays involved in the synthesis of mRNA and\ntranslation into protein molecules [J. Lewis, Current Biol. {\\bf 13}, 1398\n(2003)]. In addition, in the zebrafish embryo intercellular Notch signalling\ncouples these oscillators and a longitudinal positional information signal in\nthe form of an Fgf8 gradient exists that could be used to transform these\ncoupled temporal oscillations into the observed spatial periodicity of somites.\nHere we consider a simple model based on this known biology and study its\nconsequences for somitogenesis. Comparison is made with the known properties of\nsomite formation in the zebrafish embryo . We also study the effects of\nlocalized Fgf8 perturbations on somite patterning.\n", "category": [4]}
{"abstract": "  This work examines the problem of sequential detection of a change in the\ndrift of a Brownian motion in the case of two-sided alternatives. Applications\nto real life situations in which two-sided changes can occur are discussed.\nTraditionally, 2-CUSUM stopping rules have been used for this problem due to\ntheir asymptotically optimal character as the mean time between false alarms\ntends to $\\infty$. In particular, attention has focused on 2-CUSUM harmonic\nmean rules due to the simplicity in calculating their first moments. In this\npaper, we derive closed-form expressions for the first moment of a general\n2-CUSUM stopping rule. We use these expressions to obtain explicit upper and\nlower bounds for it. Moreover, we derive an expression for the rate of change\nof this first moment as one of the threshold parameters changes. Based on these\nexpressions we obtain explicit upper and lower bounds to this rate of change.\nUsing these expressions we are able to find the best 2-CUSUM stopping rule with\nrespect to the extended Lorden criterion. In fact, we demonstrate not only the\nexistence but also the uniqueness of the best 2-CUSUM stopping both in the case\nof a symmetric change and in the case of a non-symmetric case. Furthermore, we\ndiscuss the existence of a modification of the 2-CUSUM stopping rule that has a\nstrictly better performance than its classical 2-CUSUM counterpart for small\nvalues of the mean time between false alarms. We conclude with a discussion on\nthe open problem of strict optimality in the case of two-sided alternatives.\n", "category": [0, 2]}
{"abstract": "  A new class of space time codes with high performance is presented. The code\ndesign utilizes tailor-made permutation codes, which are known to have large\nminimal distances as spherical codes. A geometric connection between spherical\nand space time codes has been used to translate them into the final space time\ncodes. Simulations demonstrate that the performance increases with the block\nlengths, a result that has been conjectured already in previous work. Further,\nthe connection to permutation codes allows for moderate complex en-/decoding\nalgorithms.\n", "category": [0, 2]}
{"abstract": "  We demonstrate the power of the genetic algorithms to construct the cellular\nautomata model simulating the growth of 2-dimensional close-to-circular\nclusters revealing the desired properties, such as the growth rate and, at the\nsame time, the fractal behavior of their contours. The possible application of\nthe approach in the field of tumor modeling is outlined.\n", "category": [4]}
{"abstract": "  We describe an algorithm for evaluation of the interval extension of the\npower function of variables x and y given by the expression x^y. Our algorithm\nreduces the general case to the case of non-negative bases.\n", "category": [0]}
{"abstract": "  This paper considers the problem of reasoning on massive amounts of (possibly\ndistributed) data. Presently, existing proposals show some limitations: {\\em\n(i)} the quantity of data that can be handled contemporarily is limited, due to\nthe fact that reasoning is generally carried out in main-memory; {\\em (ii)} the\ninteraction with external (and independent) DBMSs is not trivial and, in\nseveral cases, not allowed at all; {\\em (iii)} the efficiency of present\nimplementations is still not sufficient for their utilization in complex\nreasoning tasks involving massive amounts of data. This paper provides a\ncontribution in this setting; it presents a new system, called DLV$^{DB}$,\nwhich aims to solve these problems. Moreover, the paper reports the results of\na thorough experimental analysis we have carried out for comparing our system\nwith several state-of-the-art systems (both logic and databases) on some\nclassical deductive problems; the other tested systems are: LDL++, XSB, Smodels\nand three top-level commercial DBMSs. DLV$^{DB}$ significantly outperforms even\nthe commercial Database Systems on recursive queries. To appear in Theory and\nPractice of Logic Programming (TPLP)\n", "category": [0, 0]}
{"abstract": "  An epidemic multi-strain model with temporary cross-immunity shows chaos,\neven in a previously unexpected parameter region. Especially dengue fever\nmodels with strong enhanced infectivity on secondary infection have previously\nshown deterministic chaos motivated by experimental findings of\nantibody-dependent-enhancement (ADE). Including temporary cross-immunity in\nsuch models, which is common knowledge among field researchers in dengue, we\nfind a deterministically chaotic attractor in the more realistic parameter\nregion of reduced infectivity on secondary infection (''inverse ADE'' parameter\nregion). This is realistic for dengue fever since on second infection people\nare more likely to be hospitalized, hence do not contribute to the force of\ninfection as much as people with first infection.\n  Our finding has wider implications beyond dengue in any multi-strain\nepidemiological systems with altered infectivity upon secondary infection,\nsince we can relax the condition of rather high infectivity on secondary\ninfection previously required for deterministic chaos. For dengue the finding\nof wide ranges of chaotic attractors open new ways to analysis of existing data\nsets.\n", "category": [3, 4]}
{"abstract": "  We analyse and compare the complexity of several algorithms for computing\nmodular polynomials. We show that an algorithm relying on floating point\nevaluation of modular functions and on interpolation, which has received little\nattention in the literature, has a complexity that is essentially (up to\nlogarithmic factors) linear in the size of the computed polynomials. In\nparticular, it obtains the classical modular polynomials $\\Phi_\\ell$ of prime\nlevel $\\ell$ in time O (\\ell^3 \\log^4 \\ell \\log \\log \\ell). Besides treating\nmodular polynomials for $\\Gamma^0 (\\ell)$, which are an important ingredient in\nmany algorithms dealing with isogenies of elliptic curves, the algorithm is\neasily adapted to more general situations. Composite levels are handled just as\neasily as prime levels, as well as polynomials between a modular function and\nits transform of prime level, such as the Schl\\\"afli polynomials and their\ngeneralisations. Our distributed implementation of the algorithm confirms the\ntheoretical analysis by computing modular equations of record level around\n10000 in less than two weeks on ten processors.\n", "category": [2, 0]}
{"abstract": "  This paper reports about the development of two provably correct approximate\nalgorithms which calculate the Euclidean shortest path (ESP) within a given\ncube-curve with arbitrary accuracy, defined by $\\epsilon >0$, and in time\ncomplexity $\\kappa(\\epsilon) \\cdot {\\cal O}(n)$, where $\\kappa(\\epsilon)$ is\nthe length difference between the path used for initialization and the\nminimum-length path, divided by $\\epsilon$. A run-time diagram also illustrates\nthis linear-time behavior of the implemented ESP algorithm.\n", "category": [0, 0]}
{"abstract": "  RNA motifs typically consist of short, modular patterns that include base\npairs formed within and between modules. Estimating the abundance of these\npatterns is of fundamental importance for assessing the statistical\nsignificance of matches in genomewide searches, and for predicting whether a\ngiven function has evolved many times in different species or arose from a\nsingle common ancestor. In this manuscript, we review in an integrated and\nself-contained manner some basic concepts of automata theory, generating\nfunctions and transfer matrix methods that are relevant to pattern analysis in\nbiological sequences. We formalize, in a general framework, the concept of\nMarkov chain embedding to analyze patterns in random strings produced by a\nmemoryless source. This conceptualization, together with the capability of\nautomata to recognize complicated patterns, allows a systematic analysis of\nproblems related to the occurrence and frequency of patterns in random strings.\nThe applications we present focus on the concept of synchronization of\nautomata, as well as automata used to search for a finite number of keywords\n(including sets of patterns generated according to base pairing rules) in a\ngeneral text.\n", "category": [2, 2, 2, 4, 4, 6]}
{"abstract": "  The time-dependent transverse response of stiff polymers, represented as\nweakly-bending wormlike chains (WLCs), is well-understood on the linear level,\nwhere transverse degrees of freedom evolve independently from the longitudinal\nones. We show that, beyond a characteristic time scale, the nonlinear coupling\nof transverse and longitudinal motion in an inextensible WLC significantly\nweakens the polymer response compared to the widely used linear response\npredictions. The corresponding feedback mechanism is rationalized by scaling\narguments and quantified by a multiple scale approach that exploits an inherent\nseparation of transverse and longitudinal correlation length scales. Crossover\nscaling laws and exact analytical and numerical solutions for characteristic\nresponse quantities are derived for different experimentally relevant setups.\nOur findings are applicable to cytoskeletal filaments as well as DNA under\ntension.\n", "category": [3, 4]}
{"abstract": "  Linear measures such as cross-correlation have been used successfully to\ndetermine time delays from the given processes. Such an analysis often precedes\nidentifying possible causal relationships between the observed processes. The\npresent study investigates the impact of a positively correlated driver whose\ncorrelation function decreases monotonically with lag on the delay estimation\nin a two-node acyclic network with one and two-delays. It is shown that\ncross-correlation analysis of the given processes can result in spurious\nidentification of multiple delays between the driver and the dependent\nprocesses. Subsequently, delay estimation of increment process as opposed to\nthe original process under certain implicit constraints is explored.\nShort-range and long-range correlated driver processes along with those of\ntheir coarse-grained counterparts are considered.\n", "category": [4, 4]}
{"abstract": "  We have built an open-source software system for the modeling of biomolecular\nreaction networks, SloppyCell, which is written in Python and makes substantial\nuse of third-party libraries for numerics, visualization, and parallel\nprogramming. We highlight here some of the powerful features that Python\nprovides that enable SloppyCell to do dynamic code synthesis, symbolic\nmanipulation, and parallel exploration of complex parameter spaces.\n", "category": [4, 4]}
{"abstract": "  In the peripheral nervous system, utrophin and the short dystrophin isoform\n(Dp116) are co-localized at the outermost layer of the myelin sheath of nerve\nfibers; together with the dystroglycan complex. In peripheral nerve, matrix\nmetalloproteinase (MMP) creates a 30 kDa fragment of beta-dystroglycan, leading\nto a disruption of the link between the extracellular matrix and the cell\nmembrane. Here we asked if the processing of the beta-dystroglycan could\ninfluence the anchorage of Dp116 or/and utrophin in normal and mdx Schwann cell\nmembrane. We showed that MMP-9 was more activated in mdx nerve than in\nwild-type one. This activation leads to an accumulation of the 30 kDa\nbeta-dystroglycan isoform and have an impact on the anchorage of Dp116 and\nutrophin isoforms in mdx Schwann cells membrane. Our results showed that Dp116\nhad greater affinity to the full length form of beta-dystroglycan than the 30\nkDa form. Moreover, we showed for the first time that the short isoform of\nutrophin (Up71) was over-expressed in mdx Schwann cells compared to wild-type.\nIn addition, this utrophin isoform (Up71) seems to have greater affinity to the\n30 kDa beta-dystroglycan which could explain a more stabilization of this 30\nkDa at the membrane compartment. Our results highlight the potential\nparticipation of the short utrophin isoform and the cleaved form of\nbeta-dystroglycan in mdx Schwann cell membrane architecture.\n", "category": [4]}
{"abstract": "  Transcription regulation typically involves the binding of proteins over long\ndistances on multiple DNA sites that are brought close to each other by the\nformation of DNA loops. The inherent complexity of the assembly of regulatory\ncomplexes on looped DNA challenges the understanding of even the simplest\ngenetic systems, including the prototypical lac operon. Here we implement a\nscalable quantitative computational approach to analyze systems regulated\nthrough multiple DNA sites with looping. Our approach applied to the lac operon\naccurately predicts the transcription rate over five orders of magnitude for\nwild type and seven mutants accounting for all the combinations of deletions of\nthe three operators. A quantitative analysis of the model reveals that the\npresence of three operators provides a mechanism to combine robust repression\nwith sensitive induction, two seemingly mutually exclusive properties that are\nrequired for optimal functioning of metabolic switches.\n", "category": [4, 4]}
{"abstract": "  Kernel-based nonparametric hazard rate estimation is considered with a\nspecial class of infinite-order kernels that achieves favorable bias and mean\nsquare error properties. A fully automatic and adaptive implementation of a\ndensity and hazard rate estimator is proposed for randomly right censored data.\nCareful selection of the bandwidth in the proposed estimators yields estimates\nthat are more efficient in terms of overall mean squared error performance, and\nin some cases achieves a nearly parametric convergence rate. Additionally,\nrapidly converging bandwidth estimates are presented for use in second-order\nkernels to supplement such kernel-based methods in hazard rate estimation.\nSimulations illustrate the improved accuracy of the proposed estimator against\nother nonparametric estimators of the density and hazard function. A real data\napplication is also presented on survival data from 13,166 breast carcinoma\npatients.\n", "category": [2, 6]}
{"abstract": "  In this note we correct an omission in our paper (Satheesh and Sandhya, 2005)\nin defining semi-selfdecomposable laws and also show with examples that the\nmarginal distributions of a stationary AR(1) process need not even be\ninfinitely divisible.\n", "category": [2, 2, 6]}
{"abstract": "  The present paper aims at introducing the innovative technologies, based on\nthe concept of \"sensory substitution\" or \"perceptual supplementation\", we are\ndeveloping in the fields of human disability and biomedical engineering.\nPrecisely, our goal is to design, develop and validate practical assistive\nbiomedical and/technical devices and/or rehabilitating procedures for persons\nwith disabilities, using artificial tongue-placed tactile biofeedback systems.\nProposed applications are dealing with: (1) pressure sores prevention in case\nof spinal cord injuries (persons with paraplegia, or tetraplegia); (2) ankle\nproprioceptive acuity improvement for driving assistance in older and/or\ndisabled adults; and (3) balance control improvement to prevent fall in older\nand/or disabled adults. This paper presents results of three feasibility\nstudies performed on young healthy adults.\n", "category": [3, 4]}
{"abstract": "  How can a microorganism adapt to a variety of environmental conditions\ndespite there exists a limited number of signal transduction machineries? We\nshow that for any growing cells whose gene expression is under stochastic\nfluctuations, adaptive cellular state is inevitably selected by noise, even\nwithout specific signal transduction network for it. In general, changes in\nprotein concentration in a cell are given by its synthesis minus dilution and\ndegradation, both of which are proportional to the rate of cell growth. In an\nadaptive state with a higher growth speed, both terms are large and balanced.\nUnder the presence of noise in gene expression, the adaptive state is less\naffected by stochasticity since both the synthesis and dilution terms are\nlarge, while for a non-adaptive state both the terms are smaller so that cells\nare easily kicked out of the original state by noise. Hence, escape time from a\ncellular state and the cellular growth rate are negatively correlated. This\nleads to a selection of adaptive states with higher growth rates, and model\nsimulations confirm this selection to take place in general. The results\nsuggest a general form of adaptation that has never been brought to light - a\nprocess that requires no specific machineries for sensory adaptation. The\npresent scheme may help explain a wide range of cellular adaptive responses\nincluding the metabolic flux optimization for maximal cell growth.\n", "category": [4]}
{"abstract": "  Pressure ulcers are recognized as a major health issue in individuals with\nspinal cord injuries and new approaches to prevent this pathology are\nnecessary. An innovative health strategy is being developed through the use of\ncomputer and sensory substitution via the tongue in order to compensate for the\nsensory loss in the buttock area for individuals with paraplegia. This sensory\ncompensation will enable individuals with spinal cord injuries to be aware of a\nlocalized excess of pressure at the skin/seat interface and, consequently, will\nenable them to prevent the formation of pressure ulcers by relieving the\ncutaneous area of suffering. This work reports an initial evaluation of this\napproach and the feasibility of creating an adapted behavior, with a change in\npressure as a response to electro-stimulated information on the tongue.\nObtained during a clinical study in 10 healthy seated subjects, the first\nresults are encouraging, with 92% success in 100 performed tests. These\nresults, which have to be completed and validated in the paraplegic population,\nmay lead to a new approach to education in health to prevent the formation of\npressure ulcers within this population. Keywords: Spinal Cord Injuries,\nPressure Ulcer, Sensory Substitution, Health Education, Biomedical Informatics.\n", "category": [3, 4]}
{"abstract": "  Boolean Networks and their dynamics are of great interest as abstract\nmodeling schemes in various disciplines, ranging from biology to computer\nscience. Whereas parallel update schemes have been studied extensively in past\nyears, the level of understanding of asynchronous updates schemes is still very\npoor. In this paper we study the propagation of external information given by\nregulatory input variables into a random Boolean network. We compute both\nanalytically and numerically the time evolution and the asymptotic behavior of\nthis propagation of external regulation (PER). In particular, this allows us to\nidentify variables which are completely determined by this external\ninformation. All those variables in the network which are not directly fixed by\nPER form a core which contains in particular all non-trivial feedback loops. We\ndesign a message-passing approach allowing to characterize the statistical\nproperties of these cores in dependence of the Boolean network and the external\ncondition. At the end we establish a link between PER dynamics and the full\nrandom asynchronous dynamics of a Boolean network.\n", "category": [3, 3, 4]}
{"abstract": "  One of the major problems in computational biology is the inability of\nexisting classification models to incorporate expanding and new domain\nknowledge. This problem of static classification models is addressed in this\npaper by the introduction of incremental learning for problems in\nbioinformatics. Many machine learning tools have been applied to this problem\nusing static machine learning structures such as neural networks or support\nvector machines that are unable to accommodate new information into their\nexisting models. We utilize the fuzzy ARTMAP as an alternate machine learning\nsystem that has the ability of incrementally learning new data as it becomes\navailable. The fuzzy ARTMAP is found to be comparable to many of the widespread\nmachine learning systems. The use of an evolutionary strategy in the selection\nand combination of individual classifiers into an ensemble system, coupled with\nthe incremental learning ability of the fuzzy ARTMAP is proven to be suitable\nas a pattern classifier. The algorithm presented is tested using data from the\nG-Coupled Protein Receptors Database and shows good accuracy of 83%. The system\npresented is also generally applicable, and can be used in problems in genomics\nand proteomics.\n", "category": [0, 4]}
{"abstract": "  The estimation of missing input vector elements in real time processing\napplications requires a system that possesses the knowledge of certain\ncharacteristics such as correlations between variables, which are inherent in\nthe input space. Computational intelligence techniques and maximum likelihood\ntechniques do possess such characteristics and as a result are important for\nimputation of missing data. This paper compares two approaches to the problem\nof missing data estimation. The first technique is based on the current state\nof the art approach to this problem, that being the use of Maximum Likelihood\n(ML) and Expectation Maximisation (EM. The second approach is the use of a\nsystem based on auto-associative neural networks and the Genetic Algorithm as\ndiscussed by Adbella and Marwala3. The estimation ability of both of these\ntechniques is compared, based on three datasets and conclusions are made.\n", "category": [6]}
{"abstract": "  The organization of the connectivity between mammalian cortical areas has\nbecome a major subject of study, because of its important role in scaffolding\nthe macroscopic aspects of animal behavior and intelligence. In this study we\npresent a computational reconstruction approach to the problem of network\norganization, by considering the topological and spatial features of each area\nin the primate cerebral cortex as subsidy for the reconstruction of the global\ncortical network connectivity. Starting with all areas being disconnected,\npairs of areas with similar sets of features are linked together, in an attempt\nto recover the original network structure. Inferring primate cortical\nconnectivity from the properties of the nodes, remarkably good reconstructions\nof the global network organization could be obtained, with the topological\nfeatures allowing slightly superior accuracy to the spatial ones. Analogous\nreconstruction attempts for the C. elegans neuronal network resulted in\nsubstantially poorer recovery, indicating that cortical area interconnections\nare relatively stronger related to the considered topological and spatial\nproperties than neuronal projections in the nematode. The close relationship\nbetween area-based features and global connectivity may hint on developmental\nrules and constraints for cortical networks. Particularly, differences between\nthe predictions from topological and spatial properties, together with the\npoorer recovery resulting from spatial properties, indicate that the\norganization of cortical networks is not entirely determined by spatial\nconstraints.\n", "category": [4, 3]}
{"abstract": "  Position determination in biological systems is often achieved through\nprotein concentration gradients. Measuring the local concentration of such a\nprotein with a spatially-varying distribution allows the measurement of\nposition within the system. In order for these systems to work effectively,\nposition determination must be robust to noise. Here, we calculate fundamental\nlimits to the precision of position determination by concentration gradients\ndue to unavoidable biochemical noise perturbing the gradients. We focus on\ngradient proteins with first order reaction kinetics. Systems of this type have\nbeen experimentally characterised in both developmental and cell biology\nsettings. For a single gradient we show that, through time-averaging, great\nprecision can potentially be achieved even with very low protein copy numbers.\nAs a second example, we investigate the ability of a system with oppositely\ndirected gradients to find its centre. With this mechanism, positional\nprecision close to the centre improves more slowly with increasing averaging\ntime, and so longer averaging times or higher copy numbers are required for\nhigh precision. For both single and double gradients, we demonstrate the\nexistence of optimal length scales for the gradients, where precision is\nmaximized, as well as analyzing how precision depends on the size of the\nconcentration measuring apparatus. Our results provide fundamental constraints\non the positional precision supplied by concentration gradients in various\ncontexts, including both in developmental biology and also within a single\ncell.\n", "category": [4, 3]}
{"abstract": "  Simple nonlinear dynamical systems with multiple stable stationary states are\noften taken as models for switchlike biological systems. This paper considers\nthe interaction of multiple such simple multistable systems when they are\nembedded together into a larger dynamical \"supersystem.\" Attention is focused\non the network structure of the resulting set of coupled differential\nequations, and the consequences of this structure on the propensity of the\nembedded switches to act independently versus cooperatively. Specifically, it\nis argued that both larger average and larger variance of the node degree\ndistribution lead to increased switch independence. Given the frequency of\nempirical observations of high variance degree distributions (e.g., power-law)\nin biological networks, it is suggested that the results presented here may aid\nin identifying switch-integrating subnetworks as comparatively homogenous,\nlow-degree, substructures. Potential applications to ecological problems such\nas the relationship of stability and complexity are also briefly discussed.\n", "category": [4, 3, 2, 3]}
{"abstract": "  This paper proposes a method to address the longstanding problem of lack of\nmonotonicity in estimation of conditional and structural quantile functions,\nalso known as the quantile crossing problem. The method consists in sorting or\nmonotone rearranging the original estimated non-monotone curve into a monotone\nrearranged curve. We show that the rearranged curve is closer to the true\nquantile curve in finite samples than the original curve, establish a\nfunctional delta method for rearrangement-related operators, and derive\nfunctional limit theory for the entire rearranged curve and its functionals. We\nalso establish validity of the bootstrap for estimating the limit law of the\nthe entire rearranged curve and its functionals. Our limit results are generic\nin that they apply to every estimator of a monotone econometric function,\nprovided that the estimator satisfies a functional central limit theorem and\nthe function satisfies some smoothness conditions. Consequently, our results\napply to estimation of other econometric functions with monotonicity\nrestrictions, such as demand, production, distribution, and structural\ndistribution functions. We illustrate the results with an application to\nestimation of structural quantile functions using data on Vietnam veteran\nstatus and earnings.\n", "category": [6, 7, 2, 6]}
{"abstract": "  Suppose that a target function is monotonic, namely, weakly increasing, and\nan original estimate of the target function is available, which is not weakly\nincreasing. Many common estimation methods used in statistics produce such\nestimates. We show that these estimates can always be improved with no harm\nusing rearrangement techniques: The rearrangement methods, univariate and\nmultivariate, transform the original estimate to a monotonic estimate, and the\nresulting estimate is closer to the true curve in common metrics than the\noriginal estimate. We illustrate the results with a computational example and\nan empirical example dealing with age-height growth charts.\n", "category": [6, 7, 2, 6]}
{"abstract": "  We present an exhaustive study of more than 250 ab initio potential energy\nsurfaces (PESs) of the model dipeptide HCO-L-Ala-NH2. The model chemistries\n(MCs) used are constructed as homo- and heterolevels involving possibly\ndifferent RHF and MP2 calculations for the geometry and the energy. The basis\nsets used belong to a sample of 39 selected representants from Pople's\nsplit-valence families, ranging from the small 3-21G to the large\n6-311++G(2df,2pd). The reference PES to which the rest are compared is the\nMP2/6-311++G(2df,2pd) homolevel, which, as far as we are aware, is the more\naccurate PES of a dipeptide in the literature. The aim of the study presented\nis twofold: On the one hand, the evaluation of the influence of polarization\nand diffuse functions in the basis set, distinguishing between those placed at\n1st-row atoms and those placed at hydrogens, as well as the effect of different\ncontraction and valence splitting schemes. On the other hand, the investigation\nof the heterolevel assumption, which is defined here to be that which states\nthat heterolevel MCs are more efficient than homolevel MCs. The heterolevel\napproximation is very commonly used in the literature, but it is seldom\nchecked. As far as we know, the only tests for peptides or related systems,\nhave been performed using a small number of conformers, and this is the first\ntime that this potentially very economical approximation is tested in full\nPESs. In order to achieve these goals, all data sets have been compared and\nanalyzed in a way which captures the nearness concept in the space of MCs.\n", "category": [4, 3, 4]}
{"abstract": "  Late long-term potentiation (L-LTP) appears essential for the formation of\nlong-term memory, with memories at least partly encoded by patterns of\nstrengthened synapses. How memories are preserved for months or years, despite\nmolecular turnover, is not well understood. Ongoing recurrent neuronal\nactivity, during memory recall or during sleep, has been hypothesized to\npreferentially potentiate strong synapses, preserving memories. This hypothesis\nhas not been evaluated in the context of a mathematical model representing\nbiochemical pathways important for L-LTP. I incorporated ongoing activity into\ntwo such models: a reduced model that represents some of the essential\nbiochemical processes, and a more detailed published model. The reduced model\nrepresents synaptic tagging and gene induction intuitively, and the detailed\nmodel adds activation of essential kinases by Ca. Ongoing activity was modeled\nas continual brief elevations of [Ca]. In each model, two stable states of\nsynaptic weight resulted. Positive feedback between synaptic weight and the\namplitude of ongoing Ca transients underlies this bistability. A tetanic or\ntheta-burst stimulus switches a model synapse from a low weight to a high\nweight stabilized by ongoing activity. Bistability was robust to parameter\nvariations. Simulations illustrated that prolonged decreased activity reset\nsynapses to low weights, suggesting a plausible forgetting mechanism. However,\nepisodic activity with shorter inactive intervals maintained strong synapses.\nBoth models support experimental predictions. Tests of these predictions are\nexpected to further understanding of how neuronal activity is coupled to\nmaintenance of synaptic strength.\n", "category": [4, 4]}
{"abstract": "  We study the growth of a directed transportation network, such as a food web,\nin which links carry resources. We propose a growth process in which new nodes\n(or species) preferentially attach to existing nodes with high indegree (in\nfood-web language, number of prey) and low outdegree (or number of predators).\nThis scheme, which we call inverse preferential attachment, is intended to\nmaximize the amount of resources available to each new node. We show that the\noutdegree (predator) distribution decays at least exponentially fast for large\noutdegree and is continuously tunable between an exponential distribution and a\ndelta function. The indegree (prey) distribution is poissonian in the\nlarge-network limit.\n", "category": [3, 4]}
{"abstract": "  The properties of certain networks are determined by hidden variables that\nare not explicitly measured. The conditional probability (propagator) that a\nvertex with a given value of the hidden variable is connected to k of other\nvertices determines all measurable properties. We study hidden variable models\nand find an averaging approximation that enables us to obtain a general\nanalytical result for the propagator. Analytic results showing the validity of\nthe approximation are obtained. We apply hidden variable models to\nprotein-protein interaction networks (PINs) in which the hidden variable is the\nassociation free-energy, determined by distributions that depend on\nbiochemistry and evolution. We compute degree distributions as well as\nclustering coefficients of several PINs of different species; good agreement\nwith measured data is obtained. For the human interactome two different\nparameter sets give the same degree distributions, but the computed clustering\ncoefficients differ by a factor of about two. This shows that degree\ndistributions are not sufficient to determine the properties of PINs.\n", "category": [4, 3, 3, 4]}
{"abstract": "  A general and basic model of primordial evolution--a soup of reacting\nfinitary and discrete processes--is employed to identify and analyze\nfundamental mechanisms that generate and maintain complex structures in\nprebiotic systems. The processes--$\\epsilon$-machines as defined in\ncomputational mechanics--and their interaction networks both provide well\ndefined notions of structure. This enables us to quantitatively demonstrate\nhierarchical self-organization in the soup in terms of complexity. We found\nthat replicating processes evolve the strategy of successively building higher\nlevels of organization by autocatalysis. Moreover, this is facilitated by local\ncomponents that have low structural complexity, but high generality. In effect,\nthe finitary process soup spontaneously evolves a selection pressure that\nfavors such components. In light of the finitary process soup's generality,\nthese results suggest a fundamental law of hierarchical systems: global\ncomplexity requires local simplicity.\n", "category": [4, 4]}
{"abstract": "  We review the decomposition method of stock return cross-correlations,\npresented previously for studying the dependence of the correlation coefficient\non the resolution of data (Epps effect). Through a toy model of random\nwalk/Brownian motion and memoryless renewal process (i.e. Poisson point\nprocess) of observation times we show that in case of analytical treatability,\nby decomposing the correlations we get the exact result for the frequency\ndependence. We also demonstrate that our approach produces reasonable fitting\nof the dependence of correlations on the data resolution in case of empirical\ndata. Our results indicate that the Epps phenomenon is a product of the finite\ntime decay of lagged correlations of high resolution data, which does not scale\nwith activity. The characteristic time is due to a human time scale, the time\nneeded to react to news.\n", "category": [5, 3, 3]}
{"abstract": "  Background: Duplication of genes is important for evolution of molecular\nnetworks. Many authors have therefore considered gene duplication as a driving\nforce in shaping the topology of molecular networks. In particular it has been\nnoted that growth via duplication would act as an implicit way of preferential\nattachment, and thereby provide the observed broad degree distributions of\nmolecular networks.\n  Results: We extend current models of gene duplication and rewiring by\nincluding directions and the fact that molecular networks are not a result of\nunidirectional growth. We introduce upstream sites and downstream shapes to\nquantify potential links during duplication and rewiring. We find that this in\nitself generates the observed scaling of transcription factors for genome sites\nin procaryotes. The dynamical model can generate a scale-free degree\ndistribution, p(k)&prop; 1/k^&gamma;, with exponent &gamma;=1 in the\nnon-growing case, and with &gamma;>1 when the network is growing.\n  Conclusions: We find that duplication of genes followed by substantial\nrecombination of upstream regions could generate main features of genetic\nregulatory networks. Our steady state degree distribution is however to broad\nto be consistent with data, thereby suggesting that selective pruning acts as a\nmain additional constraint on duplicated genes. Our analysis shows that gene\nduplication can only be a main cause for the observed broad degree\ndistributions, if there is also substantial recombinations between upstream\nregions of genes.\n", "category": [4, 4]}
{"abstract": "  We consider the problem of detecting edges in piecewise smooth functions from\ntheir N-degree spectral content, which is assumed to be corrupted by noise.\nThere are three scales involved: the \"smoothness\" scale of order 1/N, the noise\nscale of order $\\eta$ and the O(1) scale of the jump discontinuities. We use\nconcentration factors which are adjusted to the noise variance, $\\eta$ >> 1/N,\nin order to detect the underlying O(1)-edges, which are separated from the\nnoise scale, $\\eta$ << 1.\n", "category": [2, 2, 6]}
{"abstract": "  Significant fraction (98.5% in humans) of most animal genomes is non- coding\ndark matter. Its largely unknown function (1-5) is related to programming\n(rather than to spontaneous mutations) of accurate adaptation to rapidly\nchanging environment. Programmed adaptation to the same universal law for\nnon-competing animals from anaerobic yeast to human is revealed in the study of\ntheir extensively quantified mortality (6-21). Adaptation of animals with\nremoved non-coding DNA fractions may specify their contribution to genomic\nprogramming. Emergence of new adaptation programs and their (non-Mendelian)\nheredity may be studied in antibiotic mini-extinctions (22-24). On a large\nevolutionary scale rapid universal adaptation was vital for survival, and\nevolved, in otherwise lethal for diverse species major mass extinctions\n(25-28). Evolutionary and experimental data corroborate these conclusions\n(6-21, 29-32). Universal law implies certain biological universality of diverse\nspecies, thus quantifies applicability of animal models to humans). Genomic\nadaptation programming calls for unusual approach to its study and implies\nunanticipated perspectives, in particular, directed biological changes.\n", "category": [4, 3, 3, 4, 4, 4]}
{"abstract": "  In this paper we develop a scientific approach to control inter-country\nconflict. This system makes use of a neural network and a feedback control\napproach. It was found that by controlling the four controllable inputs:\nDemocracy, Dependency, Allies and Capability simultaneously, all the predicted\ndispute outcomes could be avoided. Furthermore, it was observed that\ncontrolling a single input Dependency or Capability also avoids all the\npredicted conflicts. When the influence of each input variable on conflict is\nassessed, Dependency, Capability, and Democracy emerge as key variables that\ninfluence conflict.\n", "category": [6]}
{"abstract": "  The probability distributions for bending angles in double helical DNA\nobtained in all-atom molecular dynamics simulations are compared with\ntheoretical predictions. The computed distributions remarkably agree with the\nworm-like chain theory for double helices of one helical turn and longer, and\nqualitatively differ from predictions of the semi-elastic chain model. The\ncomputed data exhibit only small anomalies in the apparent flexibility of short\nDNA and cannot account for the recently reported AFM data (Wiggins et al,\nNature nanotechnology 1, 137 (2006)). It is possible that the current atomistic\nDNA models miss some essential mechanisms of DNA bending on intermediate length\nscales. Analysis of bent DNA structures reveals, however, that the bending\nmotion is structurally heterogeneous and directionally anisotropic on the\nintermediate length scales where the experimental anomalies were detected.\nThese effects are essential for interpretation of the experimental data and\nthey also can be responsible for the apparent discrepancy.\n", "category": [4, 3, 3]}
{"abstract": "  Schroedinger's book 'What is Life?' is widely credited for having played a\ncrucial role in development of molecular and cellular biology. My essay\nrevisits the issues raised by this book from the modern perspective of\nepigenetics and systems biology. I contrast two classes of potential mechanisms\nof epigenetic stability: 'epigenetic templating' and 'systems biology'\napproaches, and consider them from the point of view expressed by Schroedinger.\nI also discuss how quantum entanglement, a nonclassical feature of quantum\nmechanics, can help to address the 'problem of small numbers' that lead\nSchroedinger to promote the idea of molecular code-script for explanation of\nstability of biological order.\n", "category": [3, 4]}
{"abstract": "  By analyzing the relationships between a socioeconomical system modeled\nthrough evolutionary game theory and a physical system modeled through quantum\nmechanics we show how although both systems are described through two theories\napparently different both are analogous and thus exactly equivalents. The\nextensions of quantum mechanics to statistical physics and information theory\nlet us use some of their definitions for the best understanding of the behavior\nof economics and biology. The quantum analogue of the replicator dynamics is\nthe von Neumann equation. A system in where all its members are in Nash\nequilibrium is equivalent to a system in a maximum entropy state. Nature is a\ngame in where its players compete for a common welfare and the equilibrium of\nthe system that they are members. They act as a whole besides individuals like\nthey obey a rule in where they prefer to work for the welfare of the collective\nbesides the individual welfare.\n", "category": [5, 3]}
{"abstract": "  We show that the mutual fund theorems of Merton (1971) extend to the problem\nof optimal investment to minimize the probability of lifetime ruin. We obtain\ntwo such theorems by considering a financial market both with and without a\nriskless asset for random consumption. The striking result is that we obtain\ntwo-fund theorems despite the additional source of randomness from consumption.\n", "category": [5, 2, 2, 5]}
{"abstract": "  We study semiparametric efficiency bounds and efficient estimation of\nparameters defined through general moment restrictions with missing data.\nIdentification relies on auxiliary data containing information about the\ndistribution of the missing variables conditional on proxy variables that are\nobserved in both the primary and the auxiliary database, when such distribution\nis common to the two data sets. The auxiliary sample can be independent of the\nprimary sample, or can be a subset of it. For both cases, we derive bounds when\nthe probability of missing data given the proxy variables is unknown, or known,\nor belongs to a correctly specified parametric family. We find that the\nconditional probability is not ancillary when the two samples are independent.\nFor all cases, we discuss efficient semiparametric estimators. An estimator\nbased on a conditional expectation projection is shown to require milder\nregularity conditions than one based on inverse probability weighting.\n", "category": [2, 6]}
{"abstract": "  The stock market has been known to form homogeneous stock groups with a\nhigher correlation among different stocks according to common economic factors\nthat influence individual stocks. We investigate the role of common economic\nfactors in the market in the formation of stock networks, using the arbitrage\npricing model reflecting essential properties of common economic factors. We\nfind that the degree of consistency between real and model stock networks\nincreases as additional common economic factors are incorporated into our\nmodel. Furthermore, we find that individual stocks with a large number of links\nto other stocks in a network are more highly correlated with common economic\nfactors than those with a small number of links. This suggests that common\neconomic factors in the stock market can be understood in terms of\ndeterministic factors.\n", "category": [5, 3]}
{"abstract": "  We investigate dynamical systems characterized by a time series of distinct\nsemi-stable activity patterns, as they are observed in cortical neural activity\npatterns. We propose and discuss a general mechanism allowing for an adiabatic\ncontinuation between attractor networks and a specific adjoined transient-state\nnetwork, which is strictly dissipative. Dynamical systems with transient states\nretain functionality when their working point is autoregulated; avoiding\nprolonged periods of stasis or drifting into a regime of rapid fluctuations. We\nshow, within a continuous-time neural network model, that a single local\nupdating rule for online learning allows simultaneously (i) for information\nstorage via unsupervised Hebbian-type learning, (ii) for adaptive regulation of\nthe working point and (iii) for the suppression of runaway synaptic growth.\nSimulation results are presented; the spontaneous breaking of time-reversal\nsymmetry and link symmetry are discussed.\n", "category": [3, 3, 3, 4]}
{"abstract": "  We report quantitative relations between corruption level and economic\nfactors, such as country wealth and foreign investment per capita, which are\ncharacterized by a power law spanning multiple scales of wealth and investments\nper capita. These relations hold for diverse countries, and also remain stable\nover different time periods. We also observe a negative correlation between\nlevel of corruption and long-term economic growth. We find similar results for\ntwo independent indices of corruption, suggesting that the relation between\ncorruption and wealth does not depend on the specific measure of corruption.\nThe functional relations we report have implications when assessing the\nrelative level of corruption for two countries with comparable wealth, and for\nquantifying the impact of corruption on economic growth and foreign\ninvestments.\n", "category": [5, 3, 3]}
{"abstract": "  BACKGROUND: Many of the mutations accumulated by naturally evolving proteins\nare neutral in the sense that they do not significantly alter a protein's\nability to perform its primary biological function. However, new protein\nfunctions evolve when selection begins to favor other, \"promiscuous\" functions\nthat are incidental to a protein's biological role. If mutations that are\nneutral with respect to a protein's primary biological function cause\nsubstantial changes in promiscuous functions, these mutations could enable\nfuture functional evolution.\n  RESULTS: Here we investigate this possibility experimentally by examining how\ncytochrome P450 enzymes that have evolved neutrally with respect to activity on\na single substrate have changed in their abilities to catalyze reactions on\nfive other substrates. We find that the enzymes have sometimes changed as much\nas four-fold in the promiscuous activities. The changes in promiscuous\nactivities tend to increase with the number of mutations, and can be largely\nrationalized in terms of the chemical structures of the substrates. The\nactivities on chemically similar substrates tend to change in a coordinated\nfashion, potentially providing a route for systematically predicting the change\nin one function based on the measurement of several others.\n  CONCLUSIONS: Our work suggests that initially neutral genetic drift can lead\nto substantial changes in protein functions that are not currently under\nselection, in effect poising the proteins to more readily undergo functional\nevolution should selection \"ask new questions\" in the future.\n", "category": [4, 4]}
{"abstract": "  In many applications, input data are sampled functions taking their values in\ninfinite dimensional spaces rather than standard vectors. This fact has complex\nconsequences on data analysis algorithms that motivate modifications of them.\nIn fact most of the traditional data analysis tools for regression,\nclassification and clustering have been adapted to functional inputs under the\ngeneral name of functional Data Analysis (FDA). In this paper, we investigate\nthe use of Support Vector Machines (SVMs) for functional data analysis and we\nfocus on the problem of curves discrimination. SVMs are large margin classifier\ntools based on implicit non linear mappings of the considered data into high\ndimensional spaces thanks to kernels. We show how to define simple kernels that\ntake into account the unctional nature of the data and lead to consistent\nclassification. Experiments conducted on real world data emphasize the benefit\nof taking into account some functional aspects of the problems.\n", "category": [2, 6, 6]}
{"abstract": "  This Note proposes a new methodology for function classification with Support\nVector Machine (SVM). Rather than relying on projection on a truncated Hilbert\nbasis as in our previous work, we use an implicit spline interpolation that\nallows us to compute SVM on the derivatives of the studied functions. To that\nend, we propose a kernel defined directly on the discretizations of the\nobserved functions. We show that this method is universally consistent.\n", "category": [2, 6]}
{"abstract": "  Functional data analysis is a growing research field as more and more\npractical applications involve functional data. In this paper, we focus on the\nproblem of regression and classification with functional predictors: the model\nsuggested combines an efficient dimension reduction procedure [functional\nsliced inverse regression, first introduced by Ferr\\'e & Yao (Statistics, 37,\n2003, 475)], for which we give a regularized version, with the accuracy of a\nneural network. Some consistency results are given and the method is\nsuccessfully confronted to real-life data.\n", "category": [2, 6]}
{"abstract": "  The evolution of multicellular organisms from monocellular ancestors\nrepresents one of the greatest advances of the history of life. The assembly of\nsuch multicellular organisms requires signalling and response between cells:\nover millions of years these signalling processes have become extremely\nsophisticated and refined by evolution, such that study of modern organisms may\nnot be able to shed much light on the original ancient processes . Here we are\ninterested in determining how simple a signalling method can be, while still\nachieving self-assembly. In 2D a coupled cellular automaton/differential\nequation approach models organisms and chemotaxic chemicals, producing\nspiralling aggregation. In 3D Lennard-Jones-like particles are used to\nrepresent single cells, and their evolution in response to signalling is\nfollowed by molecular dynamics. It is found that if a single cell is able to\nemit a signal which induces others to move towards it, then a colony of\nsingle-cell organisms can assemble into shapes as complex as a tower, a ball\natop a stalk, or a fast-moving slug. The similarity with the behaviour of\nmodern Dictyostelium slime molds signalling with cyclic adenosine monophosphate\n(cAMP) is striking.\n", "category": [4, 4]}
{"abstract": "  We consider the least angle regression and forward stagewise algorithms for\nsolving penalized least squares regression problems. In Efron, Hastie,\nJohnstone & Tibshirani (2004) it is proved that the least angle regression\nalgorithm, with a small modification, solves the lasso regression problem. Here\nwe give an analogous result for incremental forward stagewise regression,\nshowing that it solves a version of the lasso problem that enforces\nmonotonicity. One consequence of this is as follows: while lasso makes optimal\nprogress in terms of reducing the residual sum-of-squares per unit increase in\n$L_1$-norm of the coefficient $\\beta$, forward stage-wise is optimal per unit\n$L_1$ arc-length traveled along the coefficient path. We also study a condition\nunder which the coefficient paths of the lasso are monotone, and hence the\ndifferent algorithms coincide. Finally, we compare the lasso and forward\nstagewise procedures in a simulation study involving a large number of\ncorrelated predictors.\n", "category": [2, 6]}
{"abstract": "  We provide a new algorithm for the treatment of inverse problems which\ncombines the traditional SVD inversion with an appropriate thresholding\ntechnique in a well chosen new basis. Our goal is to devise an inversion\nprocedure which has the advantages of localization and multiscale analysis of\nwavelet representations without losing the stability and computability of the\nSVD decompositions. To this end we utilize the construction of localized frames\n(termed \"needlets\") built upon the SVD bases. We consider two different\nsituations: the \"wavelet\" scenario, where the needlets are assumed to behave\nsimilarly to true wavelets, and the \"Jacobi-type\" scenario, where we assume\nthat the properties of the frame truly depend on the SVD basis at hand (hence\non the operator). To illustrate each situation, we apply the estimation\nalgorithm respectively to the deconvolution problem and to the Wicksell\nproblem. In the latter case, where the SVD basis is a Jacobi polynomial basis,\nwe show that our scheme is capable of achieving rates of convergence which are\noptimal in the $L_2$ case, we obtain interesting rates of convergence for other\n$L_p$ norms which are new (to the best of our knowledge) in the literature, and\nwe also give a simulation study showing that the NEED-D estimator outperforms\nother standard algorithms in almost all situations.\n", "category": [2, 6]}
{"abstract": "  The authors apply three methods of prospective modelling to high resolution\ngeoreferenced land cover data in a Mediterranean mountain area: GIS approach,\nnon linear parametric model and neuronal network. Land cover prediction to the\nlatest known date is used to validate the models. In the frame of\nspatial-temporal dynamics in open systems results are encouraging and\ncomparable. Correct prediction scores are about 73 %. The results analysis\nfocuses on geographic location, land cover categories and parametric distance\nto reality of the residues. Crossing the three models show the high degree of\nconvergence and a relative similitude of the results obtained by the two\nstatistic approaches compared to the GIS supervised model. Steps under work are\nthe application of the models to other test areas and the identification of\nrespective advantages to develop an integrated model.\n", "category": [6, 6]}
{"abstract": "  In the simplest view of transcriptional regulation, the expression of a gene\nis turned on or off by changes in the concentration of a transcription factor\n(TF). We use recent data on noise levels in gene expression to show that it\nshould be possible to transmit much more than just one regulatory bit.\nRealizing this optimal information capacity would require that the dynamic\nrange of TF concentrations used by the cell, the input/output relation of the\nregulatory module, and the noise levels of binding and transcription satisfy\ncertain matching relations. This parameter-free prediction is in good agreement\nwith recent experiments on the Bicoid/Hunchback system in the early Drosophila\nembryo, and this system achieves ~90% of its theoretical maximum information\ntransmission.\n", "category": [4]}
{"abstract": "  Previously, we have identified the cytoplasmic zinc metalloprotease\ninsulin-degrading enzyme(IDE) in human tissues by an immunohistochemical method\ninvolving no antigen retrieval (AR) by pressure cooking to avoid artifacts by\nendogenous biotin exposure and a detection kit based on the labeled\nstreptavidin biotin (LSAB) method. Thereby, we also employed 3% hydrogen\nperoxide(H2O2) for the inhibition of endogenous peroxidase activity and\nincubated the tissue sections with the biotinylated secondary antibody at room\ntemperature (RT). We now add the immunohistochemical details that had led us to\nthis optimized procedure as they also bear a more general relevance when\ndemonstrating intracellular tissue antigens. Our most important result is that\nendogenous peroxidase inhibition by 0.3% H2O2 coincided with an apparently\npositive IDE staining in an investigated breast cancer specimen whereas\ncombining a block by 3% H2O2 with an incubation of the biotinylated secondary\nantibody at RT, yet not at 37 degrees Celsius, revealed this specimen as almost\nentirely IDE-negative. Our present data caution against three different\nimmunohistochemical pitfalls that might cause falsely positive results and\nartifacts when using an LSAB- and peroxidase-based detection method: pressure\ncooking for AR, insufficient quenching of endogenous peroxidases and heating of\ntissue sections while incubating with biotinylated secondary antibodies.\n", "category": [4, 4]}
{"abstract": "  Using former maps, geographers intend to study the evolution of the land\ncover in order to have a prospective approach on the future landscape;\npredictions of the future land cover, by the use of older maps and\nenvironmental variables, are usually done through the GIS (Geographic\nInformation System). We propose here to confront this classical geographical\napproach with statistical approaches: a linear parametric model (polychotomous\nregression modeling) and a nonparametric one (multilayer perceptron). These\nmethodologies have been tested on two real areas on which the land cover is\nknown at various dates; this allows us to emphasize the benefit of these two\nstatistical approaches compared to GIS and to discuss the way GIS could be\nimproved by the use of statistical models.\n", "category": [6, 6]}
{"abstract": "  A function $\\rho:[0,\\infty)\\to(0,1]$ is a completely monotonic function if\nand only if $\\rho(\\Vert\\mathbf{x}\\Vert^2)$ is positive definite on\n$\\mathbb{R}^d$ for all $d$ and thus it represents the correlation function of a\nweakly stationary and isotropic Gaussian random field. Radial positive definite\nfunctions are also of importance as they represent characteristic functions of\nspherically symmetric probability distributions. In this paper, we analyze the\nfunction \\[\\rho(\\beta ,\\gamma)(x)=1-\\biggl(\\frac{x^{\\beta}}{1+x^{\\beta}}\\biggr\n)^{\\gamma},\\qquad x\\ge 0, \\beta,\\gamma>0,\\] called the Dagum function, and show\nthose ranges for which this function is completely monotonic, that is, positive\ndefinite, on any $d$-dimensional Euclidean space. Important relations arise\nwith other families of completely monotonic and logarithmically completely\nmonotonic functions.\n", "category": [2, 6]}
{"abstract": "  The telegraph process models a random motion with finite velocity and it is\nusually proposed as an alternative to diffusion models. The process describes\nthe position of a particle moving on the real line, alternatively with constant\nvelocity $+ v$ or $-v$. The changes of direction are governed by an homogeneous\nPoisson process with rate $\\lambda >0.$ In this paper, we consider a change\npoint estimation problem for the rate of the underlying Poisson process by\nmeans of least squares method. The consistency and the rate of convergence for\nthe change point estimator are obtained and its asymptotic distribution is\nderived. Applications to real data are also presented.\n", "category": [2, 2, 5, 6, 6]}
{"abstract": "  Bivariate linear mixed models are useful when analyzing longitudinal data of\ntwo associated markers. In this paper, we present a bivariate linear mixed\nmodel including random effects or first-order auto-regressive process and\nindependent measurement error for both markers. Codes and tricks to fit these\nmodels using SAS Proc MIXED are provided. Limitations of this program are\ndiscussed and an example in the field of HIV infection is shown. Despite some\nlimitations, SAS Proc MIXED is a useful tool that may be easily extendable to\nmultivariate response in longitudinal studies.\n", "category": [6, 6]}
{"abstract": "  Longitudinal studies could be complicated by left-censored repeated measures.\nFor example, in Human Immunodeficiency Virus infection, there is a detection\nlimit of the assay used to quantify the plasma viral load. Simple imputation of\nthe limit of the detection or of half of this limit for left-censored measures\nbiases estimations and their standard errors. In this paper, we review two\nlikelihood-based methods proposed to handle left-censoring of the outcome in\nlinear mixed model. We show how to fit these models using SAS Proc NLMIXED and\nwe compare this tool with other programs. Indications and limitations of the\nprograms are discussed and an example in the field of HIV infection is shown.\n", "category": [6]}
{"abstract": "  In recent years, several authors have used probabilistic graphical models to\nlearn expression modules and their regulatory programs from gene expression\ndata. Here, we demonstrate the use of the synthetic data generator SynTReN for\nthe purpose of testing and comparing module network learning algorithms. We\nintroduce a software package for learning module networks, called LeMoNe, which\nincorporates a novel strategy for learning regulatory programs. Novelties\ninclude the use of a bottom-up Bayesian hierarchical clustering to construct\nthe regulatory programs, and the use of a conditional entropy measure to assign\nregulators to the regulation program nodes. Using SynTReN data, we test the\nperformance of LeMoNe in a completely controlled situation and assess the\neffect of the methodological changes we made with respect to an existing\nsoftware package, namely Genomica. Additionally, we assess the effect of\nvarious parameters, such as the size of the data set and the amount of noise,\non the inference performance. Overall, application of Genomica and LeMoNe to\nsimulated data sets gave comparable results. However, LeMoNe offers some\nadvantages, one of them being that the learning process is considerably faster\nfor larger data sets. Additionally, we show that the location of the regulators\nin the LeMoNe regulation programs and their conditional entropy may be used to\nprioritize regulators for functional validation, and that the combination of\nthe bottom-up clustering strategy with the conditional entropy-based assignment\nof regulators improves the handling of missing or hidden regulators.\n", "category": [4, 4]}
{"abstract": "  This paper considers the issue of modeling fractional data observed in the\ninterval [0,1), (0,1] or [0,1]. Mixed continuous-discrete distributions are\nproposed. The beta distribution is used to describe the continuous component of\nthe model since its density can have quite diferent shapes depending on the\nvalues of the two parameters that index the distribution. Properties of the\nproposed distributions are examined. Also, maximum likelihood and method of\nmoments estimation is discussed. Finally, practical applications that employ\nreal data are presented.\n", "category": [6]}
{"abstract": "  The classical approach to protein folding inspired by statistical mechanics\navoids the high dimensional structure of the conformation space by using\neffective coordinates. Here we introduce a network approach to capture the\nstatistical properties of the structure of conformation spaces. Conformations\nare represented as nodes of the network, while links are transitions via\nelementary rotations around a chemical bond. Self-avoidance of a polypeptide\nchain introduces degree correlations in the conformation network, which in turn\nlead to energy landscape correlations. Folding can be interpreted as a biased\nrandom walk on the conformation network. We show that the folding pathways\nalong energy gradients organize themselves into scale free networks, thus\nexplaining previous observations made via molecular dynamics simulations. We\nalso show that these energy landscape correlations are essential for recovering\nthe observed connectivity exponent, which belongs to a different universality\nclass than that of random energy models. In addition, we predict that the\nexponent and therefore the structure of the folding network fundamentally\nchanges at high temperatures, as verified by our simulations on the AK peptide.\n", "category": [4, 4]}
{"abstract": "  We have developed a linearization method to investigate the subthreshold\noscillatory behaviors in nonlinear autonomous systems. By considering firstly\nthe neuronal system as an example, we show that this theoretical approach can\npredict quantitatively the subthreshold oscillatory activities, including the\ndamping coefficients and the oscillatory frequencies which are in good\nagreement with those observed in experiments. Then we generalize the\nlinearization method to an arbitrary autonomous nonlinear system. The detailed\nextension of this theoretical approach is also presented and further discussed.\n", "category": [4]}
{"abstract": "  The widespread use of genetic testing in high risk pregnancies has created\nstrong interest in rapid and accurate molecular diagnostics for common\nchromosomal aneuploidies. We show here that digital polymerase chain reaction\n(dPCR) can be used for accurate measurement of trisomy 21 (Down's Syndrome),\nthe most common human aneuploidy. dPCR is generally applicable to any\naneuploidy, does not depend on allelic distribution or gender, and is able to\ndetect signals in the presence of mosaics or contaminating maternal DNA.\n", "category": [4]}
{"abstract": "  We consider a probabilistic cellular automaton to analyze the stochastic\ndynamics of a predator-prey system. The local rules are Markovian and are based\nin the Lotka-Volterra model. The individuals of each species reside on the\nsites of a lattice and interact with an unsymmetrical neighborhood. We look for\nthe effect of the space anisotropy in the characterization of the oscillations\nof the species population densities. Our study of the probabilistic cellular\nautomaton is based on simple and pair mean-field approximations and explicitly\ntakes into account spatial anisotropy.\n", "category": [4]}
{"abstract": "  Employing profits data of Japanese firms in 2003--2005, we kinematically\nexhibit the static log-normal distribution in the middle scale region. In the\nderivation, a Non-Gibrat's law under the detailed balance is adopted together\nwith following two approximations. Firstly, the probability density function of\nprofits growth rate is described as a tent-shaped exponential function.\nSecondly, the value of the origin of the growth rate distribution divided into\nbins is constant. The derivation is confirmed in the database consistently.\n  This static procedure is applied to a quasi-static system. We dynamically\ndescribe a quasi-static log-normal distribution in the middle scale region. In\nthe derivation, a Non-Gibrat's law under the detailed quasi-balance is adopted\ntogether with two approximations confirmed in the static system. The resultant\ndistribution is power-law with varying Pareto index in the large scale region\nand the quasi-static log-normal distribution in the middle scale region. In the\ndistribution, not only the change of Pareto index but also the change of the\nvariance of the log-normal distribution depends on the parameter of the\ndetailed quasi-balance. As a result, Pareto index and the variance of the\nlog-normal distribution are related to each other.\n", "category": [5, 3, 5]}
{"abstract": "  Hayashi and Carthew (Nature 431 [2004], 647) have shown that the packing of\ncone cells in the Drosophila retina resembles soap bubble packing, and that\nchanging E- and N-cadherin expression can change this packing, as well as cell\nshape.\n  The analogy with bubbles suggests that cell packing is driven by surface\nminimization. We find that this assumption is insufficient to model the\nexperimentally observed shapes and packing of the cells based on their cadherin\nexpression. We then consider a model in which adhesion leads to a surface\nincrease, balanced by cell cortex contraction. Using the experimentally\nobserved distributions of E- and N-cadherin, we simulate the packing and cell\nshapes in the wildtype eye. Furthermore, by changing only the corresponding\nparameters, this model can describe the mutants with different numbers of\ncells, or changes in cadherin expression.\n", "category": [4, 4]}
{"abstract": "  Invited contribution to \"Quantum Aspects of Life\", D. Abbott Ed. (World\nScientific, Singapore, 2007).\n", "category": [3, 4]}
{"abstract": "  The purpose of this paper is to study the problem of estimating a compactly\nsupported density of probability from noisy observations of its moments. In\nfact, we provide a statistical approach to the famous Hausdorff classical\nmoment problem. We prove an upper bound and a lower bound on the rate of\nconvergence of the mean squared error showing that the considered estimator\nattains minimax rate over the corresponding smoothness classes.\n", "category": [2, 6]}
{"abstract": "  In this paper we consider two processes driven by diffusions and jumps. The\njump components are Levy processes and they can both have finite activity and\ninfinite activity. Given discrete observations we estimate the covariation\nbetween the two diffusion parts and the co-jumps. The detection of the co-jumps\nallows to gain insight in the dependence structure of the jump components and\nhas important applications in finance. Our estimators are based on a threshold\nprinciple allowing to isolate the jumps. This work follows Gobbi and Mancini\n(2006) where the asymptotic normality for the estimator of the covariation,\nwith convergence speed given by the squared root of h, was obtained when the\njump components have finite activity. Here we show that the speed is the\nsquared root of h only when the activity of the jump components is moderate.\n", "category": [2, 2, 6]}
{"abstract": "  A new class of Marginal Structural Models (MSMs), History-Restricted MSMs\n(HRMSMs), was recently introduced for longitudinal data for the purpose of\ndefining causal parameters which may often be better suited for public health\nresearch or at least more practicable than MSMs \\citejoffe,feldman. HRMSMs\nallow investigators to analyze the causal effect of a treatment on an outcome\nbased on a fixed, shorter and user-specified history of exposure compared to\nMSMs. By default, the latter represent the treatment causal effect of interest\nbased on a treatment history defined by the treatments assigned between the\nstudy's start and outcome collection. We lay out in this article the formal\nstatistical framework behind HRMSMs. Beyond allowing a more flexible causal\nanalysis, HRMSMs improve computational tractability and mitigate statistical\npower concerns when designing longitudinal studies. We also develop three\nconsistent estimators of HRMSM parameters under sufficient model assumptions:\nthe Inverse Probability of Treatment Weighted (IPTW), G-computation and Double\nRobust (DR) estimators. In addition, we show that the assumptions commonly\nadopted for identification and consistent estimation of MSM parameters\n(existence of counterfactuals, consistency, time-ordering and sequential\nrandomization assumptions) also lead to identification and consistent\nestimation of HRMSM parameters.\n", "category": [2, 6, 6]}
{"abstract": "  We develop a pricing rule for life insurance under stochastic mortality in an\nincomplete market by assuming that the insurance company requires compensation\nfor its risk in the form of a pre-specified instantaneous Sharpe ratio. Our\nvaluation formula satisfies a number of desirable properties, many of which it\nshares with the standard deviation premium principle. The major result of the\npaper is that the price per contract solves a linear partial differential\nequation as the number of contracts approaches infinity. One can interpret the\nlimiting price as an expectation with respect to an equivalent martingale\nmeasure. Another important result is that if the hazard rate is stochastic,\nthen the risk-adjusted premium is greater than the net premium, even as the\nnumber of contracts approaches infinity. We present a numerical example to\nillustrate our results, along with the corresponding algorithms.\n", "category": [5, 2, 2]}
{"abstract": "  We develop a theory for pricing non-diversifiable mortality risk in an\nincomplete market. We do this by assuming that the company issuing a\nmortality-contingent claim requires compensation for this risk in the form of a\npre-specified instantaneous Sharpe ratio. We prove that our ensuing valuation\nformula satisfies a number of desirable properties. For example, we show that\nit is subadditive in the number of contracts sold. A key result is that if the\nhazard rate is stochastic, then the risk-adjusted survival probability is\ngreater than the physical survival probability, even as the number of contracts\napproaches infinity.\n", "category": [5, 2, 2]}
{"abstract": "  The present paper introduces an original biofeedback system for improving\nhuman balance control, whose underlying principle consists in providing\nadditional sensory information related to foot sole pressure distribution to\nthe user through a tongue-placed tactile output device. To assess the effect of\nthis biofeedback system on postural control during quiet standing, ten young\nhealthy adults were asked to stand as immobile as possible with their eyes\nclosed in two conditions of No-biofeedback and Biofeedback. Centre of foot\npressure (CoP) displacements were recorded using a force platform. Results\nshowed reduced CoP displacements in the Biofeedback relative to the\nNo-biofeedback condition. The present findings evidenced the ability of the\ncentral nervous system to efficiently integrate an artificial plantar-based,\ntongue-placed tactile biofeedback for controlling control posture during quiet\nstanding.\n", "category": [3, 4]}
{"abstract": "  A dynamical mean-field theory is developed to analyze stochastic single-cell\ndynamics of gene expression. By explicitly taking account of nonequilibrium and\nnonadiabatic features of the DNA state fluctuation, two-time correlation\nfunctions and response functions of single-cell dynamics are derived. The\nmethod is applied to a self-regulating gene to predict a rich variety of\ndynamical phenomena such as anomalous increase of relaxation time and\noscillatory decay of correlations. Effective \"temperature\" defined as the ratio\nof the correlation to the response in the protein number is small when the DNA\nstate change is frequent, while it grows large when the DNA state change is\ninfrequent, indicating the strong enhancement of noise in the latter case.\n", "category": [4, 4]}
{"abstract": "  I propose a path integral description of the Su-Schrieffer-Heeger\nHamiltonian, both in one and two dimensions, after mapping the real space model\nonto the time scale. While the lattice degrees of freedom are classical\nfunctions of time and are integrated out exactly, the electron particle paths\nare treated quantum mechanically. The method accounts for the variable range of\nthe electronic hopping processes. The free energy of the system and its\ntemperature derivatives are computed by summing at any $T$ over the ensemble of\nrelevant particle paths which mainly contribute to the total partition\nfunction. In the low $T$ regime, the {\\it heat capacity over T} ratio shows un\nupturn peculiar to a glass-like behavior. This feature is more sizeable in the\nsquare lattice than in the linear chain as the overall hopping potential\ncontribution to the total action is larger in higher dimensionality. The\neffects of the electron-phonon anharmonic interactions on the phonon subsystem\nare studied by the path integral cumulant expansion method.\n", "category": [3, 3, 6]}
{"abstract": "  We consider the asymptotic behaviour of the solution of one dimensional\nstochastic differential equations and Langevin equations in periodic\nbackgrounds with zero average. We prove that in several such models, there is\ngenerically a non vanishing asymptotic velocity, despite of the fact that the\naverage of the background is zero.\n", "category": [2, 2, 4]}
{"abstract": "  This article reports about a novel extension of dissipative particle dynamics\n(DPD) that allows the study of the collective dynamics of complex chemical and\nstructural systems in a spatially resolved manner with a combinatorially\ncomplex variety of different system constituents. We show that introducing\nmultipolar interactions between particles leads to extended membrane structures\nemerging in a self-organized manner and exhibiting both the necessary\nmechanical stability for transport and fluidity so as to provide a\ntwo-dimensional self-organizing dynamic reaction environment for kinetic\nstudies in the context of cell biology. We further show that the emergent\ndynamics of extended membrane bound objects is in accordance with scaling laws\nimposed by physics.\n", "category": [4, 4]}
{"abstract": "  We propose that genetic encoding of self-assembling components greatly\nenhances the evolution of complex systems and provides an efficient platform\nfor inductive generalization, i.e. the inductive derivation of a solution to a\nproblem with a potentially infinite number of instances from a limited set of\ntest examples. We exemplify this in simulations by evolving scalable circuitry\nfor several problems. One of them, digital multiplication, has been intensively\nstudied in recent years, where hitherto the evolutionary design of only\nspecific small multipliers was achieved. The fact that this and other problems\ncan be solved in full generality employing self-assembly sheds light on the\nevolutionary role of self-assembly in biology and is of relevance for the\ndesign of complex systems in nano- and bionanotechnology.\n", "category": [4, 4]}
{"abstract": "  A basic question of protein structural studies is to which extent mutations\naffect the stability. This question may be addressed starting from sequence\nand/or from structure. In proteomics and genomics studies prediction of protein\nstability free energy change (DDG) upon single point mutation may also help the\nannotation process. The experimental SSG values are affected by uncertainty as\nmeasured by standard deviations. Most of the DDG values are nearly zero (about\n32% of the DDG data set ranges from -0.5 to 0.5 Kcal/mol) and both the value\nand sign of DDG may be either positive or negative for the same mutation\nblurring the relationship among mutations and expected DDG value. In order to\novercome this problem we describe a new predictor that discriminates between 3\nmutation classes: destabilizing mutations (DDG<-0.5 Kcal/mol), stabilizing\nmutations (DDG>0.5 Kcal/mol) and neutral mutations (-0.5<=DDG<=0.5 Kcal/mol).\nIn this paper a support vector machine starting from the protein sequence or\nstructure discriminates between stabilizing, destabilizing and neutral\nmutations. We rank all the possible substitutions according to a three state\nclassification system and show that the overall accuracy of our predictor is as\nhigh as 52% when performed starting from sequence information and 58% when the\nprotein structure is available, with a mean value correlation coefficient of\n0.30 and 0.39, respectively. These values are about 20 points per cent higher\nthan those of a random predictor.\n", "category": [4, 4]}
{"abstract": "  We study a model ecosystem by means of dynamical techniques from disordered\nsystems theory. The model describes a set of species subject to competitive\ninteractions through a background of resources, which they feed upon.\nAdditionally direct competitive or co-operative interaction between species may\noccur through a random coupling matrix. We compute the order parameters of the\nsystem in a fixed point regime, and identify the onset of instability and\ncompute the phase diagram. We focus on the effects of variability of resources,\ndirect interaction between species, co-operation pressure and dilution on the\nstability and the diversity of the ecosystem. It is shown that resources can be\nexploited optimally only in absence of co-operation pressure or direct\ninteraction between species.\n", "category": [4, 3, 3]}
{"abstract": "  Biologists are leading current research on genome characterization\n(sequencing, alignment, transcription), providing a huge quantity of raw data\nabout many genome organisms. Extracting knowledge from this raw data is an\nimportant process for biologists, using usually data mining approaches.\nHowever, it is difficult to deals with these genomic information using actual\nbioinformatics data mining tools, because data are heterogeneous, huge in\nquantity and geographically distributed. In this paper, we present a new\napproach between data mining and virtual reality visualization, called visual\ndata mining. Indeed Virtual Reality becomes ripe, with efficient display\ndevices and intuitive interaction in an immersive context. Moreover, biologists\nuse to work with 3D representation of their molecules, but in a desktop\ncontext. We present a software solution, Genome3DExplorer, which addresses the\nproblem of genomic data visualization, of scene management and interaction.\nThis solution is based on a well-adapted graphical and interaction paradigm,\nwhere local and global topological characteristics of data are easily visible,\non the contrary to traditional genomic database browsers, always focused on the\nzoom and details level.\n", "category": [4]}
{"abstract": "  A major challenge in analyzing animal behavior is to discover some underlying\nsimplicity in complex motor actions. Here we show that the space of shapes\nadopted by the nematode C. elegans is surprisingly low dimensional, with just\nfour dimensions accounting for 95% of the shape variance, and we partially\nreconstruct \"equations of motion\" for the dynamics in this space. These\ndynamics have multiple attractors, and we find that the worm visits these in a\nrapid and almost completely deterministic response to weak thermal stimuli.\nStimulus-dependent correlations among the different modes suggest that one can\ngenerate more reliable behaviors by synchronizing stimuli to the state of the\nworm in shape space. We confirm this prediction, effectively \"steering\" the\nworm in real time.\n", "category": [4]}
{"abstract": "  This article considers the application of particle filtering to\ncontinuous-discrete optimal filtering problems, where the system model is a\nstochastic differential equation, and noisy measurements of the system are\nobtained at discrete instances of time. It is shown how the Girsanov theorem\ncan be used for evaluating the likelihood ratios needed in importance sampling.\nIt is also shown how the methodology can be applied to a class of models, where\nthe driving noise process is lower in the dimensionality than the state and\nthus the laws of state and noise are not absolutely continuous.\nRao-Blackwellization of conditionally Gaussian models and unknown static\nparameter models is also considered.\n", "category": [6, 6]}
{"abstract": "  The self-propelled motion of microscopic bodies immersed in a fluid medium is\nstudied using molecular dynamics simulation. The advantage of the atomistic\napproach is that the detailed level of description allows complete freedom in\nspecifying the swimmer design and its coupling with the surrounding fluid. A\nseries of two-dimensional swimming bodies employing a variety of propulsion\nmechanisms -- motivated by biological and microrobotic designs -- is\ninvestigated, including the use of moving limbs, changing body shapes and fluid\njets. The swimming efficiency and the nature of the induced, time-dependent\nflow fields are found to differ widely among body designs and propulsion\nmechanisms.\n", "category": [3, 4]}
{"abstract": "  A concentration graph associated with a random vector is an undirected graph\nwhere each vertex corresponds to one random variable in the vector. The absence\nof an edge between any pair of vertices (or variables) is equivalent to full\nconditional independence between these two variables given all the other\nvariables. In the multivariate Gaussian case, the absence of an edge\ncorresponds to a zero coefficient in the precision matrix, which is the inverse\nof the covariance matrix. It is well known that this concentration graph\nrepresents some of the conditional independencies in the distribution of the\nassociated random vector. These conditional independencies correspond to the\n\"separations\" or absence of edges in that graph. In this paper we assume that\nthere are no other independencies present in the probability distribution than\nthose represented by the graph. This property is called the perfect\nMarkovianity of the probability distribution with respect to the associated\nconcentration graph. We prove in this paper that this particular concentration\ngraph, the one associated with a perfect Markov distribution, can be determined\nby only conditioning on a limited number of variables. We demonstrate that this\nnumber is equal to the maximum size of the minimal separators in the\nconcentration graph.\n", "category": [2, 6, 6]}
{"abstract": "  Three-dimensional (3D) chromatin structure is closely related to genome\nfunction, in particular transcription. However, the folding path of the\nchromatin fiber in the interphase nucleus is unknown. Here, we systematically\nmeasured the 3D physical distance between pairwise labeled genomic positions in\ngene-dense, highly transcribed domains and gene-poor less active areas on\nchromosomes 1 and 11 in G1 nuclei of human primary fibroblasts, using\nfluorescence in situ hybridization. Interpretation of our results and those\npublished by others, based on polymer physics, shows that the folding of the\nchromatin fiber can be described as a polymer in a globular state (GS),\nmaintained by intra-polymer attractive interactions that counteract\nself-avoidance forces. The GS polymer model is able to describe chromatin\nfolding in as well the highly expressed domains as the lowly expressed ones,\nindicating that they differ in Kuhn length and chromatin compaction. Each type\nof genomic domain constitutes an ensemble of relatively compact globular\nfolding states, resulting in a considerable cellto- cell variation between\notherwise identical cells. We present evidence for different polymer folding\nregimes of the chromatin fiber on the length scale of a few mega base pairs and\non that of complete chromosome arms (several tens of Mb). Our results present a\nnovel view on the folding of the chromatin fiber in interphase and open the\npossibility to explore the nature of the intra-chromatin fiber interactions.\n", "category": [4]}
{"abstract": "  For sample covariance matrices with iid entries with sub-Gaussian tails, when\nboth the number of samples and the number of variables become large and the\nratio approaches to one, it is a well-known result of A. Soshnikov that the\nlimiting distribution of the largest eigenvalue is same as the of Gaussian\nsamples. In this paper, we extend this result to two cases. The first case is\nwhen the ratio approaches to an arbitrary finite value. The second case is when\nthe ratio becomes infinity or arbitrarily small.\n", "category": [2, 2, 6]}
{"abstract": "  We consider estimation procedures which are recursive in the sense that each\nsuccessive estimator is obtained from the previous one by a simple adjustment.\nWe propose a wide class of recursive estimation procedures for the general\nstatistical model and study convergence.\n", "category": [2, 6]}
{"abstract": "  We consider estimation procedures which are recursive in the sense that each\nsuccessive estimator is obtained from the previous one by a simple adjustment.\nWe study rate of convergence of recursive estimation procedures for the general\nstatistical model.\n", "category": [2, 6]}
{"abstract": "  We consider estimation procedures which are recursive in the sense that each\nsuccessive estimator is obtained from the previous one by a simple adjustment.\nThe model considered in the paper is very general as we do not impose any\npreliminary restrictions on the probabilistic nature of the observation process\nand cover a wide class of nonlinear recursive procedures. In this paper we\nstudy asymptotic behaviour of the recursive estimators. The results of the\npaper can be used to determine the form of a recursive procedure which is\nexpected to have the same asymptotic properties as the corresponding\nnon-recursive one defined as a solution of the corresponding estimating\nequation.\n", "category": [2, 6]}
{"abstract": "  The semimartingale stochastic approximation procedure, namely, the\nRobbins-Monro type SDE is introduced which naturally includes both generalized\nstochastic approximation algorithms with martingale noises and recursive\nparameter estimation procedures for statistical models associated with\nsemimartingales. General results concerning the asymptotic behaviour of the\nsolution are presented. In particular, the conditions ensuring the convergence,\nrate of convergence and asymptotic expansion are established. The results\nconcerning the Polyak weighted averaging procedure are also presented.\n", "category": [2, 2, 6]}
{"abstract": "  The prediction of the three-dimensional native structure of proteins from the\nknowledge of their amino acid sequence, known as the protein folding problem,\nis one of the most important yet unsolved issues of modern science. Since the\nconformational behaviour of flexible molecules is nothing more than a complex\nphysical problem, increasingly more physicists are moving into the study of\nprotein systems, bringing with them powerful mathematical and computational\ntools, as well as the sharp intuition and deep images inherent to the physics\ndiscipline. This work attempts to facilitate the first steps of such a\ntransition. In order to achieve this goal, we provide an exhaustive account of\nthe reasons underlying the protein folding problem enormous relevance and\nsummarize the present-day status of the methods aimed to solving it. We also\nprovide an introduction to the particular structure of these biological\nheteropolymers, and we physically define the problem stating the assumptions\nbehind this (commonly implicit) definition. Finally, we review the 'special\nflavor' of statistical mechanics that is typically used to study the\nastronomically large phase spaces of macromolecules. Throughout the whole work,\nmuch material that is found scattered in the literature has been put together\nhere to improve comprehension and to serve as a handy reference.\n", "category": [3, 3, 3, 4]}
{"abstract": "  We present two approaches for linear prediction of long-memory time series.\nThe first approach consists in truncating the Wiener-Kolmogorov predictor by\nrestricting the observations to the last $k$ terms, which are the only\navailable values in practice. We derive the asymptotic behaviour of the\nmean-squared error as $k$ tends to $ + \\infty$. By contrast, the second\napproach is non-parametric. An AR($k$) model is fitted to the long-memory time\nseries and we study the error that arises in this misspecified model.\n", "category": [2, 6]}
{"abstract": "  We employ perturbation analysis technique to study multi-asset portfolio\noptimisation with transaction cost. We allow for correlations in risky assets\nand obtain optimal trading methods for general utility functions. Our\nanalytical results are supported by numerical simulations in the context of the\nLong Term Growth Model.\n", "category": [5, 3, 3]}
{"abstract": "  We investigate the effects of risk perception in a simple model of epidemic\nspreading. We assume that the perception of the risk of being infected depends\non the fraction of neighbors that are ill. The effect of this factor is to\ndecrease the infectivity, that therefore becomes a dynamical component of the\nmodel. We study the problem in the mean-field approximation and by numerical\nsimulations for regular, random and scale-free networks.\n  We show that for homogeneous and random networks, there is always a value of\nperception that stops the epidemics. In the ``worst-case'' scenario of a\nscale-free network with diverging input connectivity, a linear perception\ncannot stop the epidemics; however we show that a non-linear increase of the\nperception risk may lead to the extinction of the disease. This transition is\ndiscontinuous, and is not predicted by the mean-field analysis.\n", "category": [4, 4]}
{"abstract": "  We report a first in modeling and simulation of the effects of the HIV\nproteins on the (caspase dependent) apoptotic pathway in infected cells. This\nwork is novel and is an extension on the recent reports and clarifications on\nthe FAS apoptotic pathway from the literature. We have gathered most of the\nreaction rates and initial conditions from the literature, the rest of the\nconstants have been computed by fitting our model to the experimental results\nreported. Using the model obtained we have then run the simulations for the\ninfected memory T cells, called also latent T cells, which, at the moment,\nrepresent the major obstacle to finding a cure for HIV. We can now report that\nthe infected latent T cells have an estimated lifetime of about 42 hours from\nthe moment they are re-activated. As far as we know this is the first result of\nthis type obtained for the infected memory T cells.\n", "category": [4, 4]}
{"abstract": "  We construct a unifying theory of geometric effects in mesoscopic stochastic\nkinetics. We demonstrate that the adiabatic pump and the reversible ratchet\neffects, as well as similar new phenomena in other domains, such as in\nepidemiology, all follow from geometric phase contributions to the effective\naction in the stochastic path integral representation of the moment generating\nfunction. The theory provides the universal technique for identification,\nprediction and calculation of pump-like phenomena in an arbitrary mesoscopic\nstochastic framework.\n", "category": [3, 4]}
{"abstract": "  Random networks with specified degree distributions have been proposed as\nrealistic models of population structure, yet the problem of dynamically\nmodeling SIR-type epidemics in random networks remains complex. I resolve this\ndilemma by showing how the SIR dynamics can be modeled with a system of three\nnonlinear ODE's. The method makes use of the probability generating function\n(PGF) formalism for representing the degree distribution of a random network\nand makes use of network-centric quantities such as the number of edges in a\nwell-defined category rather than node-centric quantities such as the number of\ninfecteds or susceptibles. The PGF provides a simple means of translating\nbetween network and node-centric variables and determining the epidemic\nincidence at any time. The theory also provides a simple means of tracking the\nevolution of the degree distribution among susceptibles or infecteds. The\nequations are used to demonstrate the dramatic effects that the degree\ndistribution plays on the final size of an epidemic as well as the speed with\nwhich it spreads through the population. Power law degree distributions are\nobserved to generate an almost immediate expansion phase yet have a smaller\nfinal size compared to homogeneous degree distributions such as the Poisson.\nThe equations are compared to stochastic simulations, which show good agreement\nwith the theory. Finally, the dynamic equations provide an alternative way of\ndetermining the epidemic threshold where large-scale epidemics are expected to\noccur, and below which epidemic behavior is limited to finite-sized outbreaks.\n", "category": [4, 4]}
{"abstract": "  In Biology, all motor enzymes operate on the same principle: they trap\nfavourable brownian fluctuations in order to generate directed forces and to\nmove. Whether it is possible or not to copy one such strategy to play the\nmarket was the starting point of our investigations. We found the answer is\nyes. In this paper we describe one such strategy and appraise its performance\nwith historical data from the European Monetary System (EMS), the US Dow Jones,\nthe german Dax and the french Cac40.\n", "category": [5, 3, 3]}
{"abstract": "  Generalisation of the El Farol bar problem to that of many bars here leads to\nthe Kolkata restaurant problem, where the decision to go to any restaurant or\nnot is much simpler (depending on the previous experience of course, as in the\nEl Farol bar problem). This generalised problem can be exactly analysed in some\nlimiting cases discussed here. The fluctuation in the restaurant service can be\nshown to have precisely an inverse cubic behavior, as widely seen in the stock\nmarket fluctuations.\n", "category": [3, 5]}
{"abstract": "  Contact patterns in populations fundamentally influence the spread of\ninfectious diseases. Current mathematical methods for epidemiological\nforecasting on networks largely assume that contacts between individuals are\nfixed, at least for the duration of an outbreak. In reality, contact patterns\nmay be quite fluid, with individuals frequently making and breaking social or\nsexual relationships. Here we develop a mathematical approach to predicting\ndisease transmission on dynamic networks in which each individual has a\ncharacteristic behavior (typical contact number), but the identities of their\ncontacts change in time. We show that dynamic contact patterns shape\nepidemiological dynamics in ways that cannot be adequately captured in static\nnetwork models or mass-action models. Our new model interpolates smoothly\nbetween static network models and mass-action models using a mixing parameter,\nthereby providing a bridge between disparate classes of epidemiological models.\nUsing epidemiological and sexual contact data from an Atlanta high school, we\nthen demonstrate the utility of this method for forecasting and controlling\nsexually transmitted disease outbreaks.\n", "category": [4, 4]}
{"abstract": "  In this paper, we investigate a numerical algorithm for the pricing of swing\noptions, relying on the so-called optimal quantization method. The numerical\nprocedure is described in details and numerous simulations are provided to\nassert its efficiency. In particular, we carry out a comparison with the\nLongstaff-Schwartz algorithm.\n", "category": [5, 2]}
{"abstract": "  We analytically and numerically study the probabilistic properties of\ninverted and mirror repeats in model sequences of nucleic acids. We consider\nboth perfect and non-perfect repeats, i.e. repeats with mismatches and gaps.\nThe considered sequence models are independent identically distributed (i.i.d.)\nsequences, Markov processes and long range sequences. We show that the number\nof repeats in correlated sequences is significantly larger than in i.i.d.\nsequences and that this discrepancy increases exponentially with the repeat\nlength for long range sequences.\n", "category": [4, 4]}
{"abstract": "  The problem of large-scale simultaneous hypothesis testing is re-visited.\nBagging and subagging procedures are put forth with the purpose of improving\nthe discovery power of the tests. The procedures are implemented in both\nsimulated and real data. It is shown that bagging and subagging significantly\nimprove power at the cost of a small increase in false discovery rate with the\nproposed `maximum contrast' subagging having an edge over bagging, i.e.,\nyielding similar power but significantly smaller false discovery rates.\n", "category": [6, 6]}
{"abstract": "  We present a model improving the two-angle model for interphase chromatin\n(E2A model). This model takes into account the cylindrical shape of the histone\noctamers, the H1 histones in front of the nucleosomes and the vertical distance\n$d$ between the in and outgoing DNA strands. Factoring these chromatin features\nin, one gets essential changes in the chromatin phase diagram: Not only the\nshape of the excluded-volume borderline changes but also the vertical distance\n$d$ has a dramatic influence on the forbidden area. Furthermore, we examined\nthe influence of H1 defects on the properties of the chromatin fiber. Thus we\npresent two possible strategies for chromatin compaction: The use of very dense\nstates in the phase diagram in the gaps in the excluded volume borderline or\nmissing H1 histones which can lead to very compact fibers. The chromatin fiber\nmight use both of these mechanisms to compact itself at least locally. Line\ndensities computed within the model coincident with the experimental values.\n", "category": [3, 3, 4]}
{"abstract": "  We present a particle filter construction for a system that exhibits\ntime-scale separation. The separation of time-scales allows two simplifications\nthat we exploit: i) The use of the averaging principle for the dimensional\nreduction of the system needed to solve for each particle and ii) the\nfactorization of the transition probability which allows the\nRao-Blackwellization of the filtering step. Both simplifications can be\nimplemented using the coarse projective integration framework. The resulting\nparticle filter is faster and has smaller variance than the particle filter\nbased on the original system. The method is tested on a multiscale stochastic\ndifferential equation and on a multiscale pure jump diffusion motivated by\nchemical reactions.\n", "category": [2, 2, 6, 6]}
{"abstract": "  We suggest a simple deterministic approximation for the growth of the\nfavoured-allele frequency during a selective sweep. Using this approximation we\nintroduce an accurate model for genetic hitch-hiking. Only when Ns < 10 (N is\nthe population size and s denotes the selection coefficient), are discrepancies\nbetween our approximation and direct numerical simulations of a Moran model\nnoticeable. Our model describes the gene genealogies of a contiguous segment of\nneutral loci close to the selected one, and it does not assume that the\nselective sweep happens instantaneously. This enables us to compute SNP\ndistributions on the neutral segment without bias.\n", "category": [4]}
{"abstract": "  Both short interfering RNAs (siRNAs) and microRNAs (miRNAs) mediate the\nrepression of specific sequences of mRNA through the RNA interference pathway.\nIn the last years several experiments have supported the hypothesis that siRNAs\nand miRNAs may be functionally interchangeable, at least in cultured cells. In\nthis work we verify that this hypothesis is also supported by a computational\nevidence. We show that a method specifically trained to predict the activity of\nthe exogenous siRNAs assigns a high silencing level to experimentally\ndetermined human miRNAs. This result not only supports the idea of siRNAs and\nmiRNAs equivalence but indicates that it is possible to use computational tools\ndeveloped using synthetic small interference RNAs to investigate endogenous\nmiRNAs.\n", "category": [4, 4]}
{"abstract": "  We consider the problem of binary classification where one can, for a\nparticular cost, choose not to classify an observation. We present a simple\nproof for the oracle inequality for the excess risk of structural risk\nminimizers using a lasso type penalty.\n", "category": [6]}
{"abstract": "  In this paper, we present a method to optimise rough set partition sizes, to\nwhich rule extraction is performed on HIV data. The genetic algorithm\noptimisation technique is used to determine the partition sizes of a rough set\nin order to maximise the rough sets prediction accuracy. The proposed method is\ntested on a set of demographic properties of individuals obtained from the\nSouth African antenatal survey. Six demographic variables were used in the\nanalysis, these variables are; race, age of mother, education, gravidity,\nparity, and age of father, with the outcome or decision being either HIV\npositive or negative. Rough set theory is chosen based on the fact that it is\neasy to interpret the extracted rules. The prediction accuracy of equal width\nbin partitioning is 57.7% while the accuracy achieved after optimising the\npartitions is 72.8%. Several other methods have been used to analyse the HIV\ndata and their results are stated and compared to that of rough set theory\n(RST).\n", "category": [0, 0, 4]}
{"abstract": "  A synfire chain is a simple neural network model which can propagate stable\nsynchronous spikes called a pulse packet and widely researched. However how\nsynfire chains coexist in one network remains to be elucidated. We have studied\nthe activity of a layered associative network of Leaky Integrate-and-Fire\nneurons in which connection we embed memory patterns by the Hebbian Learning.\nWe analyzed their activity by the Fokker-Planck method. In our previous report,\nwhen a half of neurons belongs to each memory pattern (memory pattern rate\n$F=0.5$), the temporal profiles of the network activity is split into\ntemporally clustered groups called sublattices under certain input conditions.\nIn this study, we show that when the network is sparsely connected ($F<0.5$),\nsynchronous firings of the memory pattern are promoted. On the contrary, the\ndensely connected network ($F>0.5$) inhibit synchronous firings. The sparseness\nand denseness also effect the basin of attraction and the storage capacity of\nthe embedded memory patterns. We show that the sparsely(densely) connected\nnetworks enlarge(shrink) the basion of attraction and increase(decrease) the\nstorage capacity.\n", "category": [4]}
{"abstract": "  Analyzing nonlinear conformational relaxation dynamics in elastic networks\ncorresponding to two classical motor proteins, we find that they respond by\nwell-defined internal mechanical motions to various initial deformations and\nthat these motions are robust against external perturbations. We show that this\nbehavior is not characteristic for random elastic networks. However, special\nnetwork architectures with such properties can be designed by evolutionary\noptimization methods. Using them, an example of an artificial elastic network,\noperating as a cyclic machine powered by ligand binding, is constructed.\n", "category": [4, 3, 3]}
{"abstract": "  This paper compares the Maximum-likelihood method and Bayesian method for\nfinite element model updating. The Maximum-likelihood method was implemented\nusing genetic algorithm while the Bayesian method was implemented using the\nMarkov Chain Monte Carlo. These methods were tested on a simple beam and an\nunsymmetrical H-shaped structure. The results show that the Bayesian method\ngave updated finite element models that predicted more accurate modal\nproperties than the updated finite element models obtained through the use of\nthe Maximum-likelihood method. Furthermore, both these methods were found to\nrequire the same levels of computational loads.\n", "category": [6]}
{"abstract": "  Let \\Theta be a smooth compact oriented manifold without boundary, embedded\nin a euclidean space and let \\gamma be a smooth map \\Theta into a riemannian\nmanifold \\Lambda. An unknown state \\theta \\in \\Theta is observed via\nX=\\theta+\\epsilon \\xi where \\epsilon>0 is a small parameter and \\xi is a white\nGaussian noise. For a given smooth prior on \\Theta and smooth estimator g of\nthe map \\gamma we derive a second-order asymptotic expansion for the related\nBayesian risk. The calculation involves the geometry of the underlying spaces\n\\Theta and \\Lambda, in particular, the integration-by-parts formula. Using this\nresult, a second-order minimax estimator of \\gamma is found based on the modern\ntheory of harmonic maps and hypo-elliptic differential operators.\n", "category": [2, 6]}
{"abstract": "  Many systems of different nature exhibit scale free behaviors. Economic\nsystems with power law distribution in the wealth is one of the examples. To\nbetter understand the working behind the complexity, we undertook an empirical\nstudy measuring the interactions between market participants. A Web server was\nsetup to administer the exchange of futures contracts whose liquidation prices\nwere coupled to event outcomes. After free registration, participants started\ntrading to compete for the money prizes upon maturity of the futures contracts\nat the end of the experiment. The evolving `cash' flow network was\nreconstructed from the transactions between players. We show that the network\ntopology is hierarchical, disassortative and scale-free with a power law\nexponent of 1.02+-0.09 in the degree distribution. The small-world property\nemerged early in the experiment while the number of participants was still\nsmall. We also show power law distributions of the net incomes and\ninter-transaction time intervals. Big winners and losers are associated with\nhigh degree, high betweenness centrality, low clustering coefficient and low\ndegree-correlation. We identify communities in the network as groups of the\nlike-minded. The distribution of the community sizes is shown to be power-law\ndistributed with an exponent of 1.19+-0.16.\n", "category": [5, 3]}
{"abstract": "  Molecular spiders are synthetic bio-molecular systems which have \"legs\" made\nof short single-stranded segments of DNA. Spiders move on a surface covered\nwith single-stranded DNA segments complementary to legs. Different mappings are\nestablished between various models of spiders and simple exclusion processes.\nFor spiders with simple gait and varying number of legs we compute the\ndiffusion coefficient; when the hopping is biased we also compute their\nvelocity.\n", "category": [3, 2, 4]}
{"abstract": "  Synthetic bio-molecular spiders with \"legs\" made of single-stranded segments\nof DNA can move on a surface which is also covered by single-stranded segments\nof DNA complementary to the leg DNA. In experimental realizations, when a leg\ndetaches from a segment of the surface for the first time it alters that\nsegment, and legs subsequently bound to these altered segments more weakly.\nInspired by these experiments we investigate spiders moving along a\none-dimensional substrate, whose legs leave newly visited sites at a slower\nrate than revisited sites. For a random walk (one-leg spider) the slowdown does\nnot effect the long time behavior. For a bipedal spider, however, the slowdown\ngenerates an effective bias towards unvisited sites, and the spider behaves\nsimilarly to the excited walk. Surprisingly, the slowing down of the spider at\nnew sites increases the diffusion coefficient and accelerates the growth of the\nnumber of visited sites.\n", "category": [3, 2, 4]}
{"abstract": "  We present a mathematically justifiable, computationally simple, sample\neigenvalue based procedure for estimating the number of high-dimensional\nsignals in white noise using relatively few samples. The main motivation for\nconsidering a sample eigenvalue based scheme is the computational simplicity\nand the robustness to eigenvector modelling errors which are can adversely\nimpact the performance of estimators that exploit information in the sample\neigenvectors.\n  There is, however, a price we pay by discarding the information in the sample\neigenvectors; we highlight a fundamental asymptotic limit of sample eigenvalue\nbased detection of weak/closely spaced high-dimensional signals from a limited\nsample size. This motivates our heuristic definition of the effective number of\nidentifiable signals which is equal to the number of \"signal\" eigenvalues of\nthe population covariance matrix which exceed the noise variance by a factor\nstrictly greater than 1+sqrt(Dimensionality of the system/Sample size). The\nfundamental asymptotic limit brings into sharp focus why, when there are too\nfew samples available so that the effective number of signals is less than the\nactual number of signals, underestimation of the model order is unavoidable (in\nan asymptotic sense) when using any sample eigenvalue based detection scheme,\nincluding the one proposed herein. The analysis reveals why adding more sensors\ncan only exacerbate the situation. Numerical simulations are used to\ndemonstrate that the proposed estimator consistently estimates the true number\nof signals in the dimension fixed, large sample size limit and the effective\nnumber of identifiable signals in the large dimension, large sample size limit.\n", "category": [2, 6]}
{"abstract": "  It has been shown that differences in fecundity variance can influence the\nprobability of invasion of a genotype in a population, i.e. a genotype with\nlower variance in offspring number can be favored in finite populations even if\nit has a somewhat lower mean fitness than a competitor. In this paper,\nGillespie's results are extended to population genetic systems with explicit\nage structure, where the demographic variance (variance in growth rate)\ncalculated in the work of Engen and colleagues is used as a generalization of\n\"variance in offspring number\" to predict the interaction between deterministic\nand random forces driving change in allele frequency. By calculating the\nvariance from the life history parameters, it is shown that selection against\nvariance in the growth rate will favor a genotypes with lower stochasticity in\nage specific survival and fertility rates. A diffusion approximation for\nselection and drift in a population with two genotypes with different life\nhistory matrices (and therefore, different growth rates and demographic\nvariances) is derived and shown to be consistent with individual based\nsimulations. It is also argued that for finite populations, perturbation\nanalyses of both the growth rate and demographic variances may be necessary to\ndetermine the sensitivity of \"fitness\" (broadly defined) to changes in the life\nhistory parameters.\n", "category": [4]}
{"abstract": "  Motivation: Similarity-measure based clustering is a crucial problem\nappearing throughout scientific data analysis. Recently, a powerful new\nalgorithm called Affinity Propagation (AP) based on message-passing techniques\nwas proposed by Frey and Dueck \\cite{Frey07}. In AP, each cluster is identified\nby a common exemplar all other data points of the same cluster refer to, and\nexemplars have to refer to themselves. Albeit its proved power, AP in its\npresent form suffers from a number of drawbacks. The hard constraint of having\nexactly one exemplar per cluster restricts AP to classes of regularly shaped\nclusters, and leads to suboptimal performance, {\\it e.g.}, in analyzing gene\nexpression data. Results: This limitation can be overcome by relaxing the AP\nhard constraints. A new parameter controls the importance of the constraints\ncompared to the aim of maximizing the overall similarity, and allows to\ninterpolate between the simple case where each data point selects its closest\nneighbor as an exemplar and the original AP. The resulting soft-constraint\naffinity propagation (SCAP) becomes more informative, accurate and leads to\nmore stable clustering. Even though a new {\\it a priori} free-parameter is\nintroduced, the overall dependence of the algorithm on external tuning is\nreduced, as robustness is increased and an optimal strategy for parameter\nselection emerges more naturally. SCAP is tested on biological benchmark data,\nincluding in particular microarray data related to various cancer types. We\nshow that the algorithm efficiently unveils the hierarchical cluster structure\npresent in the data sets. Further on, it allows to extract sparse gene\nexpression signatures for each cluster.\n", "category": [4, 3, 3]}
{"abstract": "  We consider processes with second order long range dependence resulting from\nheavy tailed durations. We refer to this phenomenon as duration-driven long\nrange dependence (DDLRD), as opposed to the more widely studied linear long\nrange dependence based on fractional differencing of an $iid$ process. We\nconsider in detail two specific processes having DDLRD, originally presented in\nTaqqu and Levy (1986), and Parke (1999). For these processes, we obtain the\nlimiting distribution of suitably standardized discrete Fourier transforms\n(DFTs) and sample autocovariances. At low frequencies, the standardized DFTs\nconverge to a stable law, as do the standardized sample autocovariances at\nfixed lags. Finite collections of standardized sample autocovariances at a\nfixed set of lags converge to a degenerate distribution. The standardized DFTs\nat high frequencies converge to a Gaussian law. Our asymptotic results are\nstrikingly similar for the two DDLRD processes studied. We calibrate our\nasymptotic results with a simulation study which also investigates the\nproperties of the semiparametric log periodogram regression estimator of the\nmemory parameter.\n", "category": [2, 6]}
{"abstract": "  With an automatic image analysis device, we studied the temporal distribution\nof the locomotor activity of E. orientalis and E. vuilleti during 24 h, and\nover several days to know whether the activity rhythms of these two Eupelmidae\nplay a role in their competitive interactions. The analysis of locomotor\nactivity rhythms of E. orientalis and E. vuilleti shows that the locomotor\nactivity of both species presents daily cyclic variations. These two Eupelmidae\nhave similar activity rhythms. Displacements of these parasitoids essentially\ntake place during the photophase. But the activity of E. vuilleti is earlier,\nbecause the individuals of this species start their activity on average 4 to 5\nh earlier than those of E. orientalis. E. vuilleti begins its displacements\nseveral hours before the onset of lighting, whereas E. orientalis is active\nonly in the presence of the light. This shift of starting activity is thus a\nfactor allowing these concurrent species to minimize their interactions during\nthe cohabitation period in traditional granaries after the harvests of cowpea.\n", "category": [4]}
{"abstract": "  There are many processes in biology in which mechanical forces are generated.\nForce-bearing networks can transduce locally developed mechanical signals very\nextensively over different parts of the cell or tissues. In this article we\nconduct an overview of this kind of mechanical transduction, focusing in\nparticular on the multiple layers of complexity displayed by the mechanisms\nthat control and trigger the conversion of a mechanical signal into a\nbiochemical function. Single molecule methodologies, through their capability\nto introduce the force in studies of biological processes in which mechanical\nstresses are developed, are unveiling subtle intertwining mechanisms between\nchemistry and mechanics and in particular are revealing how chemistry can\ncontrol mechanics. The possibility that chemistry interplays with mechanics\nshould be always considered in biochemical studies.\n", "category": [4, 4]}
{"abstract": "  In Chile and Uruguay,the gregarious Pteromalidae (Monoksa dorsiplana) has\nbeen discovered emerging from seeds of the persistent pods of Acacia caven\nattacked by the univoltin bruchid Pseudopachymeria spinipes. We investigated\nthe potential for mass rearing of this gregarious ectoparasitoid on an\nalternative bruchid host, Callosobruchus maculatus, to use it against the\nbruchidae of native and cultured species of Leguminosea seeds in South America.\nThe mass rearing of M.dorsiplana was carried out in a population cage where the\ndensity of egg-laying females per infested seed was increased from 1:1 on the\nfirst day to 5:1 on the last (fifth) day. Under these experimental conditions\negg-clutch size per host increased, and at the same time the mortality of eggs\nlaid also increased. The density of egg-laying females influenced the sex ratio\nwhich tended towards a balance of sons and daughters,in contrast to the sex\nratio of a single egg-laying female per host (1 son to 7 daughters). The mean\nweight of adults emerging from a parasitized host was negatively correlated\nwith the egg-clutch size, i.e., as egg-clutch size increased, adult weight\ndecreased. All these results show that mass rearing of the gregarious\nectoparasitoid M.dorsiplana was possible under laboratory conditions on an\nalternative bruchid host C.maculatus. As M.dorsiplana is a natural enemy of\nlarval and pupal stages of bruchidae, the next step was to investigate whether\nthe biological control of bruchid C.maculatus was possible in an experimental\nstructure of stored beans.\n", "category": [4]}
{"abstract": "  A phase field model for dealing with shape instabilities in fluid membrane\nvesicles is presented. This model takes into account the Canham-Helfrich\nbending energy with spontaneous curvature. A dynamic equation for the\nphase-field is also derived. With this model it is possible to see the vesicle\nshape deformation dynamically, when some external agent instabilizes the\nmembrane, for instance, inducing an inhomogeneous spontaneous curvature. The\nnumerical scheme used is detailed and some stationary shapes are shown together\nwith a shape diagram for vesicles of spherical topology and no spontaneous\ncurvature, in agreement with known results.\n", "category": [3, 4]}
{"abstract": "  A mathematical model of infiltrative tumour growth taking into account cell\nproliferation, death and motility is considered. The model is formulated in\nterms of local cell density and nutrient (oxygen) concentration. In the model\nthe rate of cell death depends on the local nutrient level. Thus heterogeneous\nnutrient distribution in tissue affects tumour structure and development. The\nexistence of automodel solutions is demonstrated and their properties are\ninvestigated. The results are compared to the properties of the\nKolmogorov-Petrovskii-Piskunov and Fisher equations. Influence of the nutrient\ndistribution on the autowave speed selection as well as on the relaxation to\nautomodel solution is demonstrated. The model adequately describes the data,\nobserved in experiments.\n", "category": [4, 3]}
{"abstract": "  The stability of the dynamical states characterized by a uniform firing rate\n({\\it splay states}) is analyzed in a network of globally coupled leaky\nintegrate-and-fire neurons. This is done by reducing the set of differential\nequations to a map that is investigated in the limit of large network size. We\nshow that the stability of the splay state depends crucially on the ratio\nbetween the pulse--width and the inter-spike interval. More precisely, the\nspectrum of Floquet exponents turns out to consist of three components: (i) one\nthat coincides with the predictions of the mean-field analysis [Abbott-van\nVreesvijk, 1993]; (ii) a component measuring the instability of\n\"finite-frequency\" modes; (iii) a number of \"isolated\" eigenvalues that are\nconnected to the characteristics of the single pulse and may give rise to\nstrong instabilities (the Floquet exponent being proportional to the network\nsize). Finally, as a side result, we find that the splay state can be stable\neven for inhibitory coupling.\n", "category": [3, 4]}
{"abstract": "  The metabolic networks are very well characterized for a large set of\norganisms, a unique case in within the large-scale biological networks. For\nthis reason they provide a a very interesting framework for the construction of\nanalytically tractable statistical mechanics models.\n  In this paper we introduce a solvable model for the distribution of fluxes in\nthe metabolic network. We show that the effect of the topology on the\ndistribution of fluxes is to allow for large fluctuations of their values, a\nfact that should have implications on the robustness of the system.\n", "category": [4]}
{"abstract": "  The author proposes a finance trading strategy named Entropy Oriented Trading\nand apply thermodynamics on the strategy. The state variables are chosen so\nthat the strategy satisfies the second law of thermodynamics. Using the law,\nthe author proves that the rate of investment (ROI) of the strategy is equal to\nor more than the rate of price change.\n", "category": [5, 3, 3]}
{"abstract": "  We investigate the mechanisms of histone sliding and detachment with a\nstochastic model that couples thermally-induced, passive histone sliding with\nactive motor-driven histone unwrapping. Analysis of a passive loop or twist\ndefect-mediated histone sliding mechanism shows that diffusional sliding is\nenhanced as larger portions of the DNA is peeled off the histone. The mean\ntimes to histone detachment and the mean distance traveled by the motor complex\nprior to histone detachment are computed as functions of the intrinsic speed of\nthe motor. Fast motors preferentially induce detachment over sliding. However,\nfor a fixed motor speed, increasing the histone-DNA affinity (and thereby\ndecreasing the passive sliding rate) increases the mean distance traveled by\nthe motor.\n", "category": [4, 4]}
{"abstract": "  Background: Information processing in the brain requires large amounts of\nmetabolic energy, the spatial distribution of which is highly heterogeneous\nreflecting complex activity patterns in the mammalian brain.\n  Results: Here, it is found based on empirical data that, despite this\nheterogeneity, the volume-specific cerebral glucose metabolic rate of many\ndifferent brain structures scales with brain volume with almost the same\nexponent around -0.15. The exception is white matter, the metabolism of which\nseems to scale with a standard specific exponent -1/4. The scaling exponents\nfor the total oxygen and glucose consumptions in the brain in relation to its\nvolume are identical and equal to $0.86\\pm 0.03$, which is significantly larger\nthan the exponents 3/4 and 2/3 suggested for whole body basal metabolism on\nbody mass.\n  Conclusions: These findings show explicitly that in mammals (i)\nvolume-specific scaling exponents of the cerebral energy expenditure in\ndifferent brain parts are approximately constant (except brain stem\nstructures), and (ii) the total cerebral metabolic exponent against brain\nvolume is greater than the much-cited Kleiber's 3/4 exponent. The\nneurophysiological factors that might account for the regional uniformity of\nthe exponents and for the excessive scaling of the total brain metabolism are\ndiscussed, along with the relationship between brain metabolic scaling and\ncomputation.\n", "category": [4, 4]}
{"abstract": "  Using predictive adaptive arithmetic coding and the Minimum Description\nLength principle, we derive an efficient tool for model selection problems :\nthe RIC information criterion. We then present an extension of these coding\ntechniques to non-parametrical estimation of a distribution and illustrate it\non the gray scales histogram of an image.\n  Key-words : Information criteria, MDL, model selection, non-parametrical\nestimation, histograms.\n", "category": [6]}
{"abstract": "  We consider triangular arrays of Markov chains that converge weakly to a\ndiffusion process. Second order Edgeworth type expansions for transition\ndensities are proved. The paper differs from recent results in two respects. We\nallow nonhomogeneous diffusion limits and we treat transition densities with\ntime lag converging to zero. Small time asymptotics are motivated by\nstatistical applications and by resulting approximations for the joint density\nof diffusion values at an increasing grid of points.\n", "category": [2, 2, 6]}
{"abstract": "  This paper presents a stability test for a class of interconnected nonlinear\nsystems motivated by biochemical reaction networks. One of the main results\ndetermines global asymptotic stability of the network from the diagonal\nstability of a \"dissipativity matrix\" which incorporates information about the\npassivity properties of the subsystems, the interconnection structure of the\nnetwork, and the signs of the interconnection terms. This stability test\nencompasses the \"secant criterion\" for cyclic networks presented in our\nprevious paper, and extends it to a general interconnection structure\nrepresented by a graph. A second main result allows one to accommodate state\nproducts. This extension makes the new stability criterion applicable to a\nbroader class of models, even in the case of cyclic systems. The new stability\ntest is illustrated on a mitogen activated protein kinase (MAPK) cascade model,\nand on a branched interconnection structure motivated by metabolic networks.\nFinally, another result addresses the robustness of stability in the presence\nof diffusion terms in a compartmental system made out of identical systems.\n", "category": [4]}
{"abstract": "  The behavior of interacting populations typically displays irregular temporal\nand spatial patterns that are difficult to reconcile with an underlying\ndeterministic dynamics. A classical example is the heterogeneous distribution\nof plankton communities, which has been observed to be patchy over a wide range\nof spatial and temporal scales. Here, we use plankton communities as prototype\nsystems to present theoretical approaches for the analysis of the combined\neffects of turbulent advection and stochastic growth in the spatiotemporal\ndynamics of the population. Incorporation of these two factors into\nmathematical models brings an extra level of realism to the description and\nleads to better agreement with experimental data than that of previously\nproposed models based on reaction-diffusion equations.\n", "category": [4, 3]}
{"abstract": "  The ARCH process (R. F. Engle, 1982) constitutes a paradigmatic generator of\nstochastic time series with time-dependent variance like it appears on a wide\nbroad of systems besides economics in which ARCH was born. Although the ARCH\nprocess captures the so-called \"volatility clustering\" and the asymptotic\npower-law probability density distribution of the random variable, it is not\ncapable to reproduce further statistical properties of many of these time\nseries such as: the strong persistence of the instantaneous variance\ncharacterised by large values of the Hurst exponent (H > 0.8), and asymptotic\npower-law decay of the absolute values self-correlation function. By means of\nconsidering an effective return obtained from a correlation of past returns\nthat has a q-exponential form we are able to fix the limitations of the\noriginal model. Moreover, this improvement can be obtained through the correct\nchoice of a sole additional parameter, $q_{m}$. The assessment of its validity\nand usefulness is made by mimicking daily fluctuations of SP500 financial\nindex.\n", "category": [3, 5]}
{"abstract": "  The equilibrium free energy landscape of an off-lattice model protein as a\nfunction of an internal (reaction) coordinate is reconstructed from\nout-of-equilibrium mechanical unfolding manipulations. This task is\naccomplished via two independent methods: by employing an extended version of\nthe Jarzynski equality (EJE) and the protein inherent structures (ISs). In a\nrange of temperatures around the ``folding transition'' we find a good\nquantitative agreement between the free energies obtained via EJE and IS\napproaches. This indicates that the two methodologies are consistent and able\nto reproduce equilibrium properties of the examined system. Moreover, for the\nstudied model the structural transitions induced by pulling can be related to\nthermodynamical aspects of folding.\n", "category": [3, 4]}
{"abstract": "  We present a quantitative analysis of throwing ability for major league\noutfielders and catchers. We use detailed game event data to tabulate success\nand failure events in outfielder and catcher throwing opportunities. We\nattribute a run contribution to each success or failure which are tabulated for\neach player in each season. We use four seasons of data to estimate the overall\nthrowing ability of each player using a Bayesian hierarchical model. This model\nallows us to shrink individual player estimates towards an overall population\nmean depending on the number of opportunities for each player. We use the\nposterior distribution of player abilities from this model to identify players\nwith significant positive and negative throwing contributions.\n", "category": [6]}
{"abstract": "  This paper studies oracle properties of $\\ell_1$-penalized least squares in\nnonparametric regression setting with random design. We show that the penalized\nleast squares estimator satisfies sparsity oracle inequalities, i.e., bounds in\nterms of the number of non-zero components of the oracle vector. The results\nare valid even when the dimension of the model is (much) larger than the sample\nsize and the regression matrix is not positive definite. They can be applied to\nhigh-dimensional linear regression, to nonparametric adaptive regression\nestimation and to the problem of aggregation of arbitrary estimators.\n", "category": [2, 6]}
{"abstract": "  Anchoring is a term used in psychology to describe the common human tendency\nto rely too heavily (anchor) on one piece of information when making decisions.\nA trading algorithm inspired by biological motors, introduced by L.\nGil\\cite{Gil}, is suggested as a testing ground for anchoring in financial\nmarkets. An exact solution of the algorithm is presented for arbitrary price\ndistributions. Furthermore the algorithm is extended to cover the case of a\nmarket neutral portfolio, revealing additional evidence that anchoring is\ninvolved in the decision making of market participants. The exposure of\narbitrage possibilities created by anchoring gives yet another illustration on\nthe difficulty proving market efficiency by only considering lower order\ncorrelations in past price time series\n", "category": [5, 3]}
{"abstract": "  From the spectral plot of the (normalized) graph Laplacian, the essential\nqualitative properties of a network can be simultaneously deduced. Given a\nclass of empirical networks, reconstruction schemes for elucidating the\nevolutionary dynamics leading to those particular data can then be developed.\nThis method is exemplified for protein-protein interaction networks. Traces of\ntheir evolutionary history of duplication and divergence processes are\nidentified. In particular, we can identify typical specific features that\nrobustly distinguish protein-protein interaction networks from other classes of\nnetworks, in spite of possible statistical fluctuations of the underlying data.\n", "category": [4, 3, 4]}
{"abstract": "  This paper presents a model of the dynamics of the wage income distribution.\n", "category": [5, 3, 3]}
{"abstract": "  Mechanistic home range models are important tools in modeling animal dynamics\nin spatially-complex environments. We introduce a class of stochastic models\nfor animal movement in a habitat of varying preference. Such models interpolate\nbetween spatially-implicit resource selection analysis (RSA) and\nadvection-diffusion models, possessing these two models as limiting cases. We\nfind a closed-form solution for the steady-state (equilibrium) probability\ndistribution u* using a factorization of the redistribution operator into\nsymmetric and diagonal parts. How space use is controlled by the preference\nfunction w then depends on the characteristic width of the redistribution\nkernel: when w changes rapidly compared to this width, u* ~ w, whereas on\nglobal scales large compared to this width, u* ~ w^2. We analyse the behavior\nat discontinuities in w which occur at habitat type boundaries. We simulate the\ndynamics of space use given two-dimensional prey-availability data and explore\nthe effect of the redistribution kernel width. Our factorization allows such\nnumerical simulations to be done extremely fast; we expect this to aid the\ncomputationally-intensive task of model parameter fitting and inverse modeling.\n", "category": [4]}
{"abstract": "  We consider the problem of estimating a density $f_X$ using a sample\n$Y_1,...,Y_n$ from $f_Y=f_X\\star f_{\\epsilon}$, where $f_{\\epsilon}$ is an\nunknown density. We assume that an additional sample\n$\\epsilon_1,...,\\epsilon_m$ from $f_{\\epsilon}$ is observed. Estimators of\n$f_X$ and its derivatives are constructed by using nonparametric estimators of\n$f_Y$ and $f_{\\epsilon}$ and by applying a spectral cut-off in the Fourier\ndomain. We derive the rate of convergence of the estimators in case of a known\nand unknown error density $f_{\\epsilon}$, where it is assumed that $f_X$\nsatisfies a polynomial, logarithmic or general source condition. It is shown\nthat the proposed estimators are asymptotically optimal in a minimax sense in\nthe models with known or unknown error density, if the density $f_X$ belongs to\na Sobolev space $H_{\\mathbh p}$ and $f_{\\epsilon}$ is ordinary smooth or\nsupersmooth.\n", "category": [2, 6]}
{"abstract": "  An orthogonal array OA(q^{2n-1},q^{2n-2}, q,2) is constructed from the action\nof a subset of PGL(n+1,q^2) on some non--degenerate Hermitian varieties in\nPG(n,q^2). It is also shown that the rows of this orthogonal array correspond\nto some blocks of an affine design, which for q> 2 is a non--classical model of\nthe affine space AG(2n-1,q).\n", "category": [2, 2, 6]}
{"abstract": "  In this paper we describe a new technique for the comparison of populations\nof DNA strands. Comparison is vital to the study of ecological systems, at both\nthe micro and macro scales. Existing methods make use of DNA sequencing and\ncloning, which can prove costly and time consuming, even with current\nsequencing techniques. Our overall objective is to address questions such as:\n(i) (Genome detection) Is a known genome sequence present, at least in part, in\nan environmental sample? (ii) (Sequence query) Is a specific fragment sequence\npresent in a sample? (iii) (Similarity discovery) How similar in terms of\nsequence content are two unsequenced samples? We propose a method involving\nmultiple filtering criteria that result in \"pools\" of DNA of high or very high\npurity. Because our method is similar in spirit to hashing in computer science,\nwe call it DNA hash pooling. To illustrate this method, we describe protocols\nusing pairs of restriction enzymes. The in silico empirical results we present\nreflect a sensitivity to experimental error. Our method will normally be\nperformed as a filtering step prior to sequencing in order to reduce the amount\nof sequencing required (generally by a factor of 10 or more). Even as\nsequencing becomes cheaper, an order of magnitude remains important.\n", "category": [4, 4]}
{"abstract": "  We investigate the hydrodynamic interactions between microorganisms swimming\nat low Reynolds number. By considering simple model swimmers, and combining\nanalytic and numerical approaches, we investigate the time-averaged flow field\naround a swimmer. At short distances the swimmer behaves like a pump. At large\ndistances the velocity field depends on whether the swimming stroke is\ninvariant under a combined time-reversal and parity transformation. We then\nconsider two swimmers and find that the interaction between them consists of\ntwo parts; a dead term, independent of the motion of the second swimmer, which\ntakes the expected dipolar form and a live term resulting from the simultaneous\nswimming action of both swimmers which does not. We argue that, in general, the\nlatter dominates. The swimmer--swimmer interaction is a complicated function of\ntheir relative displacement, orientation and phase, leading to motion that can\nbe attractive, repulsive or oscillatory.\n", "category": [3, 3, 3, 4]}
{"abstract": "  We consider two seemingly very different self-assembly processes: formation\nof viral capsids, and crystallization of sticky discs. At low temperatures,\nassembly is ineffective, since there are many metastable disordered states,\nwhich are a source of kinetic frustration. We use fluctuation-dissipation\nratios to extract information about the degree of this frustration. We show\nthat our analysis is a useful indicator of the long term fate of the system,\nbased on the early stages of assembly.\n", "category": [3, 4]}
{"abstract": "  We present a mathematical analysis of the effects of Hebbian learning in\nrandom recurrent neural networks, with a generic Hebbian learning rule\nincluding passive forgetting and different time scales for neuronal activity\nand learning dynamics. Previous numerical works have reported that Hebbian\nlearning drives the system from chaos to a steady state through a sequence of\nbifurcations. Here, we interpret these results mathematically and show that\nthese effects, involving a complex coupling between neuronal dynamics and\nsynaptic graph structure, can be analyzed using Jacobian matrices, which\nintroduce both a structural and a dynamical point of view on the neural network\nevolution. Furthermore, we show that the sensitivity to a learned pattern is\nmaximal when the largest Lyapunov exponent is close to 0. We discuss how neural\nnetworks may take advantage of this regime of high functional interest.\n", "category": [3, 4]}
{"abstract": "  A spontaneously active neural system that is capable of continual learning\nshould also be capable of homeostasis of both firing rate and connectivity.\nExperimental evidence suggests that both types of homeostasis exist, and that\nconnectivity is maintained at a state that is optimal for information\ntransmission and storage. This state is referred to as the critical state. We\npresent a simple stochastic computational Hebbian learning model that\nincorporates both firing rate and critical homeostasis, and we explore its\nstability and connectivity properties. We also examine the behavior of our\nmodel with a simulated seizure and with simulated acute deafferentation. We\nargue that a neural system that is more highly connected than the critical\nstate (i.e., one that is \"supercritical\") is epileptogenic. Based on our\nsimulations, we predict that the post-seizural and post-deafferentation states\nshould be supercritical and epileptogenic. Furthermore, interventions that\nboost spontaneous activity should be protective against epileptogenesis.\n", "category": [4]}
{"abstract": "  A new type of ensemble filter is proposed, which combines an ensemble Kalman\nfilter (EnKF) with the ideas of morphing and registration from image\nprocessing. This results in filters suitable for nonlinear problems whose\nsolutions exhibit moving coherent features, such as thin interfaces in wildfire\nmodeling. The ensemble members are represented as the composition of one common\nstate with a spatial transformation, called registration mapping, plus a\nresidual. A fully automatic registration method is used that requires only\ngridded data, so the features in the model state do not need to be identified\nby the user. The morphing EnKF operates on a transformed state consisting of\nthe registration mapping and the residual. Essentially, the morphing EnKF uses\nintermediate states obtained by morphing instead of linear combinations of the\nstates.\n", "category": [2, 0, 2, 3, 6, 6]}
{"abstract": "  In this paper, we present a spatial version of phytoplankton-zooplankton\nmodel that includes some important factors such as external periodic forces,\nnoise, and diffusion processes. The spatially extended\nphytoplankton-zooplankton system is from the original study by Scheffer [M\nScheffer, Fish and nutrients interplay determines algal biomass: a minimal\nmodel, Oikos \\textbf{62} (1991) 271-282]. Our results show that the spatially\nextended system exhibit a resonant patterns and frequency-locking phenomena.\nThe system also shows that the noise and the external periodic forces play a\nconstructive role in the Scheffer's model: first, the noise can enhance the\noscillation of phytoplankton species' density and format a large clusters in\nthe space when the noise intensity is within certain interval. Second, the\nexternal periodic forces can induce 4:1 and 1:1 frequency-locking and spatially\nhomogeneous oscillation phenomena to appear. Finally, the resonant patterns are\nobserved in the system when the spatial noises and external periodic forces are\nboth turned on. Moreover, we found that the 4:1 frequency-locking transform\ninto 1:1 frequency-locking when the noise intensity increased. In addition to\nelucidating our results outside the domain of Turing instability, we provide\nfurther analysis of Turing linear stability with the help of the numerical\ncalculation by using the Maple software. Significantly, oscillations are\nenhanced in the system when the noise term presents. These results indicate\nthat the oceanic plankton bloom may partly due to interplay between the\nstochastic factors and external forces instead of deterministic factors. These\nresults also may help us to understand the effects arising from undeniable\nsubject to random fluctuations in oceanic plankton bloom.\n", "category": [4, 3, 3, 4]}
{"abstract": "  Intracellular recordings of cortical neurons in vivo display intense\nsubthreshold membrane potential (Vm) activity. The power spectral density (PSD)\nof the Vm displays a power-law structure at high frequencies (>50 Hz) with a\nslope of about -2.5. This type of frequency scaling cannot be accounted for by\ntraditional models, as either single-compartment models or models based on\nreconstructed cell morphologies display a frequency scaling with a slope close\nto -4. This slope is due to the fact that the membrane resistance is\n\"short-circuited\" by the capacitance for high frequencies, a situation which\nmay not be realistic. Here, we integrate non-ideal capacitors in cable\nequations to reflect the fact that the capacitance cannot be charged\ninstantaneously. We show that the resulting \"non-ideal\" cable model can be\nsolved analytically using Fourier transforms. Numerical simulations using a\nball-and-stick model yield membrane potential activity with similar frequency\nscaling as in the experiments. We also discuss the consequences of using\nnon-ideal capacitors on other cellular properties such as the transmission of\nhigh frequencies, which is boosted in non-ideal cables, or voltage attenuation\nin dendrites. These results suggest that cable equations based on non-ideal\ncapacitors should be used to capture the behavior of neuronal membranes at high\nfrequencies.\n", "category": [4]}
{"abstract": "  We consider insurance derivatives depending on an external physical risk\nprocess, for example a temperature in a low dimensional climate model. We\nassume that this process is correlated with a tradable financial asset. We\nderive optimal strategies for exponential utility from terminal wealth,\ndetermine the indifference prices of the derivatives, and interpret them in\nterms of diversification pressure. Moreover we check the optimal investment\nstrategies for standard admissibility criteria. Finally we compare the static\nrisk connected with an insurance derivative to the reduced risk due to a\ndynamic investment into the correlated asset. We show that dynamic hedging\nreduces the risk aversion in terms of entropic risk measures by a factor\nrelated to the correlation.\n", "category": [5, 2, 2, 5]}
{"abstract": "  The joint cumulative distribution function for order statistics arising from\nseveral different populations is given in terms of the distribution function of\nthe populations. The computational cost of the formula in the case of two\npopulations is still exponential in the worst case, but it is a dramatic\nimprovement compared to the general formula by Bapat and Beg. In the case when\nonly the joint distribution function of a subset of the order statistics of\nfixed size is needed, the complexity is polynomial, for the case of two\npopulations.\n", "category": [2, 2, 6, 6]}
{"abstract": "  In this work we develop a microscopic physical model of early evolution,\nwhere phenotype,organism life expectancy, is directly related to genotype, the\nstability of its proteins in their native conformations which can be determined\nexactly in the model. Simulating the model on a computer, we consistently\nobserve the Big Bang scenario whereby exponential population growth ensues as\nsoon as favorable sequence-structure combinations (precursors of stable\nproteins) are discovered. Upon that, random diversity of the structural space\nabruptly collapses into a small set of preferred proteins. We observe that\nprotein folds remain stable and abundant in the population at time scales much\ngreater than mutation or organism lifetime, and the distribution of the\nlifetimes of dominant folds in a population approximately follows a power law.\nThe separation of evolutionary time scales between discovery of new folds and\ngeneration of new sequences gives rise to emergence of protein families and\nsuperfamilies whose sizes are power-law distributed, closely matching the same\ndistributions for real proteins. On the population level we observe emergence\nof species, subpopulations which carry similar genomes. Further we present a\nsimple theory that relates stability of evolving proteins to the sizes of\nemerging genomes. Together, these results provide a microscopic first\nprinciples picture of how first gene families developed in the course of early\nevolution\n", "category": [4, 4]}
{"abstract": "  Molecular biology is a nanotechnology that works--it has worked for billions\nof years and in an amazing variety of circumstances. At its core is a system\nfor acquiring, processing and communicating information that is universal, from\nviruses and bacteria to human beings. Advances in genetics and experience in\ndesigning computers have taken us to a stage where we can understand the\noptimisation principles at the root of this system, from the availability of\nbasic building blocks to the execution of tasks. The languages of DNA and\nproteins are argued to be the optimal solutions to the information processing\ntasks they carry out. The analysis also suggests simpler predecessors to these\nlanguages, and provides fascinating clues about their origin. Obviously, a\ncomprehensive unraveling of the puzzle of life would have a lot to say about\nwhat we may design or convert ourselves into.\n", "category": [4, 0, 2, 3]}
{"abstract": "  This paper develops a mathematical model describing the influence that\nconjugation-mediated Horizontal Gene Transfer (HGT) has on the\nmutation-selection balance in an asexually reproducing population of\nunicellular, prokaryotic organisms. It is assumed that mutation-selection\nbalance is reached in the presence of a fixed background concentration of\nantibiotic, to which the population must become resistant in order to survive.\nWe analyze the behavior of the model in the limit of low and high\nantibiotic-induced first-order death rate constants, and find that the highest\nmean fitness is obtained at low rates of bacterial conjugation. As the rate of\nconjugation crosses a threshold, the mean fitness decreases to a minimum, and\nthen rises asymptotically to a limiting value as the rate of conjugation\nbecomes infinitely large. However, this limiting value is smaller than the mean\nfitness obtained in the limit of low conjugation rate. This dependence of the\nmean fitness on the conjugation rate is fairly small for the parameter ranges\nwe have considered, and disappears as the first-order death rate constant due\nto the presence of antibiotic approaches zero. For large values of the\nantibiotic death rate constant, we have obtained an analytical solution for the\nbehavior of the mean fitness that agrees well with the results of simulations.\nThe results of this paper suggest that conjugation-mediated HGT has a slightly\ndeleterious effect on the mean fitness of a population at mutation-selection\nbalance. Therefore, we argue that HGT confers a selective advantage by allowing\nfor faster adaptation to a new or changing environment. The results of this\npaper are consistent with the observation that HGT can be promoted by\nenvironmental stresses on a population.\n", "category": [4, 4]}
{"abstract": "  The problem of reconstructing and identifying intracellular protein signaling\nand biochemical networks is of critical importance in biology today. We sought\nto develop a mathematical approach to this problem using, as a test case, one\nof the most well-studied and clinically important signaling networks in biology\ntoday, the epidermal growth factor receptor (EGFR) driven signaling cascade.\nMore specifically, we suggest a method, augmented sparse reconstruction, for\nthe identification of links among nodes of ordinary differential equation (ODE)\nnetworks from a small set of trajectories with different initial conditions.\nOur method builds a system of representation by using a collection of integrals\nof all given trajectories and by attenuating block of terms in the\nrepresentation itself. The system of representation is then augmented with\nrandom vectors, and minimization of the 1-norm is used to find sparse\nrepresentations for the dynamical interactions of each node. Augmentation by\nrandom vectors is crucial, since sparsity alone is not able to handle the large\nerror-in-variables in the representation. Augmented sparse reconstruction\nallows to consider potentially very large spaces of models and it is able to\ndetect with high accuracy the few relevant links among nodes, even when\nmoderate noise is added to the measured trajectories. After showing the\nperformance of our method on a model of the EGFR protein network, we sketch\nbriefly the potential future therapeutic applications of this approach.\n", "category": [3, 4]}
{"abstract": "  Financial markets can be described on several time scales. We use data from\nthe limit order book of the London Stock Exchange (LSE) to compare how the\nfluctuation dominated microstructure crosses over to a more systematic global\nbehavior.\n", "category": [5, 3, 3]}
{"abstract": "  We analyze the dynamics of a forecasting game which exhibits the phenomenon\nof information cascades. Each agent aims at correctly predicting a binary\nvariable and he/she can either look for independent information or herd on the\nchoice of others. We show that dynamics can be analitically described in terms\nof a Langevin equation and its collective behavior is described by the solution\nof a Kramers' problem. This provides very accurate results in the region where\nthe vast majority of agents herd, which corresponds to the most interesting one\nfrom a game theoretic point of view.\n", "category": [5, 3]}
{"abstract": "  Classical population genetics a priori assigns fitness to alleles without\nconsidering molecular or functional properties of proteins that these alleles\nencode. Here we study population dynamics in a model where fitness can be\ninferred from physical properties of proteins under a physiological assumption\nthat loss of stability of any protein encoded by an essential gene confers a\nlethal phenotype. Accumulation of mutations in organisms containing Gamma genes\ncan then be represented as diffusion within the Gamma dimensional hypercube\nwith adsorbing boundaries which are determined, in each dimension, by loss of a\nprotein stability and, at higher stability, by lack of protein sequences.\nSolving the diffusion equation whose parameters are derived from the data on\npoint mutations in proteins, we determine a universal distribution of protein\nstabilities, in agreement with existing data. The theory provides a fundamental\nrelation between mutation rate, maximal genome size and thermodynamic response\nof proteins to point mutations. It establishes a universal speed limit on rate\nof molecular evolution by predicting that populations go extinct (via lethal\nmutagenesis) when mutation rate exceeds approximately 6 mutations per essential\npart of genome per replication for mesophilic organisms and 1 to 2 mutations\nper genome per replication for thermophilic ones. Further, our results suggest\nthat in absence of error correction, modern RNA viruses and primordial genomes\nmust necessarily be very short. Several RNA viruses function close to the\nevolutionary speed limit while error correction mechanisms used by DNA viruses\nand non-mutant strains of bacteria featuring various genome lengths and\nmutation rates have brought these organisms universally about 1000 fold below\nthe natural speed limit.\n", "category": [4, 4]}
{"abstract": "  The number of fixed mutations accumulated in an evolving population often\ndisplays a variance that is significantly larger than the mean (the\noverdispersed molecular clock). By examining a generic evolutionary process on\na neutral network of high-fitness genotypes, we establish a formalism for\ncomputing all cumulants of the full probability distribution of accumulated\nmutations in terms of graph properties of the neutral network, and use the\nformalism to prove overdispersion of the molecular clock. We further show that\nsignificant overdispersion arises naturally in evolution when the neutral\nnetwork is highly sparse, exhibits large global fluctuations in neutrality, and\nsmall local fluctuations in neutrality. The results are also relevant for\nelucidating the topological structure of a neutral network from empirical\nmeasurements of the substitution process.\n", "category": [4, 4]}
{"abstract": "  In a recent paper, Marr, Muller-Linow and Hutt [Phys. Rev. E 75, 041917\n(2007)] investigate an artificial dynamic system on metabolic networks. They\nfind a less complex time evolution of this dynamic system in real networks,\ncompared to networks of reference models. The authors argue that this suggests\nthat metabolic network structure is a major factor behind the stability of\nbiochemical steady states. We reanalyze the same kind of data using a dynamic\nsystem modeling actual reaction kinetics. The conclusions about stability, from\nour analysis, are inconsistent with those of Marr et al. We argue that this\nissue calls for a more detailed type of modeling.\n", "category": [4]}
{"abstract": "  In this paper we study the possible microscopic origin of heavy-tailed\nprobability density distributions for the price variation of financial\ninstruments. We extend the standard log-normal process to include another\nrandom component in the so-called stochastic volatility models. We study these\nmodels under an assumption, akin to the Born-Oppenheimer approximation, in\nwhich the volatility has already relaxed to its equilibrium distribution and\nacts as a background to the evolution of the price process. In this\napproximation, we show that all models of stochastic volatility should exhibit\na scaling relation in the time lag of zero-drift modified log-returns. We\nverify that the Dow-Jones Industrial Average index indeed follows this scaling.\nWe then focus on two popular stochastic volatility models, the Heston and\nHull-White models. In particular, we show that in the Hull-White model the\nresulting probability distribution of log-returns in this approximation\ncorresponds to the Tsallis (t-Student) distribution. The Tsallis parameters are\ngiven in terms of the microscopic stochastic volatility model. Finally, we show\nthat the log-returns for 30 years Dow Jones index data is well fitted by a\nTsallis distribution, obtaining the relevant parameters.\n", "category": [5, 3, 3]}
{"abstract": "  Independent component analysis (ICA) has been widely used for blind source\nseparation in many fields such as brain imaging analysis, signal processing and\ntelecommunication. Many statistical techniques based on M-estimates have been\nproposed for estimating the mixing matrix. Recently, several nonparametric\nmethods have been developed, but in-depth analysis of asymptotic efficiency has\nnot been available. We analyze ICA using semiparametric theories and propose a\nstraightforward estimate based on the efficient score function by using\nB-spline approximations. The estimate is asymptotically efficient under\nmoderate conditions and exhibits better performance than standard ICA methods\nin a variety of simulations.\n", "category": [6, 2, 6, 6]}
{"abstract": "  Recently Supernova 2006gy was noted as the most luminous ever recorded, with\na total radiated energy of ~10^44 Joules. It was proposed that the progenitor\nmay have been a massive evolved star similar to eta Carinae, which resides in\nour own galaxy at a distance of about 2.3 kpc. eta Carinae appears ready to\ndetonate. Although it is too distant to pose a serious threat as a normal\nsupernova, and given its rotation axis is unlikely to produce a Gamma-Ray Burst\noriented toward the Earth, eta Carinae is about 30,000 times nearer than\n2006gy, and we re-evaluate it as a potential superluminous supernova. We find\nthat given the large ratio of emission in the optical to the X-ray, atmospheric\neffects are negligible. Ionization of the atmosphere and concomitant ozone\ndepletion are unlikely to be important. Any cosmic ray effects should be spread\nout over ~10^4 y, and similarly unlikely to produce any serious perturbation to\nthe biosphere. We also discuss a new possible effect of supernovae, endocrine\ndisruption induced by blue light near the peak of the optical spectrum. This is\na possibility for nearby supernovae at distances too large to be considered\n\"dangerous\" for other reasons. However, due to reddening and extinction by the\ninterstellar medium, eta Carinae is unlikely to trigger such effects to any\nsignificant degree.\n", "category": [3, 4]}
{"abstract": "  It is well known that complete prior ignorance is not compatible with\nlearning, at least in a coherent theory of (epistemic) uncertainty. What is\nless widely known, is that there is a state similar to full ignorance, that\nWalley calls near-ignorance, that permits learning to take place. In this paper\nwe provide new and substantial evidence that also near-ignorance cannot be\nreally regarded as a way out of the problem of starting statistical inference\nin conditions of very weak beliefs. The key to this result is focusing on a\nsetting characterized by a variable of interest that is latent. We argue that\nsuch a setting is by far the most common case in practice, and we show, for the\ncase of categorical latent variables (and general manifest variables) that\nthere is a sufficient condition that, if satisfied, prevents learning to take\nplace under prior near-ignorance. This condition is shown to be easily\nsatisfied in the most common statistical problems.\n", "category": [2, 2, 6]}
{"abstract": "  Recent in vitro data show that neurons respond to input variance with varying\nsensitivities. Here, we demonstrate that Hodgkin-Huxley (HH) neurons can\noperate in two computational regimes, one that is more sensitive to input\nvariance (differentiating) and one that is less sensitive (integrating). A\nboundary plane in the 3D conductance space separates these two regimes. For a\nreduced HH model, this plane can be derived analytically from the V nullcline,\nthus suggesting a means of relating biophysical parameters to neural\ncomputation by analyzing the neuron's dynamical system.\n", "category": [4, 3]}
{"abstract": "  In this paper we apply new geometric and combinatorial methods to the study\nof phylogenetic mixtures. The focus of the geometric approach is to describe\nthe geometry of phylogenetic mixture distributions for the two state random\ncluster model, which is a generalization of the two state symmetric (CFN)\nmodel. In particular, we show that the set of mixture distributions forms a\nconvex polytope and we calculate its dimension; corollaries include a simple\ncriterion for when a mixture of branch lengths on the star tree can mimic the\nsite pattern frequency vector of a resolved quartet tree. Furthermore, by\ncomputing volumes of polytopes we can clarify how ``common'' non-identifiable\nmixtures are under the CFN model. We also present a new combinatorial result\nwhich extends any identifiability result for a specific pair of trees of size\nsix to arbitrary pairs of trees. Next we present a positive result showing\nidentifiability of rates-across-sites models. Finally, we answer a question\nraised in a previous paper concerning ``mixed branch repulsion'' on trees\nlarger than quartet trees under the CFN model.\n", "category": [4]}
{"abstract": "  Avalanches, or Avalanche-like, events are often observed in the dynamical\nbehaviour of many complex systems which span from solar flaring to the Earth's\ncrust dynamics and from traffic flows to financial markets. Self-organized\ncriticality (SOC) is one of the most popular theories able to explain this\nintermittent charge/discharge behaviour. Despite a large amount of theoretical\nwork, empirical tests for SOC are still in their infancy. In the present paper\nwe address the common problem of revealing SOC from a simple time series\nwithout having much information about the underlying system. As a working\nexample we use a modified version of the multifractal random walk originally\nproposed as a model for the stock market dynamics. The study reveals, despite\nthe lack of the typical ingredients of SOC, an avalanche-like dynamics similar\nto that of many physical systems. While, on one hand, the results confirm the\nrelevance of cascade models in representing turbulent-like phenomena, on the\nother, they also raise the question about the current state of reliability of\nSOC inference from time series analysis.\n", "category": [3, 3, 5]}
{"abstract": "  Mortality, birth rates and retirement play a major role in demographic\nchanges. In most cases, mortality rates decreased in the past century without\nnoticeable decrease in fertility rates, this leads to a significant increase in\npopulation growth. In many poor countries like Palestinian territories the\nnumber of births has fallen and the life expectancy increased.\n  In this article we concentrate on measuring, analyzing and extrapolating the\nage structure in Palestine a few decades ago into future. A Fortran program has\nbeen designed and used for the simulation and analysis of our statistical data.\nThis study of demographic change in Palestine has shown that Palestinians will\nhave in future problems as the strongest age cohorts are the above-60-year\nolds. We therefore recommend the increase of both the retirement age and women\nemployment.\n", "category": [4]}
{"abstract": "  A novel approach to protein multiple sequence alignment is discussed:\nsubstantially this method counterparts with substitution matrix based methods\n(like Blosum or PAM based methods), and implies a more deterministic approach\nto chemical/physical sub-grouping of amino acids . Amino acids (aa) are divided\ninto sub-groups with successive derivations, that result in a clustering based\non the considered property. The properties can be user defined or chosen\nbetween default schemes, like those used in the analysis described here.\nStarting from an initial set of the 20 naturally occurring amino acids, they\nare successively divided on the basis of their polarity/hydrophobic index, with\nincreasing resolution up to four level of subdivision. Other schemes of\nsubdivision are possible: in this thesis work it was employed also a scheme\nbased on physical/structural properties (solvent exposure, lateral chain\nmobility and secondary structure tendency), that have been compared to the\nchemical scheme with testing purposes. In the method described in this chapter,\nthe total score for each position in the alignment accounts for different\ndegree of similarity between amino acids. The scoring value result form the\ncontribution of each level of selectivity for every individual property\nconsidered. Simply the method (called M_Al) analyse the n sequence alignment\nposition per position and assigns a score which have contributes by aa identity\nplus a composed valuation of the chemical or of the structural affinity between\nthe n aligned amino acids. This method has been implemented in a series of\nprograms written in python language; these programs have been tested in some\nbiological cases, with benchmark purposes.\n", "category": [4, 4]}
{"abstract": "  Network tomography has been regarded as one of the most promising\nmethodologies for performance evaluation and diagnosis of the massive and\ndecentralized Internet. This paper proposes a new estimation approach for\nsolving a class of inverse problems in network tomography, based on marginal\ndistributions of a sequence of one-dimensional linear projections of the\nobserved data. We give a general identifiability result for the proposed method\nand study the design issue of these one dimensional projections in terms of\nstatistical efficiency. We show that for a simple Gaussian tomography model,\nthere is an optimal set of one-dimensional projections such that the estimator\nobtained from these projections is asymptotically as efficient as the maximum\nlikelihood estimator based on the joint distribution of the observed data. For\npractical applications, we carry out simulation studies of the proposed method\nfor two instances of network tomography. The first is for traffic demand\ntomography using a Gaussian Origin-Destination traffic model with a power\nrelation between its mean and variance, and the second is for network delay\ntomography where the link delays are to be estimated from the end-to-end path\ndelays. We compare estimators obtained from our method and that obtained from\nusing the joint distribution and other lower dimensional projections, and show\nthat in both cases, the proposed method yields satisfactory results.\n", "category": [6, 2, 6, 6]}
{"abstract": "  Observations consisting of measurements on relationships for pairs of objects\narise in many settings, such as protein interaction and gene regulatory\nnetworks, collections of author-recipient email, and social networks. Analyzing\nsuch data with probabilisic models can be delicate because the simple\nexchangeability assumptions underlying many boilerplate models no longer hold.\nIn this paper, we describe a latent variable model of such data called the\nmixed membership stochastic blockmodel. This model extends blockmodels for\nrelational data to ones which capture mixed membership latent relational\nstructure, thus providing an object-specific low-dimensional representation. We\ndevelop a general variational inference algorithm for fast approximate\nposterior inference. We explore applications to social and protein interaction\nnetworks.\n", "category": [6, 0, 2, 3, 6, 6]}
{"abstract": "  We introduce a linear space of finitely additive measures to treat the\nproblem of optimal expected utility from consumption under a stochastic clock\nand an unbounded random endowment process. In this way we establish existence\nand uniqueness for a large class of utility maximization problems including the\nclassical ones of terminal wealth or consumption, as well as the problems\ndepending on a random time-horizon or multiple consumption instances. As an\nexample we treat explicitly the problem of maximizing the logarithmic utility\nof a consumption stream, where the local time of an Ornstein-Uhlenbeck process\nacts as a stochastic clock.\n", "category": [5, 2, 2]}
{"abstract": "  The Behrens-Fisher problem concerns testing the equality of the means of two\nnormal populations with possibly different variances. The null hypothesis in\nthis problem induces a statistical model for which the likelihood function may\nhave more than one local maximum. We show that such multimodality contradicts\nthe null hypothesis in the sense that if this hypothesis is true then the\nprobability of multimodality converges to zero when both sample sizes tend to\ninfinity. Additional results include a finite-sample bound on the probability\nof multimodality under the null and asymptotics for the probability of\nmultimodality under the alternative.\n", "category": [2, 6]}
{"abstract": "  We propose the variable selection procedure incorporating prior constraint\ninformation into lasso. The proposed procedure combines the sample and prior\ninformation, and selects significant variables for responses in a narrower\nregion where the true parameters lie. It increases the efficiency to choose the\ntrue model correctly. The proposed procedure can be executed by many\nconstrained quadratic programming methods and the initial estimator can be\nfound by least square or Monte Carlo method. The proposed procedure also enjoys\ngood theoretical properties. Moreover, the proposed procedure is not only used\nfor linear models but also can be used for generalized linear models({\\sl\nGLM}), Cox models, quantile regression models and many others with the help of\nWang and Leng (2007)'s LSA, which changes these models as the approximation of\nlinear models. The idea of combining sample and prior constraint information\ncan be also used for other modified lasso procedures. Some examples are used\nfor illustration of the idea of incorporating prior constraint information in\nvariable selection procedures.\n", "category": [6]}
{"abstract": "  Ni\\'eb\\'e is a food leguminous plant cultivated in tropical Africa for its\nseeds rich in proteins. The main problem setted by its production is the\nconservation of harvests. In the fields as in the stocks, the seeds are\ndestroyed by pests (bruchids). These bruchids are always associated with\nseveral entomophagous species of hymenoptera. Four entomophagous species were\nlisted : an egg parasitoid (U lariophaga Stephan), and three solitary larval\nand pupal ectoparasitoids (D. Basalis Rondoni, Pteromalidae; E. vuilleti\nCrawford and E. orientalis Crawford, Eupelmidae). The survey of the populations\nshows that at the beginning of storage, E orientalis is the most abundant\nspecie (72 %) whereas E. vuilleti and D. Basalis respectively represent 12 %\nand 16 % of the hymenoptera. During storage, the E orientalis population\ndecreases gradually and it disappears completely in less than two months after\nthe beginning of storage. E. Vuilleti population becomes gradually more\nimportant than D. basalis population which regress until less than 10 % of the\nemerging parasitoids. E vuilleti adopts ovicide and larvicide behaviour against\nD. Basalis. This behaviour explains its population regression inside granaries.\nIf the aggressive behaviour of this Eupelmidae is a constant, that could also\nexplain the disappearance of E orientalis. However if this species is\nmaintained in stocks, it would be an effective control agent of bruchids\naccording to their parasitic capacities. This study shows that ovicide and\nlarvicide behaviour of E vuilleti is not expressed against E orientalis. When\nthe females have exclusively the hosts already parasitized by E orientalis,\nthey do not lay eggs. The disappearance of E orientalis could not thus be\nexplained by the presence of E. vuilleti.\n", "category": [4]}
{"abstract": "  Selective control in a population is the ability to control a member of the\npopulation while leaving the other members relatively unaffected. The concept\nof selective control is developed using cell death or apoptosis in\nheterogeneous cell populations as an example. Apoptosis signaling in\nheterogeneous cells is described by an ensemble of gene networks with identical\ntopology but different link strengths. Selective control depends on the\nstatistics of signaling in the ensemble of networks and we analyse the effects\nof superposition, non-linearity and feedback on these statistics. Parallel\npathways promote normal statistics while series pathways promote skew\ndistributions which in the most extreme cases become log-normal. We also show\nthat feedback and non-linearity can produce bimodal signaling statistics, as\ncan discreteness and non-linearity. Two methods for optimizing selective\ncontrol are presented. The first is an exhaustive search method and the second\nis a linear programming based approach. Though control of a single gene in the\nsignaling network yields little selectivity, control of a few genes typically\nyields higher levels of selectivity. The statistics of gene combinations\nsusceptible to selective control is studied and is used to identify general\ncontrol strategies. We found that selectivity is promoted by acting on the\nleast sensitive nodes in the case of weak populations, while selective control\nof robust populations is optimized through perturbations of more sensitive\nnodes. High throughput experiments with heterogeneous cell lines could be\ndesigned in an analogous manner, with the further possibility of incorporating\nthe selectivity optimization process into a closed-loop control system.\n", "category": [4, 3]}
{"abstract": "  We consider the problem of maximizing expected utility from consumption in a\nconstrained incomplete semimartingale market with a random endowment process,\nand establish a general existence and uniqueness result using techniques from\nconvex duality. The notion of asymptotic elasticity of Kramkov and\nSchachermayer is extended to the time-dependent case. By imposing no smoothness\nrequirements on the utility function in the temporal argument, we can treat\nboth pure consumption and combined consumption/terminal wealth problems, in a\ncommon framework. To make the duality approach possible, we provide a detailed\ncharacterization of the enlarged dual domain which is reminiscent of the\nenlargement of $L^1$ to its topological bidual $(L^{\\infty})^*$, a space of\nfinitely-additive measures. As an application, we treat the case of a\nconstrained It\\^ o-process market-model.\n", "category": [5, 2, 2]}
{"abstract": "  This paper presents a dynamic linear model for modeling hourly ozone\nconcentrations over the eastern United States. That model, which is developed\nwithin an Bayesian hierarchical framework, inherits the important feature of\nsuch models that its coefficients, treated as states of the process, can change\nwith time. Thus the model includes a time--varying site invariant mean field as\nwell as time varying coefficients for 24 and 12 diurnal cycle components. This\ncost of this model's great flexibility comes at the cost of computational\ncomplexity, forcing us to use an MCMC approach and to restrict application of\nour model domain to a small number of monitoring sites. We critically assess\nthis model and discover some of its weaknesses in this type of application.\n", "category": [6]}
{"abstract": "  Singular Value Decomposition (SVD) is the basic body of many statistical\nalgorithms and few users question whether SVD is properly handling its job.\n  SVD aims at evaluating the decomposition that best approximates a data\nmatrix, given some rank restriction. However often we are interested in the\nbest components of the decomposition rather than in the best approximation .\nThis conflict of objectives leads us to introduce {\\em Total SVD}, where the\nword \"Total\" is taken as in \"Total\" least squares.\n  SVD is a least squares method and, therefore, is very sensitive to gross\nerrors in the data matrix. We make SVD robust by imposing a weight to each of\nthe matrix entries. Breakdown properties are excellent.\n  Algorithmic aspects are handled; they rely on high dimension fixed point\ncomputations.\n", "category": [6, 2, 6]}
{"abstract": "  This paper considers M-estimation of a nonlinear regression model with\nmultiple change-points occuring at unknown times. The multi-phase random design\nregression model, discontinuous in each change-point, have an arbitrary error\n$\\epsilon$. In the case when the number of jumps is known, the M-estimator of\nlocations of breaks and of regression parameters are studied. These estimators\nare consistent and the distribution of the regression parameter estimators is\nGaussian. The estimator of each change-point converges, with the rate $n^{-1}$,\nto the smallest minimizer of the independent compound Poisson processes. The\nresults are valid for a large class of error distributions.\n", "category": [2, 2, 6, 6]}
{"abstract": "  We show that the Kullback-Leibler distance is a good measure of the\nstatistical uncertainty of correlation matrices estimated by using a finite set\nof data. For correlation matrices of multivariate Gaussian variables we\nanalytically determine the expected values of the Kullback-Leibler distance of\na sample correlation matrix from a reference model and we show that the\nexpected values are known also when the specific model is unknown. We propose\nto make use of the Kullback-Leibler distance to estimate the information\nextracted from a correlation matrix by correlation filtering procedures. We\nalso show how to use this distance to measure the stability of filtering\nprocedures with respect to statistical uncertainty. We explain the\neffectiveness of our method by comparing four filtering procedures, two of them\nbeing based on spectral analysis and the other two on hierarchical clustering.\nWe compare these techniques as applied both to simulations of factor models and\nempirical data. We investigate the ability of these filtering procedures in\nrecovering the correlation matrix of models from simulations. We discuss such\nan ability in terms of both the heterogeneity of model parameters and the\nlength of data series. We also show that the two spectral techniques are\ntypically more informative about the sample correlation matrix than techniques\nbased on hierarchical clustering, whereas the latter are more stable with\nrespect to statistical uncertainty.\n", "category": [3, 3, 5]}
{"abstract": "  Existence of stochastic financial equilibria giving rise to semimartingale\nasset prices is established under a general class of assumptions. These\nequilibria are expressed in real terms and span complete markets or markets\nwith withdrawal constraints.We deal with random endowment density streams which\nadmit jumps and general time-dependent utility functions on which only\nregularity conditions are imposed. As an integral part of the proof of the main\nresult, we establish a novel characterization of semimartingale functions.\n", "category": [5, 2, 2]}
{"abstract": "  This paper provides a new version of the condition of Di Nunno et al. (2003),\nAnkirchner and Imkeller (2005) and Biagini and \\{O}ksendal (2005) ensuring the\nsemimartingale property for a large class of continuous stochastic processes.\nUnlike our predecessors, we base our modeling framework on the concept of\nportfolio proportions which yields a short self-contained proof of the main\ntheorem, as well as a counterexample, showing that analogues of our results do\nnot hold in the discontinuous setting.\n", "category": [5, 2, 5]}
{"abstract": "  The effectiveness of utility-maximization techniques for portfolio management\nrelies on our ability to estimate correctly the parameters of the dynamics of\nthe underlying financial assets. In the setting of complete or incomplete\nfinancial markets, we investigate whether small perturbations of the market\ncoefficient processes lead to small changes in the agent's optimal behavior\nderived from the solution of the related utility-maximization problems.\nSpecifically, we identify the topologies on the parameter process space and the\nsolution space under which utility-maximization is a continuous operation, and\nwe provide a counterexample showing that our results are best possible, in a\ncertain sense. A novel result about the structure of the solution of the\nutility-maximization problem where prices are modeled by continuous\nsemimartingales is established as an offshoot of the proof of our central\ntheorem.\n", "category": [5, 2, 2]}
{"abstract": "  This paper studies the problem of maximizing the expected utility of terminal\nwealth for a financial agent with an unbounded random endowment, and with a\nutility function which supports both positive and negative wealth. We prove the\nexistence of an optimal trading strategy within a class of permissible\nstrategies -- those strategies whose wealth process is a supermartingale under\nall pricing measures with finite relative entropy. We give necessary and\nsufficient conditions for the absence of utility-based arbitrage, and for the\nexistence of a solution to the primal problem.\n  We consider two utility-based methods which can be used to price contingent\nclaims. Firstly we investigate marginal utility-based price processes\n(MUBPP's). We show that such processes can be characterized as local\nmartingales under the normalized optimal dual measure for the utility\nmaximizing investor. Finally, we present some new results on utility\nindifference prices, including continuity properties and volume asymptotics for\nthe case of a general utility function, unbounded endowment and unbounded\ncontingent claims.\n", "category": [5, 2, 2]}
{"abstract": "  We investigate the ergodic problem of growth-rate maximization under a class\nof risk constraints in the context of incomplete, It\\^{o}-process models of\nfinancial markets with random ergodic coefficients. Including {\\em\nvalue-at-risk} (VaR), {\\em tail-value-at-risk} (TVaR), and {\\em limited\nexpected loss} (LEL), these constraints can be both wealth-dependent(relative)\nand wealth-independent (absolute). The optimal policy is shown to exist in an\nappropriate admissibility class, and can be obtained explicitly by uniform,\nstate-dependent scaling down of the unconstrained (Merton) optimal portfolio.\nThis implies that the risk-constrained wealth-growth optimizer locally behaves\nlike a CRRA-investor, with the relative risk-aversion coefficient depending on\nthe current values of the market coefficients.\n", "category": [5, 2, 2]}
{"abstract": "  We perform a stability analysis for the utility maximization problem in a\ngeneral semimartingale model where both liquid and illiquid assets (random\nendowments) are present. Small misspecifications of preferences (as modeled via\nexpected utility), as well as views of the world or the market model (as\nmodeled via subjective probabilities) are considered. Simple sufficient\nconditions are given for the problem to be well-posed, in the sense the optimal\nwealth and the marginal utility-based prices are continuous functionals of\npreferences and probabilistic views.\n", "category": [5, 2, 2]}
{"abstract": "  Multivariate statistics are often available as well as necessary in\nhypothesis tests. We study how to use such statistics to control not only false\ndiscovery rate (FDR) but also positive FDR (pFDR) with good power. We show that\nFDR can be controlled through nested regions of multivariate $p$-values of test\nstatistics. If the distributions of the test statistics are known, then the\nregions can be constructed explicitly to achieve FDR control with maximum power\namong procedures satisfying certain conditions. On the other hand, our focus is\nwhere the distributions are only partially known. Under certain conditions, a\ntype of nested regions are proposed and shown to attain (p)FDR control with\nasymptotically maximum power as the pFDR control level approaches its\nattainable limit. The procedure based on the nested regions is compared with\nthose based on other nested regions that are easier to construct as well as\nthose based on more straightforward combinations of the test statistics.\n", "category": [2, 6]}
{"abstract": "  Recent research has studied the role of sparsity in high dimensional\nregression and signal reconstruction, establishing theoretical limits for\nrecovering sparse models from sparse data. This line of work shows that\n$\\ell_1$-regularized least squares regression can accurately estimate a sparse\nlinear model from $n$ noisy examples in $p$ dimensions, even if $p$ is much\nlarger than $n$. In this paper we study a variant of this problem where the\noriginal $n$ input variables are compressed by a random linear transformation\nto $m \\ll n$ examples in $p$ dimensions, and establish conditions under which a\nsparse linear model can be successfully recovered from the compressed data. A\nprimary motivation for this compression procedure is to anonymize the data and\npreserve privacy by revealing little information about the original data. We\ncharacterize the number of random projections that are required for\n$\\ell_1$-regularized compressed regression to identify the nonzero coefficients\nin the true model with probability approaching one, a property called\n``sparsistence.'' In addition, we show that $\\ell_1$-regularized compressed\nregression asymptotically predicts as well as an oracle linear model, a\nproperty called ``persistence.'' Finally, we characterize the privacy\nproperties of the compression procedure in information-theoretic terms,\nestablishing upper bounds on the mutual information between the compressed and\nuncompressed data that decay to zero.\n", "category": [6, 0, 2]}
{"abstract": "  We study the problem of the emergence of cooperation in the spatial\nPrisoner's Dilemma. The pioneering work by Nowak and May showed that large\ninitial populations of cooperators can survive and sustain cooperation in a\nsquare lattice with imitate-the-best evolutionary dynamics. We revisit this\nproblem in a cost-benefit formulation suitable for a number of biological\napplications. We show that if a fixed-amount reward is established for\ncooperators to share, a single cooperator can invade a population of defectors\nand form structures that are resilient to re-invasion even if the reward\nmechanism is turned off. We discuss analytically the case of the invasion by a\nsingle cooperator and present agent-based simulations for small initial\nfractions of cooperators. Large cooperation levels, in the sustainability\nrange, are found. In the conclusions we discuss possible applications of this\nmodel as well as its connections with other mechanisms proposed to promote the\nemergence of cooperation.\n", "category": [4, 2, 3, 3, 6]}
{"abstract": "  We consider the static and dynamic models of Cournot duopoly with tax\nevasion. In the dynamic model we introduce the time delay and we analyze the\nlocal stability of the stationary state. There is a critical value of the delay\nwhen the Hopf bifurcation occurs.\n", "category": [2, 5]}
{"abstract": "  The problem of the definition and the estimation of generative models based\non deformable templates from raw data is of particular importance for modelling\nnon aligned data affected by various types of geometrical variability. This is\nespecially true in shape modelling in the computer vision community or in\nprobabilistic atlas building for Computational Anatomy (CA). A first coherent\nstatistical framework modelling the geometrical variability as hidden variables\nhas been given by Allassonni\\`ere, Amit and Trouv\\'e (JRSS 2006). Setting the\nproblem in a Bayesian context they proved the consistency of the MAP estimator\nand provided a simple iterative deterministic algorithm with an EM flavour\nleading to some reasonable approximations of the MAP estimator under low noise\nconditions. In this paper we present a stochastic algorithm for approximating\nthe MAP estimator in the spirit of the SAEM algorithm. We prove its convergence\nto a critical point of the observed likelihood with an illustration on images\nof handwritten digits.\n", "category": [6]}
{"abstract": "  Let $A_1,...,A_N$ be complex self-adjoint matrices and let $\\rho$ be a\ndensity matrix. The Robertson uncertainty principle $$ det(Cov_\\rho(A_h,A_j))\n\\geq det(- \\frac{i}{2} Tr(\\rho [A_h,A_j])) $$ gives a bound for the quantum\ngeneralized covariance in terms of the commutators $[A_h,A_j]$. The right side\nmatrix is antisymmetric and therefore the bound is trivial (equal to zero) in\nthe odd case $N=2m+1$.\n  Let $f$ be an arbitrary normalized symmetric operator monotone function and\nlet $<\\cdot, \\cdot >_{\\rho,f}$ be the associated quantum Fisher information. In\nthis paper we conjecture the inequality $$ det (Cov_\\rho(A_h,A_j)) \\geq det\n(\\frac{f(0)}{2} < i[\\rho, A_h],i[\\rho,A_j] >_{\\rho,f}) $$ that gives a\nnon-trivial bound for any natural number $N$ using the commutators $i[\\rho,\nA_h]$. The inequality has been proved in the cases $N=1,2$ by the joint efforts\nof many authors. In this paper we prove the case N=3 for real matrices.\n", "category": [2, 2, 6]}
{"abstract": "  Linear structural error-in-variables models with univariate observations are\nrevisited for studying modified least squares estimators of the slope and\nintercept. New marginal central limit theorems (CLT's) are established for\nthese estimators, assuming the existence of four moments for the measurement\nerrors and that the explanatory variables are in the domain of attraction of\nthe normal law. The latter condition for the explanatory variables is used the\nfirst time, and is so far the most general in this context. It is also optimal,\nor nearly optimal, for our CLT's. Moreover, due to the obtained CLT's being in\nStudentized and self-normalized forms to begin with, they are a priori nearly,\nor completely, data-based, and free of unknown parameters of the joint\ndistribution of the error and explanatory variables. Consequently, they lead to\na variety of readily available, or easily derivable, large-sample approximate\nconfidence intervals (CI's) for the slope and intercept. In contrast, in\nrelated CLT's in the literature so far, the variances of the limiting normal\ndistributions, in general, are complicated and depend on various, typically\nunknown, moments of the error and explanatory variables. Thus, the\ncorresponding CI's for the slope and intercept in the literature, unlike those\nof the present paper, are available only under some additional model\nassumptions.\n", "category": [2, 6]}
{"abstract": "  Motivated by finance and technical applications, the objective of this paper\nis to consider adaptive estimation of regression and density distribution based\non Fourier-Legendre expansion, and construction of confidence intervals - also\nadaptive. The estimators are asymptotically optimal and adaptive in the sense\nthat they can adapt to unknown smoothness.\n", "category": [2, 2, 6]}
{"abstract": "  We study the dynamics of co-evolution of producers and customers described by\nbit-strings representing individual traits. Individual ''size-like'' properties\nare controlled by binary encounters which outcome depends upon a recognition\nprocess. Depending upon the parameter set-up, mutual selection of producers and\ncustomers results in different types of attractors, either an exclusive niches\nregime or a competition regime.\n", "category": [5, 3]}
{"abstract": "  Power-law distributions occur in many situations of scientific interest and\nhave significant consequences for our understanding of natural and man-made\nphenomena. Unfortunately, the detection and characterization of power laws is\ncomplicated by the large fluctuations that occur in the tail of the\ndistribution -- the part of the distribution representing large but rare events\n-- and by the difficulty of identifying the range over which power-law behavior\nholds. Commonly used methods for analyzing power-law data, such as\nleast-squares fitting, can produce substantially inaccurate estimates of\nparameters for power-law distributions, and even in cases where such methods\nreturn accurate answers they are still unsatisfactory because they give no\nindication of whether the data obey a power law at all. Here we present a\nprincipled statistical framework for discerning and quantifying power-law\nbehavior in empirical data. Our approach combines maximum-likelihood fitting\nmethods with goodness-of-fit tests based on the Kolmogorov-Smirnov statistic\nand likelihood ratios. We evaluate the effectiveness of the approach with tests\non synthetic data and give critical comparisons to previous approaches. We also\napply the proposed methods to twenty-four real-world data sets from a range of\ndifferent disciplines, each of which has been conjectured to follow a power-law\ndistribution. In some cases we find these conjectures to be consistent with the\ndata while in others the power law is ruled out.\n", "category": [3, 3, 6, 6]}
{"abstract": "  In the setting of additive regression model for continuous time process, we\nestablish the optimal uniform convergence rates and optimal asymptotic\nquadratic error of additive regression. To build our estimate, we use the\nmarginal integration method.\n", "category": [2, 2, 6, 6]}
{"abstract": "  We establish some uniform limit results in the setting of additive regression\nmodel estimation. Our results allow to give an asymptotic 100% confidence bands\nfor these components. These results are stated in the framework of i.i.d random\nvectors when the marginal integration estimation method is used.\n", "category": [2, 2, 6, 6]}
{"abstract": "  In this manuscript we analyse the leading statistical properties of\nfluctuations of (log) 3-month US Treasury bill quotation in the secondary\nmarket, namely: probability density function, autocorrelation, absolute values\nautocorrelation, and absolute values persistency. We verify that this financial\ninstrument, in spite of its high liquidity, shows very peculiar properties.\nParticularly, we verify that log-fluctuations belong to the Levy class of\nstochastic variables.\n", "category": [5, 3, 3]}
{"abstract": "  A Bayesian approach is used to estimate the covariance matrix of Gaussian\ndata. Ideas from Gaussian graphical models and model selection are used to\nconstruct a prior for the covariance matrix that is a mixture over all\ndecomposable graphs. For this prior the probability of each graph size is\nspecified by the user and graphs of equal size are assigned equal probability.\nMost previous approaches assume that all graphs are equally probable. We show\nempirically that the prior that assigns equal probability over graph sizes\noutperforms the prior that assigns equal probability over all graphs, both in\nidentifying the correct decomposable graph and in more efficiently estimating\nthe covariance matrix.\n", "category": [6]}
{"abstract": "  Motivated by the work of Segal and Segal on the Black-Scholes pricing formula\nin the quantum context, we study a quantum extension of the Black-Scholes\nequation within the context of Hudson-Parthasarathy quantum stochastic\ncalculus. Our model includes stock markets described by quantum Brownian motion\nand Poisson process.\n", "category": [5, 2]}
{"abstract": "  Longitudinal data tracking repeated measurements on individuals are highly\nvalued for research because they offer controls for unmeasured individual\nheterogeneity that might otherwise bias results. Random effects or mixed models\napproaches, which treat individual heterogeneity as part of the model error\nterm and use generalized least squares to estimate model parameters, are often\ncriticized because correlation between unobserved individual effects and other\nmodel variables can lead to biased and inconsistent parameter estimates.\nStarting with an examination of the relationship between random effects and\nfixed effects estimators in the standard unobserved effects model, this article\ndemonstrates through analysis and simulation that the mixed model approach has\na ``bias compression'' property under a general model for individual\nheterogeneity that can mitigate bias due to uncontrolled differences among\nindividuals. The general model is motivated by the complexities of longitudinal\nstudent achievement measures, but the results have broad applicability to\nlongitudinal modeling.\n", "category": [6]}
{"abstract": "  We provide sensitivity comparisons for two competing versions of the\ndimension reduction method principal Hessian directions (pHd). These\ncomparisons consider the effects of small perturbations on the estimation of\nthe dimension reduction subspace via the influence function. We show that the\ntwo versions of pHd can behave completely differently in the presence of\ncertain observational types. Our results also provide evidence that outliers in\nthe traditional sense may or may not be highly influential in practice. Since\ninfluential observations may lurk within otherwise typical data, we consider\nthe influence function in the empirical setting for the efficient detection of\ninfluential observations in practice.\n", "category": [6]}
{"abstract": "  There are clear benefits associated with a particular consumer choice for\nmany current markets. For example, as we consider here, some products might\ncarry environmental or `green' benefits. Some consumers might value these\nbenefits while others do not. However, as evidenced by myriad failed attempts\nof environmental products to maintain even a niche market, such benefits do not\nnecessarily outweigh the extra purchasing cost. The question we pose is, how\ncan such an initially economically-disadvantaged green product evolve to hold\nthe greater share of the market? We present a simple mathematical model for the\ndynamics of product competition in a heterogeneous consumer population. Our\nmodel preassigns a hierarchy to the products, which designates the consumer\nchoice when prices are comparable, while prices are dynamically rescaled to\nreflect increasing returns to scale. Our approach allows us to model many\nscenarios of technology substitution and provides a method for generalizing\nmarket forces. With this model, we begin to forecast irreversible trends\nassociated with consumer dynamics as well as policies that could be made to\ninfluence transitions\n", "category": [5, 3]}
{"abstract": "  We report on a study of the Tehran Price Index (TEPIX) from 2001 to 2006 as\nan emerging market that has been affected by several political crises during\nthe recent years, and analyze the non-Gaussian probability density function\n(PDF) of the log returns of the stocks' prices. We show that while the average\nof the index did not fall very much over the time period of the study, its\nday-to-day fluctuations strongly increased due to the crises. Using an approach\nbased on multiplicative processes with a detrending procedure, we study the\nscale-dependence of the non-Gaussian PDFs, and show that the temporal\ndependence of their tails indicates a gradual and systematic increase in the\nprobability of the appearance of large increments in the returns on approaching\ndistinct critical time scales over which the TEPIX has exhibited maximum\nuncertainty.\n", "category": [5, 3, 3]}
{"abstract": "  We study an AMOC time series model with an abrupt change in the mean and\ndependent errors that fulfill certain mixing conditions. We obtain confidence\nintervals for the unknown change-point via bootstrapping methods.\n  Precisely we use a block bootstrap of the estimated centered error sequence.\nThen we reconstruct a sequence with a change in the mean using the same\nestimators as before. The difference between the change-point estimator of the\nresampled sequence and the one for the original sequence can be use as an\napproximation of the difference between the real change-point and its\nestimator. This enables us to construct confidence intervals using the\nempirical distribution of the resampled time series.\n  A simulation study shows that the resampled confidence intervals are usually\ncloser to their target levels and at the same time smaller than the asymptotic\nintervals.\n", "category": [2, 6]}
{"abstract": "  In this note we prove a large deviation bound on the sum of random variables\nwith the following dependency structure: there is a dependency graph $G$ with a\nbounded chromatic number, in which each vertex represents a random variable.\nVariables that are represented by neighboring vertices may be arbitrarily\ndependent, but collections of variables that form an independent set in $G$ are\n$t$-wise independent.\n", "category": [2, 2, 6]}
{"abstract": "  An explicit formula for the chaotic representation of the powers of\nincrements, (X_{t+t_0}-X_{t_0})^n, of a Levy process is presented. There are\ntwo different chaos expansions of a square integrable functional of a Levy\nprocess: one with respect to the compensated Poisson random measure and the\nother with respect to the orthogonal compensated powers of the jumps of the\nLevy process. Computationally explicit formulae for both of these chaos\nexpansions of (X_{t+t_0}-X_{t_0})^n are given in this paper. Simulation results\nverify that the representation is satisfactory. The CRP of a number of\nfinancial derivatives can be found by expressing them in terms of\n(X_{t+t_0}-X_{t_0})^n using Taylor's expansion.\n", "category": [2, 2, 6]}
{"abstract": "  We derive concentration inequalities for functions of the empirical measure\nof large random matrices with infinitely divisible entries and, in particular,\nstable ones. We also give concentration results for some other functionals of\nthese random matrices, such as the largest eigenvalue or the largest singular\nvalue.\n", "category": [2, 2, 6]}
{"abstract": "  Coherence and phase synchronization between time series corresponding to\ndifferent spatial locations are usually interpreted as indicators of the\nconnectivity between locations. In neurophysiology, time series of electric\nneuronal activity are essential for studying brain interconnectivity. Such\nsignals can either be invasively measured from depth electrodes, or computed\nfrom very high time resolution, non-invasive, extracranial recordings of scalp\nelectric potential differences (EEG: electroencephalogram) and magnetic fields\n(MEG: magnetoencephalogram) by means of a tomography such as sLORETA\n(standardized low resolution brain electromagnetic tomography). There are two\nproblems in this case. First, in the usual situation of unknown cortical\ngeometry, the estimated signal at each brain location is a vector with three\ncomponents (i.e. a current density vector), which means that coherence and\nphase synchronization must be generalized to pairs of multivariate time series.\nSecond, the inherent low spatial resolution of the EEG/MEG tomography\nintroduces artificially high zero-lag coherence and phase synchronization. In\nthis report, solutions to both problems are presented. Two additional\ngeneralizations are briefly mentioned: (1) conditional coherence and phase\nsynchronization; and (2) non-stationary time-frequency analysis. Finally, a\nnon-parametric randomization method for connectivity significance testing is\noutlined. The new connectivity measures proposed here can be applied to pairs\nof univariate EEG/MEG signals, as is traditional in the published literature.\nHowever, these calculations cannot be interpreted as connectivity, since it is\nin general incorrect to associate an extracranial electrode or sensor to the\nunderlying cortex.\n", "category": [6, 6]}
{"abstract": "  It is generally accepted that many time series of practical interest exhibit\nstrong dependence, i.e., long memory. For such series, the sample\nautocorrelations decay slowly and log-log periodogram plots indicate a\nstraight-line relationship. This necessitates a class of models for describing\nsuch behavior. A popular class of such models is the autoregressive\nfractionally integrated moving average (ARFIMA) which is a linear process.\nHowever, there is also a need for nonlinear long memory models. For example,\nseries of returns on financial assets typically tend to show zero correlation,\nwhereas their squares or absolute values exhibit long memory. Furthermore, the\nsearch for a realistic mechanism for generating long memory has led to the\ndevelopment of other nonlinear long memory models. In this chapter, we will\npresent several nonlinear long memory models, and discuss the properties of the\nmodels, as well as associated parametric andsemiparametric estimators.\n", "category": [2, 5, 6]}
{"abstract": "  The goal of this paper is to give a short and self contained proof of general\nbounds for subgeometric rates of convergence, under practical conditions. The\nmain result whose proof, based on coupling, provides an intuitive understanding\nof the results of Nummelin and Tuominen (1983) and Tuominen and Tweedie (1994).\nTo obtain practical rates, a very general drift condition, recently introduced\nin Douc et al (2004) is used.\n", "category": [2, 6]}
{"abstract": "  A general theory of innovation and progress in human society is outlined,\nbased on the combat between two opposite forces (conservatism/inertia and\nspeculative herding \"bubble\" behavior). We contend that human affairs are\ncharacterized by ubiquitous ``bubbles'', which involve huge risks which would\nnot otherwise be taken using standard cost/benefit analysis. Bubbles result\nfrom self-reinforcing positive feedbacks. This leads to explore uncharted\nterritories and niches whose rare successes lead to extraordinary discoveries\nand provide the base for the observed accelerating development of technology\nand of the economy. But the returns are very heterogeneous, very risky and may\nnot occur. In other words, bubbles, which are characteristic definitions of\nhuman activity, allow huge risks to get huge returns over large scales. We\noutline some underlying mathematical structure and a few results involving\npositive feedbacks, emergence, heavy-tailed power laws, outliers/kings/black\nswans, the problem of predictability and the illusion of control, as well as\nsome policy implications.\n", "category": [3, 5]}
{"abstract": "  The generation interval is the time between the infection time of an infected\nperson and the infection time of his or her infector. Probability density\nfunctions for generation intervals have been an important input for epidemic\nmodels and epidemic data analysis. In this paper, we specify a general\nstochastic SIR epidemic model and prove that the mean generation interval\ndecreases when susceptible persons are at risk of infectious contact from\nmultiple sources. The intuition behind this is that when a susceptible person\nhas multiple potential infectors, there is a ``race'' to infect him or her in\nwhich only the first infectious contact leads to infection. In an epidemic, the\nmean generation interval contracts as the prevalence of infection increases. We\ncall this global competition among potential infectors. When there is rapid\ntransmission within clusters of contacts, generation interval contraction can\nbe caused by a high local prevalence of infection even when the global\nprevalence is low. We call this local competition among potential infectors.\nUsing simulations, we illustrate both types of competition.\n  Finally, we show that hazards of infectious contact can be used instead of\ngeneration intervals to estimate the time course of the effective reproductive\nnumber in an epidemic. This approach leads naturally to partial likelihoods for\nepidemic data that are very similar to those that arise in survival analysis,\nopening a promising avenue of methodological research in infectious disease\nepidemiology.\n", "category": [4, 2, 6]}
{"abstract": "  Probabilistic graphical models (PGMs) have become a popular tool for\ncomputational analysis of biological data in a variety of domains. But, what\nexactly are they and how do they work? How can we use PGMs to discover patterns\nthat are biologically relevant? And to what extent can PGMs help us formulate\nnew hypotheses that are testable at the bench? This note sketches out some\nanswers and illustrates the main ideas behind the statistical approach to\nbiological pattern discovery.\n", "category": [4, 0, 3, 6, 6]}
{"abstract": "  Multifractal analysis and extensive statistical tests are performed upon\nintraday minutely data within individual trading days for four stock market\nindexes (including HSI, SZSC, S&P500, and NASDAQ) to check whether the indexes\n(instead of the returns) possess multifractality. We find that the mass\nexponent $\\tau(q)$ is linear and the singularity $\\alpha(q)$ is close to 1 for\nall trading days and all indexes. Furthermore, we find strong evidence showing\nthat the scaling behaviors of the original data sets cannot be distinguished\nfrom those of the shuffled time series. Hence, the so-called multifractality in\nthe intraday stock market indexes is merely an illusion.\n", "category": [5, 3]}
{"abstract": "  In the modern Bayesian view classical probability theory is simply an\nextension of conventional logic, i.e., a quantitative tool that allows for\nconsistent reasoning in the presence of uncertainty. Classical theory\npresupposes, however, that--at least in principle--the amount of evidence that\nan experimenter can accumulate always matches the size of the hypothesis space.\nI investigate how the framework for consistent reasoning must be modified in\nnon-classical situations where hypotheses form a continuum, yet the maximum\nevidence accessible through experiment is not allowed to exceed some finite\nupper bound. Invoking basic consistency requirements pertaining to the\npreparation and composition of systems, as well as to the continuity of\nprobabilities, I show that the modified theory must have an internal symmetry\nisomorphic to the unitary group. It thus appears that the only consistent\nalgorithm for plausible reasoning about a continuum of hypotheses on the basis\nof finite evidence is furnished by quantum theory in complex Hilbert space.\n", "category": [2, 6]}
{"abstract": "  In this work are considered some questions of Monte-Carlo modeling on\nnontrivial bundles. As a basic example is used problem of generation of\nstraight lines in 3D space, related with modeling of interaction of a solid\nbody with a flux of particles and with some other tasks. Space of lines used in\ngiven model is example of nontrivial fiber bundle, that is equivalent with\ntangent sheaf of a sphere.\n", "category": [2, 6]}
{"abstract": "  We address the question of designing isotropic analysis functions on the\nsphere which are perfectly limited in the spectral domain and optimally\nlocalized in the spatial domain. This work is motivated by the need of\nlocalized analysis tools in domains where the data is lying on the sphere,\ne.g.{} the science of the Cosmic Microwave Background. Our construction is\nderived from the localized frames introduced by Narcowich, Petrushev, Ward,\n2006. The analysis frames are optimized for given applications and compared\nnumerically using various criteria.\n", "category": [2, 2, 2, 6]}
{"abstract": "  Suppose several two-valued input-output systems are designed by setting the\nlevels of several controllable factors. For this situation, Taguchi method has\nproposed to assign the controllable factors to the orthogonal array and use\nANOVA model for the standardized SN ratio, which is a natural measure for\nevaluating the performance of each input-output system. Though this procedure\nis simple and useful in application indeed, the result can be unreliable when\nthe estimated standard errors of the standardized SN ratios are unbalanced. In\nthis paper, we treat the data arising from the full factorial or fractional\nfactorial designs of several controllable factors as the frequencies of\nhigh-dimensional contingency tables, and propose a general testing procedure\nfor the main effects or the interaction effects of the controllable factors.\n", "category": [6]}
{"abstract": "  We consider the quadratic family of maps given by $f_{a}(x)=1-a x^2$ with\n$x\\in [-1,1]$, where $a$ is a Benedicks-Carleson parameter. For each of these\nchaotic dynamical systems we study the extreme value distribution of the\nstationary stochastic processes $X_0,X_1,...$, given by $X_{n}=f_a^n$, for\nevery integer $n\\geq0$, where each random variable $X_n$ is distributed\naccording to the unique absolutely continuous, invariant probability of $f_a$.\nUsing techniques developed by Benedicks and Carleson, we show that the limiting\ndistribution of $M_n=\\max\\{X_0,...,X_{n-1}\\}$ is the same as that which would\napply if the sequence $X_0,X_1,...$ was independent and identically\ndistributed. This result allows us to conclude that the asymptotic distribution\nof $M_n$ is of Type III (Weibull).\n", "category": [2, 2, 2, 6]}
{"abstract": "  Adaptive populations such as those in financial markets and distributed\ncontrol can be modeled by the Minority Game. We consider how their dynamics\ndepends on the agents' initial preferences of strategies, when the agents use\nlinear or quadratic payoff functions to evaluate their strategies. We find that\nthe fluctuations of the population making certain decisions (the volatility)\ndepends on the diversity of the distribution of the initial preferences of\nstrategies. When the diversity decreases, more agents tend to adapt their\nstrategies together. In systems with linear payoffs, this results in dynamical\ntransitions from vanishing volatility to a non-vanishing one. For low signal\ndimensions, the dynamical transitions for the different signals do not take\nplace at the same critical diversity. Rather, a cascade of dynamical\ntransitions takes place when the diversity is reduced. In contrast, no phase\ntransitions are found in systems with the quadratic payoffs. Instead, a basin\nboundary of attraction separates two groups of samples in the space of the\nagents' decisions. Initial states inside this boundary converge to small\nvolatility, while those outside diverge to a large one. Furthermore, when the\npreference distribution becomes more polarized, the dynamics becomes more\nerratic. All the above results are supported by good agreement between\nsimulations and theory.\n", "category": [5, 3]}
{"abstract": "  Conformal prediction uses past experience to determine precise levels of\nconfidence in new predictions. Given an error probability $\\epsilon$, together\nwith a method that makes a prediction $\\hat{y}$ of a label $y$, it produces a\nset of labels, typically containing $\\hat{y}$, that also contains $y$ with\nprobability $1-\\epsilon$. Conformal prediction can be applied to any method for\nproducing $\\hat{y}$: a nearest-neighbor method, a support-vector machine, ridge\nregression, etc.\n  Conformal prediction is designed for an on-line setting in which labels are\npredicted successively, each one being revealed before the next is predicted.\nThe most novel and valuable feature of conformal prediction is that if the\nsuccessive examples are sampled independently from the same distribution, then\nthe successive predictions will be right $1-\\epsilon$ of the time, even though\nthey are based on an accumulating dataset rather than on independent datasets.\n  In addition to the model under which successive examples are sampled\nindependently, other on-line compression models can also use conformal\nprediction. The widely used Gaussian linear model is one of these.\n  This tutorial presents a self-contained account of the theory of conformal\nprediction and works through several numerical examples. A more comprehensive\ntreatment of the topic is provided in \"Algorithmic Learning in a Random World\",\nby Vladimir Vovk, Alex Gammerman, and Glenn Shafer (Springer, 2005).\n", "category": [0, 6]}
{"abstract": "  In this paper, a geometric function is introduced to reflect the attenuation\nspeed of impact of one firm's default to its partner. If two firms are\ncompetitions (copartners), the default intensity of one firm will decrease\n(increase) abruptly when the other firm defaults. As time goes on, the impact\nwill decrease gradually until extinct. In this model, the joint distribution\nand marginal distributions of default times are derived by employing the change\nof measure, so can we value the fair swap premium of a CDS.\n", "category": [5, 2]}
{"abstract": "  This paper deals with the filtering problem for a class of discrete time\nstochastic volatility models in which the disturbances have rational\nprobability density functions. This includes the Cauchy distributions and\nStudent t-distributions with odd number of degrees of freedom. Using state\nspace realizations to represent the rational probability density functions we\nare able to solve the filtering problem exactly. However the size of the\ninvolved state space matrices grows exponentially with each time step of the\nfilter. Therefore we use stochastically balanced truncation techniques to\napproximate the high order rational functions involved. In a simulation study\nwe show the applicability of this approach. In addition a simple method of\nmoments estimator is derived.\n", "category": [2, 2, 6]}
{"abstract": "  In this paper, we consider the problem of partitioning a small data sample\ndrawn from a mixture of $k$ product distributions. We are interested in the\ncase that individual features are of low average quality $\\gamma$, and we want\nto use as few of them as possible to correctly partition the sample. We analyze\na spectral technique that is able to approximately optimize the total data\nsize--the product of number of data points $n$ and the number of features\n$K$--needed to correctly perform this partitioning as a function of $1/\\gamma$\nfor $K>n$. Our goal is motivated by an application in clustering individuals\naccording to their population of origin using markers, when the divergence\nbetween any two of the populations is small.\n", "category": [6, 6]}
{"abstract": "  We present a novel solution technique for the blind subspace deconvolution\n(BSSD) problem, where temporal convolution of multidimensional hidden\nindependent components is observed and the task is to uncover the hidden\ncomponents using the observation only. We carry out this task for the\nundercomplete case (uBSSD): we reduce the original uBSSD task via linear\nprediction to independent subspace analysis (ISA), which we can solve. As it\nhas been shown recently, applying temporal concatenation can also reduce uBSSD\nto ISA, but the associated ISA problem can easily become `high dimensional'\n[1]. The new reduction method circumvents this dimensionality problem. We\nperform detailed studies on the efficiency of the proposed technique by means\nof numerical simulations. We have found several advantages: our method can\nachieve high quality estimations for smaller number of samples and it can cope\nwith deeper temporal convolutions.\n", "category": [6]}
{"abstract": "  State Space Models (SSM) is a MATLAB 7.0 software toolbox for doing time\nseries analysis by state space methods. The software features fully interactive\nconstruction and combination of models, with support for univariate and\nmultivariate models, complex time-varying (dynamic) models, non-Gaussian\nmodels, and various standard models such as ARIMA and structural time-series\nmodels. The software includes standard functions for Kalman filtering and\nsmoothing, simulation smoothing, likelihood evaluation, parameter estimation,\nsignal extraction and forecasting, with incorporation of exact initialization\nfor filters and smoothers, and support for missing observations and multiple\ntime series input with common analysis structure. The software also includes\nimplementations of TRAMO model selection and Hillmer-Tiao decomposition for\nARIMA models. The software will provide a general toolbox for doing time series\nanalysis on the MATLAB platform, allowing users to take advantage of its\nreadily available graph plotting and general matrix computation capabilities.\n", "category": [6, 6]}
{"abstract": "  The distance metric plays an important role in nearest neighbor (NN)\nclassification. Usually the Euclidean distance metric is assumed or a\nMahalanobis distance metric is optimized to improve the NN performance. In this\npaper, we study the problem of embedding arbitrary metric spaces into a\nEuclidean space with the goal to improve the accuracy of the NN classifier. We\npropose a solution by appealing to the framework of regularization in a\nreproducing kernel Hilbert space and prove a representer-like theorem for NN\nclassification. The embedding function is then determined by solving a\nsemidefinite program which has an interesting connection to the soft-margin\nlinear binary support vector machine classifier. Although the main focus of\nthis paper is to present a general, theoretical framework for metric embedding\nin a NN setting, we demonstrate the performance of the proposed method on some\nbenchmark datasets and show that it performs better than the Mahalanobis metric\nlearning algorithm in terms of leave-one-out and generalization errors.\n", "category": [6]}
{"abstract": "  Consider a set of order statistics that arise from sorting samples from two\ndifferent populations, each with their own, possibly different distribution\nfunction. The probability that these order statistics fall in disjoint, ordered\nintervals, and that of the smallest statistics, a certain number come from the\nfirst populations, are given in terms of the two distribution functions. The\nresult is applied to computing the joint probability of the number of\nrejections and the number of false rejections for the Benjamini-Hochberg false\ndiscovery rate procedure.\n", "category": [6, 2, 2, 6]}
{"abstract": "  Based on criteria of mathematical simplicity and consistency with empirical\nmarket data, a model with volatility driven by fractional noise has been\nconstructed which provides a fairly accurate mathematical parametrization of\nthe data. Here, some features of the model are discussed and, using agent-based\nmodels, one tries to find which agent strategies and (or) properties of the\nfinancial institutions might be responsible for the features of the fractional\nvolatility model.\n", "category": [5, 3, 3]}
{"abstract": "  In this paper we consider nonparametric estimation for dependent data, where\nthe observations do not necessarily come from a linear process. We study\ndensity estimation and also discuss associated problems in nonparametric\nregression using the 2-mixing dependence measure. We compare the results under\n2-mixing with those derived under the assumption that the process is linear. In\nthe context of panel time series where one observes data from several\nindividuals, it is often too strong to assume the joint linearity of processes.\nInstead the methods developed in this paper enable us to quantify the\ndependence through 2-mixing which allows for nonlinearity. We propose an\nestimator of the panel mean function and obtain its rate of convergence. We\nshow that under certain conditions the rate of convergence can be improved by\nallowing the number of individuals in the panel to increase with time.\n", "category": [2, 6]}
{"abstract": "  We introduce a multiscale test statistic based on local order statistics and\nspacings that provides simultaneous confidence statements for the existence and\nlocation of local increases and decreases of a density or a failure rate. The\nprocedure provides guaranteed finite-sample significance levels, is easy to\nimplement and possesses certain asymptotic optimality and adaptivity\nproperties.\n", "category": [2, 6]}
{"abstract": "  This paper gives a method for computing distributions associated with\npatterns in the state sequence of a hidden Markov model, conditional on\nobserving all or part of the observation sequence. Probabilities are computed\nfor very general classes of patterns (competing patterns and generalized later\npatterns), and thus, the theory includes as special cases results for a large\nclass of problems that have wide application. The unobserved state sequence is\nassumed to be Markovian with a general order of dependence. An auxiliary Markov\nchain is associated with the state sequence and is used to simplify the\ncomputations. Two examples are given to illustrate the use of the methodology.\nWhereas the first application is more to illustrate the basic steps in applying\nthe theory, the second is a more detailed application to DNA sequences, and\nshows that the methods can be adapted to include restrictions related to\nbiological knowledge.\n", "category": [6, 6, 6]}
{"abstract": "  This paper treats the problem of detecting periodicity in a sequence of\nphoton arrival times, which occurs, for example, in attempting to detect\ngamma-ray pulsars. A particular focus is on how auxiliary information,\ntypically source intensity, background intensity, and incidence angles and\nenergies associated with each photon arrival should be used to maximize the\ndetection power. We construct a class of likelihood-based tests, score tests,\nwhich give rise to event weighting in a principled and natural way, and derive\nexpressions quantifying the power of the tests. These results can be used to\ncompare the efficacies of different weight functions, including cuts in energy\nand incidence angle. The test is targeted toward a template for the periodic\nlightcurve, and we quantify how deviation from that template affects the power\nof detection.\n", "category": [6, 6]}
{"abstract": "  The affine rank minimization problem consists of finding a matrix of minimum\nrank that satisfies a given system of linear equality constraints. Such\nproblems have appeared in the literature of a diverse set of fields including\nsystem identification and control, Euclidean embedding, and collaborative\nfiltering. Although specific instances can often be solved with specialized\nalgorithms, the general affine rank minimization problem is NP-hard. In this\npaper, we show that if a certain restricted isometry property holds for the\nlinear transformation defining the constraints, the minimum rank solution can\nbe recovered by solving a convex optimization problem, namely the minimization\nof the nuclear norm over the given affine space. We present several random\nensembles of equations where the restricted isometry property holds with\noverwhelming probability. The techniques used in our analysis have strong\nparallels in the compressed sensing framework. We discuss how affine rank\nminimization generalizes this pre-existing concept and outline a dictionary\nrelating concepts from cardinality minimization to those of rank minimization.\n", "category": [2, 2, 6]}
{"abstract": "  In a recent paper, we analyzed the properties of a new kind of spherical\nwavelets (called needlets) for statistical inference procedures on spherical\nrandom fields; the investigation was mainly motivated by applications to\ncosmological data. In the present work, we exploit the asymptotic uncorrelation\nof random needlet coefficients at fixed angular distances to construct\nsubsampling statistics evaluated on Voronoi cells on the sphere. We illustrate\nhow such statistics can be used for isotropy tests and for bootstrap estimation\nof nuisance parameters, even when a single realization of the spherical random\nfield is observed. The asymptotic theory is developed in detail in the high\nresolution sense.\n", "category": [2, 2, 6]}
{"abstract": "  Smoothing methods and SiZer are a useful statistical tool for discovering\nstatistically significant structure in data. Based on scale space ideas\noriginally developed in the computer vision literature, SiZer (SIgnificant ZERo\ncrossing of the derivatives) is a graphical device to assess which observed\nfeatures are `really there' and which are just spurious sampling artifacts. In\nthis paper, we develop SiZer like ideas in time series analysis to address the\nimportant issue of significance of trends. This is not a straightforward\nextension, since one data set does not contain the information needed to\ndistinguish `trend' from `dependence'. A new visualization is proposed, which\nshows the statistician the range of trade-offs that are available. Simulation\nand real data results illustrate the effectiveness of the method.\n", "category": [6]}
{"abstract": "  In this work, based on a realization of an inhomogeneous Poisson process\nwhose intensity function depends on a real unknown parameter, we consider a\nsimple hypothesis against a sequence of close (contiguous) alternatives. Under\ncertain regularity conditions we obtain the power loss of the score test with\nrespect to the Neyman-Pearson test. The power loss measures the performance of\na second order efficient test by the help of third order asymptotic properties\nof the problem under consideration.\n", "category": [2, 6]}
{"abstract": "  We improve the inequality used in Pronzato [2003. Removing non-optimal\nsupport points in D-optimum design algorithms. Statist. Probab. Lett. 63,\n223-228] to remove points from the design space during the search for a\n$D$-optimum design. Let $\\xi$ be any design on a compact space $\\mathcal{X}\n\\subset \\mathbb{R}^m$ with a nonsingular information matrix, and let\n$m+\\epsilon$ be the maximum of the variance function $d(\\xi,\\mathbf{x})$ over\nall $\\mathbf{x} \\in \\mathcal{X}$. We prove that any support point\n$\\mathbf{x}_{*}$ of a $D$-optimum design on $\\mathcal{X}$ must satisfy the\ninequality $d(\\xi,\\mathbf{x}_{*}) \\geq\nm(1+\\epsilon/2-\\sqrt{\\epsilon(4+\\epsilon-4/m)}/2)$. We show that this new lower\nbound on $d(\\xi,\\mathbf{x}_{*})$ is, in a sense, the best possible, and how it\ncan be used to accelerate algorithms for $D$-optimum design.\n", "category": [2, 6]}
{"abstract": "  This paper gives a critical account of the minority game literature. The\nminority game is a simple congestion game: players need to choose between two\noptions, and those who have selected the option chosen by the minority win. The\nlearning model proposed in this literature seems to differ markedly from the\nlearning models commonly used in economics. We relate the learning model from\nthe minority game literature to standard game-theoretic learning models, and\nshow that in fact it shares many features with these models. However, the\npredictions of the learning model differ considerably from the predictions of\nmost other learning models. We discuss the main predictions of the learning\nmodel proposed in the minority game literature, and compare these to\nexperimental findings on congestion games.\n", "category": [5, 3]}
{"abstract": "  In this paper, we present a unified approach to function approximation in\nreproducing kernel Hilbert spaces (RKHS) that establishes a previously\nunrecognized optimality property for several well-known function approximation\ntechniques, such as minimum-norm interpolation, smoothing splines, and\npseudo-inverses. We consider the problem of approximating a function belonging\nto an arbitrary real-valued RKHS on R^d based on approximate observations of\nthe function. The observations are approximate in the sense that the actual\nobservations (i.e., the true function values) are known only to belong to a\nconvex set of admissible observations. We seek a minimax optimal approximation\nfor the function that minimizes the supremum of the RKHS norm on the error\nbetween the true function and the chosen approximation subject only to the\nconditions that the true function belongs to a uniformly bounded uncertainty\nset of functions that satisfy the constraints on the observations and that the\napproximation is a member of the RKHS. We refer to such a solution as a minimax\nrobust reconstruction. We characterize the solution to the minimax robust\nreconstruction problem and show that it is equivalent to solving a\nstraightforward convex optimization problem. We demonstrate that a minimax\nrobust reconstruction will generally be more stable than an approximation based\non interpolation through a nominal set of observations and that, subject to\nsome mild regularity conditions on the convex set of admissible observations,\nthe minimax robust reconstruction is unconditionally stable. We motivate our\nresults by characterizing the minimax robust reconstruction for several\nspecific convex observational models and discuss relationships with other\napproaches to function approximation.\n", "category": [2, 6]}
{"abstract": "  This is an expos\\'e on the use of O'Sullivan penalised splines in\ncontemporary semiparametric regression, including mixed model and Bayesian\nformulations. O'Sullivan penalised splines are similar to P-splines, but have\nan advantage of being a direct generalisation of smoothing splines. Exact\nexpressions for the O'Sullivan penalty matrix are obtained. Comparisons between\nthe two reveals that O'Sullivan penalised splines more closely mimic the\nnatural boundary behaviour of smoothing splines. Implementation in modern\ncomputing environments such as Matlab, R and BUGS is discussed.\n", "category": [6]}
{"abstract": "  The computation of the Tukey depth, also called halfspace depth, is very\ndemanding, even in low dimensional spaces, because it requires the\nconsideration of all possible one-dimensional projections. In this paper we\npropose a random depth which approximates the Tukey depth. It only takes into\naccount a finite number of one-dimensional projections which are chosen at\nrandom. Thus, this random depth requires a very small computation time even in\nhigh dimensional spaces. Moreover, it is easily extended to cover the\nfunctional framework.\n  We present some simulations indicating how many projections should be\nconsidered depending on the sample size and on the dimension of the sample\nspace. We also compare this depth with some others proposed in the literature.\nIt is noteworthy that the random depth, based on a very low number of\nprojections, obtains results very similar to those obtained with other depths.\n", "category": [6]}
{"abstract": "  We present in this paper a new tool for outliers detection in the context of\nmultiple regression models. This graphical tool is based on recursive\nestimation of the parameters. Simulations were carried out to illustrate the\nperformance of this graphical procedure. As a conclusion, this tool is applied\nto real data containing outliers according to the classical available tools.\n", "category": [6]}
{"abstract": "  In this paper we prove the Local Asymptotic Mixed Normality (LAMN) property\nfor the statistical model given by the observation of local means of a\ndiffusion process $X$. Our data are given by $ \\int_0^1 X_{\\frac{s+i}{n}} \\dd\n\\mu (s)$ for $i=0,...,n-1$ and the unknown parameter appears in the diffusion\ncoefficient of the process $X$ only. Although the data are nor Markovian\nneither Gaussian we can write down, with help of Malliavin calculus, an\nexplicit expression for the log-likelihood of the model, and then study the\nasymptotic expansion. We actually find that the asymptotic information of this\nmodel is the same one as for a usual discrete sampling of $X$.\n", "category": [2, 2, 6]}
{"abstract": "  The paper studies large sample asymptotic properties of the Maximum\nLikelihood Estimator (MLE) for the parameter of a continuous time Markov chain,\nobserved in white noise. Using the method of weak convergence of likelihoods\ndue to I.Ibragimov and R.Khasminskii, consistency, asymptotic normality and\nconvergence of moments are established for MLE under certain strong ergodicity\nconditions of the chain.\n", "category": [2, 2, 6]}
{"abstract": "  In this paper, we propose a statistical theory on measurement and estimation\nof Rayleigh fading channels in wireless communications and provide complete\nsolutions to the fundamental problems: What is the optimum estimator for the\nstatistical parameters associated with the Rayleigh fading channel, and how\nmany measurements are sufficient to estimate these parameters with the\nprescribed margin of error and confidence level? Our proposed statistical\ntheory suggests that two testing signals of different strength be used. The\nmaximum likelihood (ML) estimator is obtained for estimation of the statistical\nparameters of the Rayleigh fading channel that is both sufficient and complete\nstatistic. Moreover, the ML estimator is the minimum variance (MV) estimator\nthat in fact achieves the Cramer-Rao lower bound.\n", "category": [2, 2, 6, 6]}
{"abstract": "  In most papers establishing consistency for learning algorithms it is assumed\nthat the observations used for training are realizations of an i.i.d. process.\nIn this paper we go far beyond this classical framework by showing that support\nvector machines (SVMs) essentially only require that the data-generating\nprocess satisfies a certain law of large numbers. We then consider the\nlearnability of SVMs for $\\a$-mixing (not necessarily stationary) processes for\nboth classification and regression, where for the latter we explicitly allow\nunbounded noise.\n", "category": [6, 6]}
{"abstract": "  We consider the problem of forecasting the next (observable) state of an\nunknown ergodic dynamical system from a noisy observation of the present state.\nOur main result shows, for example, that support vector machines (SVMs) using\nGaussian RBF kernels can learn the best forecaster from a sequence of noisy\nobservations if (a) the unknown observational noise process is bounded and has\na summable $\\alpha$-mixing rate and (b) the unknown ergodic dynamical system is\ndefined by a Lipschitz continuous function on some compact subset of\n$\\mathbb{R}^d$ and has a summable decay of correlations for Lipschitz\ncontinuous functions. In order to prove this result we first establish a\ngeneral consistency result for SVMs and all stochastic processes that satisfy a\nmixing notion that is substantially weaker than $\\alpha$-mixing.\n", "category": [6, 2, 2, 6]}
{"abstract": "  In this paper we review our earlier work on quantum computing and the Nash\nEquilibrium, in particular, tracing the history of the discovery of new Nash\nEquilibria and then reviewing the ways in which quantum computing may be\nexpected to generate new classes of Nash equilibria. We then extend this work\nthrough a substantive analysis of examples provided by Meyer, Flitney, Iqbal\nand Weigert and Cheon and Tsutsui with respect to quantized games, quantum game\nstrategies and the extension of Nash Equilibrium to solvable games in Hilbert\nspace. Finally, we review earlier work by Sato, Taiji and Ikegami on non-linear\ncomputation and computational classes by way of reference to coherence,\ndecoherence and quantum computating systems.\n", "category": [5, 3, 3, 5]}
{"abstract": "  Let \\svec = (s_1,...,s_m) and \\tvec = (t_1,...,t_n) be vectors of nonnegative\ninteger-valued functions of m,n with equal sum S = sum_{i=1}^m s_i =\nsum_{j=1}^n t_j. Let M(\\svec,\\tvec) be the number of m*n matrices with\nnonnegative integer entries such that the i-th row has row sum s_i and the j-th\ncolumn has column sum t_j for all i,j. Such matrices occur in many different\nsettings, an important example being the contingency tables (also called\nfrequency tables) important in statistics. Define s=max_i s_i and t=max_j t_j.\nPrevious work has established the asymptotic value of M(\\svec,\\tvec) as\nm,n\\to\\infty with s and t bounded (various authors independently, 1971-1974),\nand when \\svec,\\tvec are constant vectors with m/n,n/m,s/n >= c/log n for\nsufficiently large (Canfield and McKay, 2007). In this paper we extend the\nsparse range to the case st=o(S^(2/3)). The proof in part follows a previous\nasymptotic enumeration of 0-1 matrices under the same conditions (Greenhill,\nMcKay and Wang, 2006). We also generalise the enumeration to matrices over any\nsubset of the nonnegative integers that includes 0 and 1.\n", "category": [2, 2, 6]}
{"abstract": "  The understanding of complex social or economic systems is an important\nscientific challenge. Here we present a comprehensive study of the Spanish\nStock Exchange showing that most financial firms trading in that market are\ncharacterized by a resulting strategy and can be classified in groups of firms\nwith different specialization. Few large firms overally act as trending firms\nwhereas many heterogeneous firm act as reversing firms. The herding properties\nof these two groups are markedly different and consistently observed over a\nfour-year period of trading.\n", "category": [5, 3]}
{"abstract": "  Probability models are proposed for passage time data collected in\nexperiments with a device designed to measure particle flow during aerial\napplication of fertilizer. Maximum likelihood estimation of flow intensity is\nreviewed for the simple linear Boolean model, which arises with the assumption\nthat each particle requires the same known passage time. M-estimation is\ndeveloped for a generalization of the model in which passage times behave as a\nrandom sample from a distribution with a known mean. The generalized model\nimproves fit in these experiments. An estimator of total particle flow is\nconstructed by conditioning on lengths of multi-particle clumps.\n", "category": [6]}
{"abstract": "  In many modern applications, including analysis of gene expression and text\ndocuments, the data are noisy, high-dimensional, and unordered--with no\nparticular meaning to the given order of the variables. Yet, successful\nlearning is often possible due to sparsity: the fact that the data are\ntypically redundant with underlying structures that can be represented by only\na few features. In this paper we present treelets--a novel construction of\nmulti-scale bases that extends wavelets to nonsmooth signals. The method is\nfully adaptive, as it returns a hierarchical tree and an orthonormal basis\nwhich both reflect the internal structure of the data. Treelets are especially\nwell-suited as a dimensionality reduction and feature selection tool prior to\nregression and classification, in situations where sample sizes are small and\nthe data are sparse with unknown groupings of correlated or collinear\nvariables. The method is also simple to implement and analyze theoretically.\nHere we describe a variety of situations where treelets perform better than\nprincipal component analysis, as well as some common variable selection and\ncluster averaging schemes. We illustrate treelets on a blocked covariance model\nand on several data sets (hyperspectral image data, DNA microarray data, and\ninternet advertisements) with highly complex dependencies between variables.\n", "category": [6]}
{"abstract": "  In the context of multiple hypotheses testing, the proportion $\\pi_0$ of true\nnull hypotheses in the pool of hypotheses to test often plays a crucial role,\nalthough it is generally unknown a priori. A testing procedure using an\nimplicit or explicit estimate of this quantity in order to improve its\nefficency is called adaptive. In this paper, we focus on the issue of False\nDiscovery Rate (FDR) control and we present new adaptive multiple testing\nprocedures with control of the FDR. First, in the context of assuming\nindependent $p$-values, we present two new procedures and give a unified review\nof other existing adaptive procedures that have provably controlled FDR. We\nreport extensive simulation results comparing these procedures and testing\ntheir robustness when the independence assumption is violated. The new proposed\nprocedures appear competitive with existing ones. The overall best, though, is\nreported to be Storey's estimator, but for a parameter setting that does not\nappear to have been considered before. Second, we propose adaptive versions of\nstep-up procedures that have provably controlled FDR under positive dependences\nand unspecified dependences of the $p$-values, respectively. While simulations\nonly show an improvement over non-adaptive procedures in limited situations,\nthese are to our knowledge among the first theoretically founded adaptive\nmultiple testing procedures that control the FDR when the $p$-values are not\nindependent.\n", "category": [2, 6]}
{"abstract": "  In climate studies, detecting spatial patterns that largely deviate from the\nsample mean still remains a statistical challenge. Although a Principal\nComponent Analysis (PCA), or equivalently a Empirical Orthogonal Functions\n(EOF) decomposition, is often applied on this purpose, it can only provide\nmeaningful results if the underlying multivariate distribution is Gaussian.\nIndeed, PCA is based on optimizing second order moments quantities and the\ncovariance matrix can only capture the full dependence structure for\nmultivariate Gaussian vectors. Whenever the application at hand can not satisfy\nthis normality hypothesis (e.g. precipitation data), alternatives and/or\nimprovements to PCA have to be developed and studied. To go beyond this second\norder statistics constraint that limits the applicability of the PCA, we take\nadvantage of the cumulant function that can produce higher order moments\ninformation. This cumulant function, well-known in the statistical literature,\nallows us to propose a new, simple and fast procedure to identify spatial\npatterns for non-Gaussian data. Our algorithm consists in maximizing the\ncumulant function. To illustrate our approach, its implementation for which\nexplicit computations are obtained is performed on three family of of\nmultivariate random vectors. In addition, we show that our algorithm\ncorresponds to selecting the directions along which projected data display the\nlargest spread over the marginal probability density tails.\n", "category": [2, 6]}
{"abstract": "  A two-sex Basic Reproduction Number (BRN) is used to investigate the\nconditions under which the Human Immunodeficiency Virus (HIV) may spread\nthrough heterosexual contacts in Sub-Saharan Africa. (The BRN is the expected\nnumber of new infections generated by one infected individual; the disease\nspreads if the BRN is larger than 1). A simple analytical expression for the\nBRN is derived on the basis of recent data on survival rates, transmission\nprobabilities, and levels of sexual activity. Baseline results show that in the\npopulation at large (characterized by equal numbers of men and women) the BRN\nis larger than 1 if every year each person has 82 sexual contacts with\ndifferent partners. the BRN is also larger than 1 for commercial sex workers\n(CSWs) and their clients (two populations of different sizes) if each CSW has\nabout 256 clients per year and each client visits one CSW every two weeks. A\nsensitivity analysis explores the effect on the BRN of a doubling (or a\nhalving) of the transmission probabilities. Implications and extensions are\ndiscussed.\n", "category": [6]}
{"abstract": "  In this short note I apply the methodology of game-theoretic probability to\ncalculating non-asymptotic confidence intervals for the coefficient of a simple\nfirst order scalar autoregressive model. The most distinctive feature of the\nproposed procedure is that with high probability it produces confidence\nintervals that always cover the true parameter value when applied sequentially.\n", "category": [2, 6, 6]}
{"abstract": "  In this article, we derive a new generalization of Chebyshev inequality for\nrandom vectors. We demonstrate that the new generalization is much less\nconservative than the classical generalization.\n", "category": [2, 0, 2, 6, 6]}
{"abstract": "  This paper addresses the issues of conservativeness and computational\ncomplexity of probabilistic robustness analysis. We solve both issues by\ndefining a new sampling strategy and robustness measure. The new measure is\nshown to be much less conservative than the existing one. The new sampling\nstrategy enables the definition of efficient hierarchical sample reuse\nalgorithms that reduce significantly the computational complexity and make it\nindependent of the dimension of the uncertainty space. Moreover, we show that\nthere exists a one to one correspondence between the new and the existing\nrobustness measures and provide a computationally simple algorithm to derive\none from the other.\n", "category": [6, 2]}
{"abstract": "  It is becoming increasingly apparent that probabilistic approaches can\novercome conservatism and computational complexity of the classical worst-case\ndeterministic framework and may lead to designs that are actually safer. In\nthis paper we argue that a comprehensive probabilistic robustness analysis\nrequires a detailed evaluation of the robustness function and we show that such\nevaluation can be performed with essentially any desired accuracy and\nconfidence using algorithms with complexity linear in the dimension of the\nuncertainty space. Moreover, we show that the average memory requirements of\nsuch algorithms are absolutely bounded and well within the capabilities of\ntoday's computers.\n  In addition to efficiency, our approach permits control over statistical\nsampling error and the error due to discretization of the uncertainty radius.\nFor a specific level of tolerance of the discretization error, our techniques\nprovide an efficiency improvement upon conventional methods which is inversely\nproportional to the accuracy level; i.e., our algorithms get better as the\ndemands for accuracy increase.\n", "category": [6, 2, 6]}
{"abstract": "  In this paper, we derive an explicit formula for constructing the confidence\ninterval of binomial parameter with guaranteed coverage probability. The\nformula overcomes the limitation of normal approximation which is asymptotic in\nnature and thus inevitably introduce unknown errors in applications. Moreover,\nthe formula is very tight in comparison with classic Clopper-Pearson's approach\nfrom the perspective of interval width. Based on the rigorous formula, we also\nobtain approximate formulas with excellent performance of coverage probability.\n", "category": [2, 2, 6, 6]}
{"abstract": "  Notions of Darwinian selection have been implicit in economic theory for at\nleast sixty years. Richard Nelson and Sidney Winter have argued that while\nevolutionary thinking was prevalent in prewar economics, the postwar\nNeoclassical school became almost entirely preoccupied with equilibrium\nconditions and their mathematical conditions. One of the problems with the\neconomic interpretation of firm selection through competition has been a weak\ngrasp on an incomplete scientific paradigm. As I.F. Price notes, \"The\nbiological metaphor has long lurked in the background of management theory\nlargely because the message of 'survival of the fittest' (usually wrongly\nattributed to Charles Darwin rather than Herbert Spencer) provides a seemingly\nnatural model for market competition (e.g. Alchian 1950, Merrell 1984,\nHenderson 1989, Moore 1993), without seriously challenging the underlying\nparadigms of what an organisation is.\" In this paper we examine the application\nof dynamic fitness landscape models to economic theory, particularly the theory\nof technology substitution, drawing on recent work by Kauffman, Arthur,\nMcKelvey, Nelson and Winter, and Windrum and Birchenhall. In particular we use\nProfessor Post's early work with John Holland on the genetic algorithm to\nexplain some of the key differences between static and dynamic approaches to\neconomic modeling.\n", "category": [5, 3]}
{"abstract": "  We consider testing statistical hypotheses about densities of signals in\ndeconvolution models. A new approach to this problem is proposed. We\nconstructed score tests for the deconvolution with the known noise density and\nefficient score tests for the case of unknown density. The tests are\nincorporated with model selection rules to choose reasonable model dimensions\nautomatically by the data. Consistency of the tests is proved.\n", "category": [2, 6, 6]}
{"abstract": "  Although both systems analyzed are described through two theories apparently\ndifferent (quantum mechanics and game theory) it is shown that both are\nanalogous and thus exactly equivalents. The quantum analogue of the replicator\ndynamics is the von Neumann equation. Quantum mechanics could be used to\nexplain more correctly biological and economical processes. It could even\nencloses theories like games and evolutionary dynamics. We can take some\nconcepts and definitions from quantum mechanics and physics for the best\nunderstanding of the behavior of economics and biology. Also, we could maybe\nunderstand nature like a game in where its players compete for a common welfare\nand the equilibrium of the system that they are members. All the members of our\nsystem will play a game in which its maximum payoff is the equilibrium of the\nsystem. They act as a whole besides individuals like they obey a rule in where\nthey prefer to work for the welfare of the collective besides the individual\nwelfare. A system where its members are in Nash Equilibrium (or ESS) is exactly\nequivalent to a system in a maximum entropy state. A system is stable only if\nit maximizes the welfare of the collective above the welfare of the individual.\nIf it is maximized the welfare of the individual above the welfare of the\ncollective the system gets unstable an eventually collapses. The results of\nthis work shows that the \"globalization\" process has a behavior exactly\nequivalent to a system that is tending to a maximum entropy state and predicts\nthe apparition of big common markets and strong common currencies that will\nfind its \"equilibrium\" by decreasing its number until they get a state\ncharacterized by only one common currency and only one common market around the\nworld.\n", "category": [5, 3]}
{"abstract": "  We investigate the behavior of the Shanghai Stock Exchange Composite (SSEC)\nindex for the period from 1990:12 to 2007:06 using an unconstrained two-regime\nthreshold autoregressive (TAR) model with an unit root developed by Caner and\nHansen. The method allows us to simultaneously consider non-stationarity and\nnonlinearity in financial time series. Our finding indicates that the Shanghai\nstock market exhibits nonlinear behavior with two regimes and has unit roots in\nboth regimes. The important implications of the threshold effect in stock\nmarkets are also discussed.\n", "category": [5, 3, 3]}
{"abstract": "  Social interactions and personal tastes shape our consumption behavior of\ncultural products. In this study, we present a computational model of a\ncultural market and we aim to analyze the behavior of the consumer population\nas an emergent phenomena. Our results suggest that the final market shares of\ncultural products dramatically depend on consumer heterogeneity and social\ninteraction pressure. Furthermore, the relation between the resulting market\nshares and social interaction is robust with respect to a wide range of\nvariation in the parameter values and the type of topology.\n", "category": [5, 0, 3, 3]}
{"abstract": "  This paper studies a portfolio optimization problem in a discrete-time\nMarkovian model of a financial market, in which asset price dynamics depend on\nan external process of economic factors. There are transaction costs with a\nstructure that covers, in particular, the case of fixed plus proportional\ncosts. We prove that there exists a self-financing trading strategy maximizing\nthe average growth rate of the portfolio wealth. We show that this strategy has\na Markovian form. Our result is obtained by large deviations estimates on\nempirical measures of the price process and by a generalization of the\nvanishing discount method to discontinuous transition operators.\n", "category": [5, 2, 2]}
{"abstract": "  In the present work we investigate the multiscale nature of the correlations\nfor high frequency data (1 minute) in different futures markets over a period\nof two years, starting on the 1st of January 2003 and ending on the 31st of\nDecember 2004. In particular, by using the concept of \"local\" Hurst exponent,\nwe point out how the behaviour of this parameter, usually considered as a\nbenchmark for persistency/antipersistency recognition in time series, is\nlargely time-scale dependent in the market context. These findings are a direct\nconsequence of the intrinsic complexity of a system where trading strategies\nare scale-adaptive. Moreover, our analysis points out different regimes in the\ndynamical behaviour of the market indices under consideration.\n", "category": [5, 3, 3, 3]}
{"abstract": "  We set up a structural model to study credit risk for a portfolio containing\nseveral or many credit contracts. The model is based on a jump--diffusion\nprocess for the risk factors, i.e. for the company assets. We also include\ncorrelations between the companies. We discuss that models of this type have\nmuch in common with other problems in statistical physics and in the theory of\ncomplex systems. We study a simplified version of our model analytically.\nFurthermore, we perform extensive numerical simulations for the full model. The\nobservables are the loss distribution of the credit portfolio, its moments and\nother quantities derived thereof. We compile detailed information about the\nparameter dependence of these observables. In the course of setting up and\nanalyzing our model, we also give a review of credit risk modeling for a\nphysics audience.\n", "category": [5, 3, 3, 5]}
{"abstract": "  Obtaining more accurate equity value estimates is the starting point for\nstock selection, value-based indexing in a noisy market, and beating benchmark\nindices through tactical style rotation. Unfortunately, discounted cash flow,\nmethod of comparables, and fundamental analysis typically yield discrepant\nvaluation estimates. Moreover, the valuation estimates typically disagree with\nmarket price. Can one form a superior valuation estimate by averaging over the\nindividual estimates, including market price? This article suggests a Bayesian\nframework for combining two or more estimates into a superior valuation\nestimate. The framework justifies the common practice of averaging over several\nestimates to arrive at a final point estimate.\n", "category": [5, 0, 3, 3, 3, 3, 3, 6]}
{"abstract": "  Most of the econometric and econophysics models have been borrowed from the\nstatistical physics, and as a cosequence, a new interdisciplinary science\ncalled econophysics has emerged. In this paper we planned to extend the analogy\nbetween different economic processes or phenomena and processes and phenomena\nfrom different fields of physics, other than statistical physics. On the basis\nof the economic development process and amplification phenomenon analogy, a new\neconophysics model, named economic amplifier, on the electronic amplification\nprinciple from applied physics was proposed und largely analyzed.\n", "category": [5, 3]}
{"abstract": "  Tools of the theory of critical phenomena, namely the scaling analysis and\nuniversality, are argued to be applicable to large complex web-like network\nstructures. Using a detailed analysis of the real data of the International\nTrade Network we argue that the scaled link weight distribution has an\napproximate log-normal distribution which remains robust over a period of 53\nyears. Another universal feature is observed in the power-law growth of the\ntrade strength with gross domestic product, the exponent being similar for all\ncountries. Using the 'rich-club' coefficient measure of the weighted networks\nit has been shown that the size of the rich-club controlling half of the\nworld's trade is actually shrinking. While the gravity law is known to describe\nwell the social interactions in the static networks of population migration,\ninternational trade, etc, here for the first time we studied a non-conservative\ndynamical model based on the gravity law which excellently reproduced many\nempirical features of the ITN.\n", "category": [5, 3]}
{"abstract": "  Bilateral trade relationships in the international level between pairs of\ncountries in the world give rise to the notion of the International Trade\nNetwork (ITN). This network has attracted the attention of network researchers\nas it serves as an excellent example of the weighted networks, the link weight\nbeing defined as a measure of the volume of trade between two countries. In\nthis paper we analyzed the international trade data for 53 years and studied in\ndetail the variations of different network related quantities associated with\nthe ITN. Our observation is that the ITN has also a scale invariant structure\nlike many other real-world networks.\n", "category": [5, 3]}
{"abstract": "  The distribution of the return intervals $\\tau$ between volatilities above a\nthreshold $q$ for financial records has been approximated by a scaling\nbehavior. To explore how accurate is the scaling and therefore understand the\nunderlined non-linear mechanism, we investigate intraday datasets of 500 stocks\nwhich consist of the Standard & Poor's 500 index. We show that the cumulative\ndistribution of return intervals has systematic deviations from scaling. We\nsupport this finding by studying the m-th moment $\\mu_m \\equiv\n<(\\tau/<\\tau>)^m>^{1/m}$, which show a certain trend with the mean interval\n$<\\tau>$. We generate surrogate records using the Schreiber method, and find\nthat their cumulative distributions almost collapse to a single curve and\nmoments are almost constant for most range of $<\\tau>$. Those substantial\ndifferences suggest that non-linear correlations in the original volatility\nsequence account for the deviations from a single scaling law. We also find\nthat the original and surrogate records exhibit slight tendencies for short and\nlong $<\\tau>$, due to the discreteness and finite size effects of the records\nrespectively. To avoid as possible those effects for testing the multiscaling\nbehavior, we investigate the moments in the range $10<<\\tau>\\leq100$, and find\nthe exponent $\\alpha$ from the power law fitting $\\mu_m\\sim<\\tau>^\\alpha$ has a\nnarrow distribution around $\\alpha\\neq0$ which depend on m for the 500 stocks.\nThe distribution of $\\alpha$ for the surrogate records are very narrow and\ncentered around $\\alpha=0$. This suggests that the return interval distribution\nexhibit multiscaling behavior due to the non-linear correlations in the\noriginal volatility.\n", "category": [5, 3]}
{"abstract": "  We consider the problem of portfolio selection within the classical Markowitz\nmean-variance framework, reformulated as a constrained least-squares regression\nproblem. We propose to add to the objective function a penalty proportional to\nthe sum of the absolute values of the portfolio weights. This penalty\nregularizes (stabilizes) the optimization problem, encourages sparse portfolios\n(i.e. portfolios with only few active positions), and allows to account for\ntransaction costs. Our approach recovers as special cases the\nno-short-positions portfolios, but does allow for short positions in limited\nnumber. We implement this methodology on two benchmark data sets constructed by\nFama and French. Using only a modest amount of training data, we construct\nportfolios whose out-of-sample performance, as measured by Sharpe ratio, is\nconsistently and significantly better than that of the naive evenly-weighted\nportfolio which constitutes, as shown in recent literature, a very tough\nbenchmark.\n", "category": [5, 2, 6]}
{"abstract": "  We investigate the strength and the direction of information transfer in the\nU.S. stock market between the composite stock price index of stock market and\nprices of individual stocks using the transfer entropy. Through the\ndirectionality of the information transfer, we find that individual stocks are\ninfluenced by the index of the market.\n", "category": [5, 3]}
{"abstract": "  Discussion of \"2004 IMS Medallion Lecture: Local Rademacher complexities and\noracle inequalities in risk minimization\" by V. Koltchinskii [arXiv:0708.0083]\n", "category": [5, 2, 6]}
{"abstract": "  Discussion of ``2004 IMS Medallion Lecture: Local Rademacher complexities and\noracle inequalities in risk minimization'' by V. Koltchinskii [arXiv:0708.0083]\n", "category": [5, 2, 6]}
{"abstract": "  Discussion of ``2004 IMS Medallion Lecture: Local Rademacher complexities and\noracle inequalities in risk minimization'' by V. Koltchinskii [arXiv:0708.0083]\n", "category": [5, 2, 6]}
{"abstract": "  Discussion of ``2004 IMS Medallion Lecture: Local Rademacher complexities and\noracle inequalities in risk minimization'' by V. Koltchinskii [arXiv:0708.0083]\n", "category": [5, 2, 6]}
{"abstract": "  Discussion of ``2004 IMS Medallion Lecture: Local Rademacher complexities and\noracle inequalities in risk minimization'' by V. Koltchinskii [arXiv:0708.0083]\n", "category": [5, 2, 6]}
{"abstract": "  We consider models of financial markets in which all parties involved find\nincentives to participate. Strategies are evaluated directly by their virtual\nwealths. By tuning the price sensitivity and market impact, a phase diagram\nwith several attractor behaviors resembling those of real markets emerge,\nreflecting the roles played by the arbitrageurs and trendsetters, and including\na phase with irregular price trends and positive sums. The positive-sumness of\nthe players' wealths provides participation incentives for them. Evolution and\nthe bid-ask spread provide mechanisms for the gain in wealth of both the\nplayers and market-makers. New players survive in the market if the\nevolutionary rate is sufficiently slow. We test the applicability of the model\non real Hang Seng Index data over 20 years. Comparisons with other models show\nthat our model has a superior average performance when applied to real\nfinancial data.\n", "category": [5, 3]}
{"abstract": "  We introduce a new formulation of asset trading games in continuous time in\nthe framework of the game-theoretic probability established by Shafer and Vovk\n(Probability and Finance: It's Only a Game! (2001) Wiley). In our formulation,\nthe market moves continuously, but an investor trades in discrete times, which\ncan depend on the past path of the market. We prove that an investor can\nessentially force that the asset price path behaves with the variation exponent\nexactly equal to two. Our proof is based on embedding high-frequency\ndiscrete-time games into the continuous-time game and the use of the Bayesian\nstrategy of Kumon, Takemura and Takeuchi (Stoch. Anal. Appl. 26 (2008)\n1161--1180) for discrete-time coin-tossing games. We also show that the main\ngrowth part of the investor's capital processes is clearly described by the\ninformation quantities, which are derived from the Kullback--Leibler\ninformation with respect to the empirical fluctuation of the asset price.\n", "category": [5, 2]}
{"abstract": "  We investigate the local fractal properties of the financial time series\nbased on the evolution of the Warsaw Stock Exchange Index (WIG) connected with\nthe largest developing financial market in Europe. Calculating the local Hurst\nexponent for the WIG time series we find an interesting dependence between the\nbehavior of the local fractal properties of the WIG time series and the crashes\nappearance on the financial market.\n", "category": [5, 0, 3]}
{"abstract": "  Continuous-time random walks are a well suited tool for the description of\nmarket behaviour at the smallest scale: the tick-to-tick evolution. We will\napply this kind of market model to the valuation of perpetual American options:\nderivatives with no maturity that can be exercised at any time. Our approach\nleads to option prices that fulfil financial formulas when canonical\nassumptions on the dynamics governing the process are made, but it is still\nsuitable for more exotic market conditions.\n", "category": [5, 3]}
{"abstract": "  We investigated the network structures of the Japanese stock market through\nthe minimum spanning tree. We defined grouping coefficient to test the validity\nof conventional grouping by industrial categories, and found a decreasing in\ntrend for the coefficient. This phenomenon supports the increasing external\ninfluences on the market due to the globalization. To reduce this influence, we\nused S&P500 index as the international market and removed its correlation with\nevery stock. We found stronger grouping in this measurement, compared to the\noriginal analysis, which agrees with our assumption that the international\nmarket influences to the Japanese market.\n", "category": [5, 3]}
{"abstract": "  In this paper, we investigate the Merton portfolio management problem in the\ncontext of non-exponential discounting. This gives rise to time-inconsistency\nof the decision-maker. If the decision-maker at time t=0 can commit his/her\nsuccessors, he/she can choose the policy that is optimal from his/her point of\nview, and constrain the others to abide by it, although they do not see it as\noptimal for them. If there is no commitment mechanism, one must seek a\nsubgame-perfect equilibrium strategy between the successive decision-makers. In\nthe line of the earlier work by Ekeland and Lazrak we give a precise definition\nof equilibrium strategies in the context of the portfolio management problem,\nwith finite horizon, we characterize it by a system of partial differential\nequations, and we show existence in the case when the utility is CRRA and the\nterminal time T is small. We also investigate the infinite-horizon case and we\ngive two different explicit solutions in the case when the utility is CRRA (in\ncontrast with the case of exponential discount, where there is only one). Some\nof our results are proved under the assumption that the discount function h(t)\nis a linear combination of two exponentials, or is the product of an\nexponential by a linear function.\n", "category": [5, 2, 2]}
{"abstract": "  In this small note we use results derived in Berestycki et al. to correct the\ncelebrated formulae of Hagan et al. We derive explicitly the correct zero order\nterm in the expansion of the implied volatility in time to maturity. The new\nterm is consistent as $\\beta\\to 1$. Furthermore, numerical simulations show\nthat it reduces or eliminates known pathologies of the earlier formula.\n", "category": [5, 2, 5]}
{"abstract": "  The stochastic knapsack has been used as a model in wide ranging applications\nfrom dynamic resource allocation to admission control in telecommunication. In\nrecent years, a variation of the model has become a basic tool in studying\nproblems that arise in revenue management and dynamic/flexible pricing; and it\nis in this context that our study is undertaken. Based on a dynamic programming\nformulation and associated properties of the value function, we study in this\npaper a class of control that we call switch-over policies -- start from\naccepting only orders of the highest price, and switch to including lower\nprices as time goes by, with the switch-over times optimally decided via convex\nprogramming. We establish the asymptotic optimality of the switch-over policy,\nand develop pricing models based on this policy to optimize the price\nreductions over the decision horizon.\n", "category": [5, 2, 2]}
{"abstract": "  Several models for the pricing of derivative securities in illiquid markets\nare discussed. A typical type of nonlinear partial differential equations\narising from these investigation is studied. The scaling properties of these\nequations are discussed. Explicit solutions for one of the models are obtained\nand studied.\n", "category": [5, 2, 2]}
{"abstract": "  This paper applies a regularization procedure called increasing rearrangement\nto monotonize Edgeworth and Cornish-Fisher expansions and any other related\napproximations of distribution and quantile functions of sample statistics.\nBesides satisfying the logical monotonicity, required of distribution and\nquantile functions, the procedure often delivers strikingly better\napproximations to the distribution and quantile functions of the sample mean\nthan the original Edgeworth-Cornish-Fisher expansions.\n", "category": [6, 7]}
{"abstract": "  We provide a new characterization of mean-variance hedging strategies in a\ngeneral semimartingale market. The key point is the introduction of a new\nprobability measure $P^{\\star}$ which turns the dynamic asset allocation\nproblem into a myopic one. The minimal martingale measure relative to\n$P^{\\star}$ coincides with the variance-optimal martingale measure relative to\nthe original probability measure $P$.\n", "category": [5, 2]}
{"abstract": "  We consider optimal execution strategies for block market orders placed in a\nlimit order book (LOB). We build on the resilience model proposed by Obizhaeva\nand Wang (2005) but allow for a general shape of the LOB defined via a given\ndensity function. Thus, we can allow for empirically observed LOB shapes and\nobtain a nonlinear price impact of market orders. We distinguish two\npossibilities for modeling the resilience of the LOB after a large market\norder: the exponential recovery of the number of limit orders, i.e., of the\nvolume of the LOB, or the exponential recovery of the bid-ask spread. We\nconsider both of these resilience modes and, in each case, derive explicit\noptimal execution strategies in discrete time. Applying our results to a\nblock-shaped LOB, we obtain a new closed-form representation for the optimal\nstrategy, which explicitly solves the recursive scheme given in Obizhaeva and\nWang (2005). We also provide some evidence for the robustness of optimal\nstrategies with respect to the choice of the shape function and the\nresilience-type.\n", "category": [5, 2]}
{"abstract": "  Parameters defined via general estimating equations (GEE) can be estimated by\nmaximizing the empirical likelihood (EL). Newey and Smith [Econometrica 72\n(2004) 219--255] have recently shown that this EL estimator exhibits desirable\nhigher-order asymptotic properties, namely, that its $O(n^{-1})$ bias is small\nand that bias-corrected EL is higher-order efficient. Although EL possesses\nthese properties when the model is correctly specified, this paper shows that,\nin the presence of model misspecification, EL may cease to be root n convergent\nwhen the functions defining the moment conditions are unbounded (even when\ntheir expectations are bounded). In contrast, the related exponential tilting\n(ET) estimator avoids this problem. This paper shows that the ET and EL\nestimators can be naturally combined to yield an estimator called exponentially\ntilted empirical likelihood (ETEL) exhibiting the same $O(n^{-1})$ bias and the\nsame $O(n^{-2})$ variance as EL, while maintaining root n convergence under\nmodel misspecification.\n", "category": [2, 5, 6]}
{"abstract": "  This paper presents a methodology to introduce time-dependent parameters for\na wide family of models preserving their analytic tractability. This family\nincludes hybrid models with stochastic volatility, stochastic interest-rates,\njumps and their non-hybrid counterparts. The methodology is applied to Heston's\nmodel. A bootstrapping algorithm is presented for calibration. A case study\nworks out the calibration of the time-dependent parameters to the volatility\nsurface of the Eurostoxx 50 index. The methodology is also applied to the\nanalytic valuation of forward start vanilla options driven by Heston's model.\nThis result is used to explore the forward skew of the case study.\n", "category": [5, 2, 6]}
{"abstract": "  In this paper the correlation between education, research and macroeconomic\nstrength of countries at a global scale is analyzed on the basis of statistical\ndata published by the UNIDO and OECD. It uses sets of composite indicators\ndescribing the economical performance and competitiveness as well as those\nrelevant for human development, education, knowledge and technology achievement\nand correlates them. It turns out that for countries with a human development\nindex (HDI) below 0.7 the basic education and technology achievement indices\nare the driving force for further development, whereas for the industrialized\ncountries the knowledge index as a composite education and communication index\nhas the strongest effect on the economic strength of a country as measured by\nthe gross domestic product.\n", "category": [3, 3, 5]}
{"abstract": "  Economies grow by upgrading the type of products they produce and export. The\ntechnology, capital, institutions and skills needed to make such new products\nare more easily adapted from some products than others. We study the network of\nrelatedness between products, or product space, finding that most upscale\nproducts are located in a densely connected core while lower income products\noccupy a less connected periphery. We show that countries tend to move to goods\nclose to those they are currently specialized in, allowing nations located in\nmore connected parts of the product space to upgrade their exports basket more\nquickly. Most countries can reach the core only if they jump over empirically\ninfrequent distances in the product space. This may help explain why poor\ncountries have trouble developing more competitive exports, failing to converge\nto the income levels of rich countries.\n", "category": [5, 3]}
{"abstract": "  Despite the fact that the Euler allocation principle has been adopted by many\nfinancial institutions for their internal capital allocation process, a\ncomprehensive description of Euler allocation seems still to be missing. We try\nto fill this gap by presenting the theoretical background as well as practical\naspects. In particular, we discuss how Euler risk contributions can be\nestimated for some important risk measures. We furthermore investigate the\nanalysis of CDO tranche expected losses by means of Euler's theorem and suggest\nan approach to measure the impact of risk factors on non-linear portfolios.\n", "category": [5, 6]}
{"abstract": "  We propose an extended public goods interaction model to study the evolution\nof cooperation in heterogeneous population. The investors are arranged on the\nwell known scale-free type network, the Barab\\'{a}si-Albert model. Each\ninvestor is supposed to preferentially distribute capital to pools in its\nportfolio based on the knowledge of pool sizes. The extent that investors\nprefer larger pools is determined by investment strategy denoted by a tunable\nparameter $\\alpha$, with larger $\\alpha$ corresponding to more preference to\nlarger pools. As comparison, we also study this interaction model on square\nlattice, and find that the heterogeneity contacts favors cooperation.\nAdditionally, the influence of local topology to the game dynamics under\ndifferent $\\alpha$ strategies are discussed. It is found that the system with\nsmaller $\\alpha$ strategy can perform comparatively better than the larger\n$\\alpha$ ones.\n", "category": [5, 3, 3, 5]}
{"abstract": "  We fit the volatility fluctuations of the S&P 500 index well by a Chi\ndistribution, and the distribution of log-returns by a corresponding\nsuperposition of Gaussian distributions. The Fourier transform of this is,\nremarkably, of the Tsallis type. An option pricing formula is derived from the\nsame superposition of Black-Scholes expressions. An explicit analytic formula\nis deduced from a perturbation expansion around a Black-Scholes formula with\nthe mean volatility. The expansion has two parts. The first takes into account\nthe non-Gaussian character of the stock-fluctuations and is organized by powers\nof the excess kurtosis, the second is contract based, and is organized by the\nmoments of moneyness of the option. With this expansion we show that for the\nDow Jones Euro Stoxx 50 option data, a Delta-hedging strategy is close to being\noptimal.\n", "category": [5, 3, 2, 3, 3]}
{"abstract": "  The trade size $\\omega$ has direct impact on the price formation of the stock\ntraded. Econophysical analyses of transaction data for the US and Australian\nstock markets have uncovered market-specific scaling laws, where a master curve\nof price impact can be obtained in each market when stock capitalization $C$ is\nincluded as an argument in the scaling relation. However, the rationale of\nintroducing stock capitalization in the scaling is unclear and the anomalous\nnegative correlation between price change $r$ and trade size $\\omega$ for small\ntrades is unexplained. Here we show that these issues can be addressed by\ntaking into account the aggressiveness of orders that result in trades together\nwith a proper normalization technique. Using order book data from the Chinese\nmarket, we show that trades from filled and partially filled limit orders have\nvery different price impact. The price impact of trades from partially filled\norders is constant when the volume is not too large, while that of filled\norders shows power-law behavior $r\\sim \\omega^\\alpha$ with $\\alpha\\approx2/3$.\nWhen returns and volumes are normalized by stock-dependent averages,\ncapitalization-independent scaling laws emerge for both types of trades.\nHowever, no scaling relation in terms of stock capitalization can be\nconstructed. In addition, the relation $\\alpha=\\alpha_\\omega/\\alpha_r$ is\nverified, where $\\alpha_\\omega$ and $\\alpha_r$ are the tail exponents of trade\nsizes and returns. These observations also enable us to explain the anomalous\nnegative correlation between $r$ and $\\omega$ for small-size trades. We\nanticipate that these regularities may hold in other order-driven markets.\n", "category": [5, 3]}
{"abstract": "  In this pedagogical study, carried out by adopting standard mathematical\nmethods of nonlinear dynamics, we have presented some simple analytical models\nto understand terminal behaviour in industrial growth. This issue has also been\naddressed from a dynamical systems perspective, with especial emphasis on the\nconcept of the Balanced Scorecard. Our study enables us to make the general\nclaim that although the fortunes of an industrial organization can rise with\nexponential rapidity on relatively short time scales, its growth will\nultimately and inevitably be saturated on long time scales by various factors\nwhich are nonlinear in character. We have mathematically demonstrated the\nlikely occurrence of this feature under various possible circumstances,\nincluding the Red Ocean and the Blue Ocean. Finally and most importantly, our\narguments and their associated mathematical modelling have received remarkable\nsupport from the growth pattern indicated by empirical data gathered from a\nwell-recognized global company like IBM.\n", "category": [5, 3]}
{"abstract": "  We study the distributions of event-time returns and clock-time returns at\ndifferent microscopic timescales using ultra-high-frequency data extracted from\nthe limit-order books of 23 stocks traded in the Chinese stock market in 2003.\nWe find that the returns at the one-trade timescale obey the inverse cubic law.\nFor larger timescales (2-32 trades and 1-5 minutes), the returns follow the\nStudent distribution with power-law tails. With the decrease of timescale, the\ntail becomes fatter, which is consistent with the vibrational theory.\n", "category": [5, 3]}
{"abstract": "  Time reversal invariance can be summarized as follows: no difference can be\nmeasured if a sequence of events is run forward or backward in time. Because\nprice time series are dominated by a randomness that hides possible structures\nand orders, the existence of time reversal invariance requires care to be\ninvestigated. Different statistics are constructed with the property to be zero\nfor time series which are time reversal invariant; they all show that\nhigh-frequency empirical foreign exchange prices are not invariant. The same\nstatistics are applied to mathematical processes that should mimic empirical\nprices. Monte Carlo simulations show that only some ARCH processes with a\nmulti-timescales structure can reproduce the empirical findings. A GARCH(1,1)\nprocess can only reproduce some asymmetry. On the other hand, all the\nstochastic volatility type processes are time reversal invariant. This clear\ndifference related to the process structures gives some strong selection\ncriterion for processes.\n", "category": [5, 3, 5]}
{"abstract": "  We consider the mean-variance hedging problem under partial information in\nthe case where the flow of observable events does not contain the full\ninformation on the underlying asset price process. We introduce a martingale\nequation of a new type and characterize the optimal strategy in terms of the\nsolution of this equation. We give relations between this equation and backward\nstochastic differential equations for the value process of the problem.\n", "category": [5, 2]}
{"abstract": "  This study investigates empirically whether the degree of stock market\nefficiency is related to the prediction power of future price change using the\nindices of twenty seven stock markets. Efficiency refers to weak-form efficient\nmarket hypothesis (EMH) in terms of the information of past price changes. The\nprediction power corresponds to the hit-rate, which is the rate of the\nconsistency between the direction of actual price change and that of predicted\none, calculated by the nearest neighbor prediction method (NN method) using the\nout-of-sample. In this manuscript, the Hurst exponent and the approximate\nentropy (ApEn) are used as the quantitative measurements of the degree of\nefficiency. The relationship between the Hurst exponent, reflecting the various\ntime correlation property, and the ApEn value, reflecting the randomness in the\ntime series, shows negative correlation. However, the average prediction power\non the direction of future price change has the strongly positive correlation\nwith the Hurst exponent, and the negative correlation with the ApEn. Therefore,\nthe market index with less market efficiency has higher prediction power for\nfuture price change than one with higher market efficiency when we analyze the\nmarket using the past price change pattern. Furthermore, we show that the Hurst\nexponent, a measurement of the long-term memory property, provides more\nsignificant information in terms of prediction of future price changes than the\nApEn and the NN method.\n", "category": [5, 3]}
{"abstract": "  World currency network constitutes one of the most complex structures that is\nassociated with the contemporary civilization. On a way towards quantifying its\ncharacteristics we study the cross correlations in changes of the daily foreign\nexchange rates within the basket of 60 currencies in the period December 1998\n-- May 2005. Such a dynamics turns out to predominantly involve one outstanding\neigenvalue of the correlation matrix. The magnitude of this eigenvalue depends\nhowever crucially on which currency is used as a base currency for the\nremaining ones. Most prominent it looks from the perspective of a peripheral\ncurrency. This largest eigenvalue is seen to systematically decrease and thus\nthe structure of correlations becomes more heterogeneous, when more significant\ncurrencies are used as reference. An extreme case in this later respect is the\nUSD in the period considered. Besides providing further insight into subtle\nnature of complexity, these observations point to a formal procedure that in\ngeneral can be used for practical purposes of measuring the relative currencies\nsignificance on various time horizons.\n", "category": [5, 3]}
{"abstract": "  This paper studies the topological properties of the World Trade Web (WTW)\nand its evolution over time by employing a weighted network analysis. We show\nthat the WTW, viewed as a weighted network, displays statistical features that\nare very different from those obtained by using a traditional binary-network\napproach. In particular, we find that: (i) the majority of existing links are\nassociated to weak trade relationships; (ii) the weighted WTW is only weakly\ndisassortative; (iii) countries holding more intense trade relationships are\nmore clustered.\n", "category": [5, 3]}
{"abstract": "  In this paper we develop a Bayesian procedure for estimating multivariate\nstochastic volatility (MSV) using state space models. A multiplicative model\nbased on inverted Wishart and multivariate singular beta distributions is\nproposed for the evolution of the volatility, and a flexible sequential\nvolatility updating is employed. Being computationally fast, the resulting\nestimation procedure is particularly suitable for on-line forecasting. Three\nperformance measures are discussed in the context of model selection: the\nlog-likelihood criterion, the mean of standardized one-step forecast errors,\nand sequential Bayes factors. Finally, the proposed methods are applied to a\ndata set comprising eight exchange rates vis-a-vis the US dollar.\n", "category": [5, 6, 6]}
{"abstract": "  We develop a behavioral model for liquidity and volatility based on empirical\nregularities in trading order flow in the London Stock Exchange. This can be\nviewed as a very simple agent based model in which all components of the model\nare validated against real data. Our empirical studies of order flow uncover\nseveral interesting regularities in the way trading orders are placed and\ncancelled. The resulting simple model of order flow is used to simulate price\nformation under a continuous double auction, and the statistical properties of\nthe resulting simulated sequence of prices are compared to those of real data.\nThe model is constructed using one stock (AZN) and tested on 24 other stocks.\nFor low volatility, small tick size stocks (called Group I) the predictions are\nvery good, but for stocks outside Group I they are not good. For Group I, the\nmodel predicts the correct magnitude and functional form of the distribution of\nthe volatility and the bid-ask spread, without adjusting any parameters based\non prices. This suggests that at least for Group I stocks, the volatility and\nheavy tails of prices are related to market microstructure effects, and\nsupports the hypothesis that, at least on short time scales, the large\nfluctuations of absolute returns are well described by a power law with an\nexponent that varies from stock to stock.\n", "category": [5, 3]}
{"abstract": "  This paper approaches the definition and properties of dynamic convex risk\nmeasures through the notion of a family of concave valuation operators\nsatisfying certain simple and credible axioms. Exploring these in the simplest\ncontext of a finite time set and finite sample space, we find natural\nrisk-transfer and time-consistency properties for a firm seeking to spread its\nrisk across a group of subsidiaries.\n", "category": [5, 2]}
{"abstract": "  Here we propose a method, based on detrended covariance which we call\ndetrended cross-correlation analysis (DXA), to investigate power-law\ncross-correlations between different simultaneously-recorded time series in the\npresence of non-stationarity. We illustrate the method by selected examples\nfrom physics, physiology, and finance.\n", "category": [5, 3]}
{"abstract": "  We consider microstructure as an arbitrary contamination of the underlying\nlatent securities price, through a Markov kernel $Q$. Special cases include\nadditive error, rounding and combinations thereof. Our main result is that,\nsubject to smoothness conditions, the two scales realized volatility is robust\nto the form of contamination $Q$. To push the limits of our result, we show\nwhat happens for some models that involve rounding (which is not, of course,\nsmooth) and see in this situation how the robustness deteriorates with\ndecreasing smoothness. Our conclusion is that under reasonable smoothness, one\ndoes not need to consider too closely how the microstructure is formed, while\nif severe non-smoothness is suspected, one needs to pay attention to the\nprecise structure and also the use to which the estimator of volatility will be\nput.\n", "category": [5, 2, 6]}
{"abstract": "  The maximum entropy principle can be used to assign utility values when only\npartial information is available about the decision maker's preferences. In\norder to obtain such utility values it is necessary to establish an analogy\nbetween probability and utility through the notion of a utility density\nfunction. According to some authors [Soofi (1990), Abbas (2006a) (2006b),\nSandow et al. (2006), Friedman and Sandow (2006), Darooneh (2006)] the maximum\nentropy utility solution embeds a large family of utility functions. In this\npaper we explore the maximum entropy principle to estimate the utility function\nof a risk averse decision maker.\n", "category": [5, 3]}
{"abstract": "  The investor is interested in the expected return and he is also concerned\nabout the risk and the uncertainty assumed by the investment. One of the most\npopular concepts used to measure the risk and the uncertainty is the variance\nand/or the standard-deviation. In this paper we explore the following issues:\nIs the standard-deviation a good measure of risk and uncertainty? What are the\npotentialities of the entropy in this context? Can entropy present some\nadvantages as a measure of uncertainty and simultaneously verify some basic\nassumptions of the portfolio management theory, namely the effect of\ndiversification?\n", "category": [5, 3]}
{"abstract": "  The correlated stochastic volatility models constitute a natural extension of\nthe Black and Scholes-Merton framework: here the volatility is not a constant,\nbut a stochastic process correlated with the price log-return one. At present,\nseveral stochastic volatility models are discussed in the literature, differing\nin the dynamics attached to the volatility. The aim of the present work is to\ncompare the most recent results about three popular models: the Vasicek, Heston\nand exponential Ornstein-Uhlenbeck models. We analyzed for each of them the\ntheoretical results known in the literature (volatility and return\ndistribution, higher-order moments and different-time correlations) in order to\ntest their predictive effectiveness on the outcomes of original numerical\nsimulations, paying particular attention to their ability to reproduce\nempirical statistical properties of prices. The numerical results demonstrate\nthat these models can be implemented maintaining all their features, especially\nin view of financial applications like market risk management or option\npricing. In order to critically compare the models, we also perform an\nempirical analysis of financial time series from the Italian stock market,\nshowing the exponential Ornstein-Uhlenbeck model's ability to capture the\nstylized facts of volatility and log-return probability distributions.\n", "category": [5, 3]}
{"abstract": "  We investigate how simultaneously recorded long-range power-law correlated\nmulti-variate signals cross-correlate. To this end we introduce a two-component\nARFIMA stochastic process and a two-component FIARCH process to generate\ncoupled fractal signals with long-range power-law correlations which are at the\nsame time long-range cross-correlated. We study how the degree of\ncross-correlations between these signals depends on the scaling exponents\ncharacterizing the fractal correlations in each signal and on the coupling\nbetween the signals. Our findings have relevance when studying parallel outputs\nof multiple-component of physical, physiological and social systems.\n", "category": [5, 3]}
{"abstract": "  The search for more realistic modeling of financial time series reveals\nseveral stylized facts of real markets. In this work we focus on the\nmultifractal properties found in price and index signals. Although the usual\nMinority Game (MG) models do not exhibit multifractality, we study here one of\nits variants that does. We show that the nonsynchronous MG models in the\nnonergodic phase is multifractal and in this sense, together with other\nstylized facts, constitute a better modeling tool. Using the Structure Function\n(SF) approach we detected the stationary and the scaling range of the time\nseries generated by the MG model and, from the linear (nonlinear) behavior of\nthe SF we identified the fractal (multifractal) regimes. Finally, using the\nWavelet Transform Modulus Maxima (WTMM) technique we obtained its multifractal\nspectrum width for different dynamical regimes.\n", "category": [5, 3]}
{"abstract": "  We study the persistence phenomenon in a socio-econo dynamics model using\ncomputer simulations at a finite temperature on hypercubic lattices in\ndimensions up to 5. The model includes a ` social\\rq local field which contains\nthe magnetization at time $t$. The nearest neighbour quenched interactions are\ndrawn from a binary distribution which is a function of the bond concentration,\n$p$. The decay of the persistence probability in the model depends on both the\nspatial dimension and $p$. We find no evidence of ` blocking\\rq in this model.\nWe also discuss the implications of our results for possible applications in\nthe social and economic fields. It is suggested that the absence, or otherwise,\nof blocking could be used as a criterion to decide on the validity of a given\nmodel in different scenarios.\n", "category": [5, 3, 3, 3, 5]}
{"abstract": "  The relaxation dynamics of aftershocks after large volatility shocks are\ninvestigated based on two high-frequency data sets of the Shanghai Stock\nExchange Composite (SSEC) index. Compared with previous relevant work, we have\ndefined main financial shocks based on large volatilities rather than large\ncrashes. We find that the occurrence rate of aftershocks with the magnitude\nexceeding a given threshold for both daily volatility (constructed using\n1-minute data) and minutely volatility (using intra-minute data) decays as a\npower law. The power-law relaxation exponent increases with the volatility\nthreshold and is significantly greater than 1. Taking financial volatility as\nthe counterpart of seismic activity, the power-law relaxation in financial\nvolatility deviates remarkably from the Omori law in Geophysics.\n", "category": [5, 3]}
{"abstract": "  The notion of utility maximising entropy (u-entropy) of a probability\ndensity, which was introduced and studied by Slomczynski and Zastawniak (Ann.\nProb 32 (2004) 2261-2285, arXiv:math.PR/0410115 v1), is extended in two\ndirections. First, the relative u-entropy of two probability measures in\narbitrary probability spaces is defined. Then, specialising to discrete\nprobability spaces, we also introduce the absolute u-entropy of a probability\nmeasure. Both notions are based on the idea, borrowed from mathematical\nfinance, of maximising the expected utility of the terminal wealth of an\ninvestor. Moreover, u-entropy is also relevant in thermodynamics, as it can\nreplace the standard Boltzmann-Shannon entropy in the Second Law. If the\nutility function is logarithmic or isoelastic (a power function), then the\nwell-known notions of the Boltzmann-Shannon and Renyi relative entropy are\nrecovered. We establish the principal properties of relative and discrete\nu-entropy and discuss the links with several related approaches in the\nliterature.\n", "category": [2, 5]}
{"abstract": "  Empirical analysis of the foreign exchange market is conducted based on\nmethods to quantify similarities among multi-dimensional time series with\nspectral distances introduced in [A.-H. Sato, Physica A, 382 (2007) 258--270].\nAs a result it is found that the similarities among currency pairs fluctuate\nwith the rotation of the earth, and that the similarities among best quotation\nrates are associated with those among quotation frequencies. Furthermore it is\nshown that the Jensen-Shannon spectral divergence is proportional to a mean of\nthe Kullback-Leibler spectral distance both empirically and numerically. It is\nconfirmed that these spectral distances are connected with distributions for\nbehavioral parameters of the market participants from numerical simulation.\nThis concludes that spectral distances of representative quantities of\nfinancial markets are related into diversification of behavioral parameters of\nthe market participants.\n", "category": [5, 3, 3]}
{"abstract": "  The log returns of financial time series are usually modeled by means of the\nstationary GARCH(1,1) stochastic process or its generalizations which can not\nproperly describe the nonstationary deterministic components of the original\nseries. We analyze the influence of deterministic trends on the GARCH(1,1)\nparameters using Monte Carlo simulations. The statistical ensembles contain\nnumerically generated time series composed by GARCH(1,1) noise superposed on\ndeterministic trends. The GARCH(1,1) parameters characteristic for financial\ntime series longer than one year are not affected by the detrending errors. We\nalso show that if the ARCH coefficient is greater than the GARCH coefficient,\nthen the estimated GARCH(1,1) parameters depend on the number of monotonic\nparts of the trend and on the ratio between the trend and the noise amplitudes.\n", "category": [5, 3]}
{"abstract": "  Increasingly, a huge amount of statistics have been gathered which clearly\nindicates that income and wealth distributions in various countries or\nsocieties follow a robust pattern, close to the Gibbs distribution of energy in\nan ideal gas in equilibrium. However, it also deviates in the low income and\nmore significantly for the high income ranges. Application of physics models\nprovides illuminating ideas and understanding, complementing the observations.\n", "category": [5, 3, 3, 3]}
{"abstract": "  American options are studied in a general discrete market in the presence of\nproportional transaction costs, modelled as bid-ask spreads. Pricing algorithms\nand constructions of hedging strategies, stopping times and martingale\nrepresentations are presented for short (seller's) and long (buyer's) positions\nin an American option with an arbitrary payoff. This general approach extends\nthe special cases considered in the literature concerned primarily with\ncomputing the prices of American puts under transaction costs by relaxing any\nrestrictions on the form of the payoff, the magnitude of the transaction costs\nor the discrete market model itself. The largely unexplored case of pricing,\nhedging and stopping for the American option buyer under transaction costs is\nalso covered. The pricing algorithms are computationally efficient, growing\nonly polynomially with the number of time steps in a recombinant tree model.\nThe stopping times realising the ask (seller's) and bid (buyer's) option prices\ncan differ from one another. The former is generally a so-called mixed\n(randomised) stopping time, whereas the latter is always a pure (ordinary)\nstopping time.\n", "category": [5, 2]}
{"abstract": "  We investigate scaling and memory effects in return intervals between price\nvolatilities above a certain threshold $q$ for the Japanese stock market using\ndaily and intraday data sets. We find that the distribution of return intervals\ncan be approximated by a scaling function that depends only on the ratio\nbetween the return interval $\\tau$ and its mean $<\\tau>$. We also find memory\neffects such that a large (or small) return interval follows a large (or small)\ninterval by investigating the conditional distribution and mean return\ninterval. The results are similar to previous studies of other markets and\nindicate that similar statistical features appear in different financial\nmarkets. We also compare our results between the period before and after the\nbig crash at the end of 1989. We find that scaling and memory effects of the\nreturn intervals show similar features although the statistical properties of\nthe returns are different.\n", "category": [5, 3, 3]}
{"abstract": "  In this work, we aim to gain a better understanding of the volatility smile\nobserved in options markets through microsimulation (MS). We adopt two types of\nactive traders in our MS model: speculators and arbitrageurs, and call and put\noptions on one underlying asset. Speculators make decisions based on their\nexpectations of the asset price at the option expiration time. Arbitrageurs\ntrade at different arbitrage opportunities such as violation of put-call\nparity. Difference in liquidity among options is also included. Notwithstanding\nits simplicity, our model can generate implied volatility (IV) curves similar\nto empirical observations. Our results suggest that the volatility smile is\nrelated to the competing effect of heterogeneous trading behavior and the\nimpact of differential liquidity.\n", "category": [5, 3]}
{"abstract": "  Following the statistical mechanics methodology, firstly introduced in\nmacroeconomics by Aoki [1996,2002], we provide some insights to the well known\nworks of Greenwald and Stiglitz [1990, 1993]. Specifically, we reach\nanalytically a closed form solution of their models overcoming the aggregation\nproblem. The key idea is to represent the economy as an evolving complex\nsystem, composed by heterogeneous interacting agents, that can partitioned into\na space of macroscopic states. This meso level of aggregation permits to adopt\nmean field interaction modeling and master equation techniques.\n", "category": [5, 3]}
{"abstract": "  Long memory and volatility clustering are two stylized facts frequently\nrelated to financial markets. Traditionally, these phenomena have been studied\nbased on conditionally heteroscedastic models like ARCH, GARCH, IGARCH and\nFIGARCH, inter alia. One advantage of these models is their ability to capture\nnonlinear dynamics. Another interesting manner to study the volatility\nphenomena is by using measures based on the concept of entropy. In this paper\nwe investigate the long memory and volatility clustering for the SP 500, NASDAQ\n100 and Stoxx 50 indexes in order to compare the US and European Markets.\nAdditionally, we compare the results from conditionally heteroscedastic models\nwith those from the entropy measures. In the latter, we examine Shannon\nentropy, Renyi entropy and Tsallis entropy. The results corroborate the\nprevious evidence of nonlinear dynamics in the time series considered.\n", "category": [5, 3]}
{"abstract": "  In this study, we have investigated factors of determination which can affect\nthe connected structure of a stock network. The representative index for\ntopological properties of a stock network is the number of links with other\nstocks. We used the multi-factor model, extensively acknowledged in financial\nliterature. In the multi-factor model, common factors act as independent\nvariables while returns of individual stocks act as dependent variables. We\ncalculated the coefficient of determination, which represents the measurement\nvalue of the degree in which dependent variables are explained by independent\nvariables. Therefore, we investigated the relationship between the number of\nlinks in the stock network and the coefficient of determination in the\nmulti-factor model. We used individual stocks traded on the market indices of\nKorea, Japan, Canada, Italy and the UK. The results are as follows. We found\nthat the mean coefficient of determination of stocks with a large number of\nlinks have higher values than those with a small number of links with other\nstocks. These results suggest that common factors are significantly\ndeterministic factors to be taken into account when making a stock network.\nFurthermore, stocks with a large number of links to other stocks can be more\naffected by common factors.\n", "category": [5, 3]}
{"abstract": "  We investigated the topological properties of stock networks through a\ncomparison of the original stock network with the estimated stock network from\nthe correlation matrix created by the random matrix theory (RMT). We used\nindividual stocks traded on the market indices of Korea, Japan, Canada, the\nUSA, Italy, and the UK. The results are as follows. As the correlation matrix\nreflects the more eigenvalue property, the estimated stock network from the\ncorrelation matrix gradually increases the degree of consistency with the\noriginal stock network. Each stock with a different number of links to other\nstocks in the original stock network shows a different response. In particular,\nthe largest eigenvalue is a significant deterministic factor in terms of the\nformation of a stock network.\n", "category": [5, 3]}
{"abstract": "  We propose a novel method to quantify the clustering behavior in a complex\ntime series and apply it to a high-frequency data of the financial markets. We\nfind that regardless of used data sets, all data exhibits the volatility\nclustering properties, whereas those which filtered the volatility clustering\neffect by using the GARCH model reduce volatility clustering significantly. The\nresult confirms that our method can measure the volatility clustering effect in\nfinancial market.\n", "category": [5, 3]}
{"abstract": "  Statistical analysis of financial data most focused on testing the validity\nof Brownian motion (Bm). Analysis performed on several time series have shown\ndeviation from the Bm hypothesis, that is at the base of the evaluation of many\nfinancial derivatives. We inquiry in the behavior of measures of performance\nbased on maximum drawdown movements (MDD), testing their stability when the\nunderlying process deviates from the Bm hypothesis. In particular we consider\nthe fractional Brownian motion (fBm), and fluctuations estimated empirically on\nraw market data. The case study of the rising part of speculative bubbles is\nreported.\n", "category": [5, 3, 3]}
{"abstract": "  In this note we study the bilateral merchandise trade flows between 186\ncountries over the 1948-2005 period using data from the International Monetary\nFund. We use Pajek to identify network structure and behavior across thresholds\nand over time. In particular, we focus on the evolution of trade \"islands\" in\nthe a world trade network in which countries are linked with directed edges\nweighted according to fraction of total dollars sent from one country to\nanother. We find mixed evidence for globalization.\n", "category": [5, 3]}
{"abstract": "  A model is developed to study the effectiveness of innovation and its impact\non structure creation and structure change on agent-based societies. The\nabstract model that is developed is easily adapted to any particular field. In\nany interacting environment, the agents receive something from the environment\n(the other agents) in exchange for their effort and pay the environment a\ncertain amount of value for the fulfilling of their needs or for the very price\nof existence in that environment. This is coded by two bit strings and the\ndynamics of the exchange is based on the matching of these strings to those of\nthe other agents. Innovation is related to the adaptation by the agents of\ntheir bit strings to improve some utility function.\n", "category": [5, 3]}
{"abstract": "  This paper formulates and studies a general continuous-time behavioral\nportfolio selection model under Kahneman and Tversky's (cumulative) prospect\ntheory, featuring S-shaped utility (value) functions and probability\ndistortions. Unlike the conventional expected utility maximization model, such\na behavioral model could be easily mis-formulated (a.k.a. ill-posed) if its\ndifferent components do not coordinate well with each other. Certain classes of\nan ill-posed model are identified. A systematic approach, which is\nfundamentally different from the ones employed for the utility model, is\ndeveloped to solve a well-posed model, assuming a complete market and general\nIt\\^o processes for asset prices. The optimal terminal wealth positions,\nderived in fairly explicit forms, possess surprisingly simple structure\nreminiscent of a gambling policy betting on a good state of the world while\naccepting a fixed, known loss in case of a bad one. An example with a two-piece\nCRRA utility is presented to illustrate the general results obtained, and is\nsolved completely for all admissible parameters. The effect of the behavioral\ncriterion on the risky allocations is finally discussed.\n", "category": [5, 2, 2]}
{"abstract": "  A consistency criterion for price impact functions in limit order markets is\nproposed that prohibits chain arbitrage exploitation. Both the bid-ask spread\nand the feedback of sequential market orders of the same kind onto both sides\nof the order book are essential to ensure consistency at the smallest time\nscale. All the stocks investigated in Paris Stock Exchange have consistent\nprice impact functions.\n", "category": [5, 3, 3]}
{"abstract": "  This paper analyzes correlations in patterns of trading of different members\nof the London Stock Exchange. The collection of strategies associated with a\nmember institution is defined by the sequence of signs of net volume traded by\nthat institution in hour intervals. Using several methods we show that there\nare significant and persistent correlations between institutions. In addition,\nthe correlations are structured into correlated and anti-correlated groups.\nClustering techniques using the correlations as a distance metric reveal a\nmeaningful clustering structure with two groups of institutions trading in\nopposite directions.\n", "category": [5, 3]}
{"abstract": "  We present analytical investigations of a multiplicative stochastic process\nthat models a simple investor dynamics in a random environment. The dynamics of\nthe investor's budget, $x(t)$, depends on the stochasticity of the return on\ninvestment, $r(t)$, for which different model assumptions are discussed. The\nfat-tail distribution of the budget is investigated and compared with\ntheoretical predictions. Weare mainly interested in the most probable value\n$x_mp$ of the budget that reaches a constant value over time. Based on an\nanalytical investigation of the dynamics, we are able to predict $x_mp^stat$.\nWe find a scaling law that relates the most probable value to the\ncharacteristic parameters describing the stochastic process. Our analytical\nresults are confirmed by stochastic computer simulations that show a very good\nagreement with the predictions.\n", "category": [5, 3]}
{"abstract": "  This is a review article for Encyclopedia of Complexity and System Science,\nto be published by Springer http://refworks.springer.com/complexity/. The paper\nreviews statistical models for money, wealth, and income distributions\ndeveloped in the econophysics literature since late 1990s.\n", "category": [5, 3, 3, 3, 6]}
{"abstract": "  The participants of the electricity market concern very much the market price\nevolution. Various technologies have been developed for price forecast. SVM\n(Support Vector Machine) has shown its good performance in market price\nforecast. Two approaches for forming the market bidding strategies based on SVM\nare proposed. One is based on the price forecast accuracy, with which the being\nrejected risk is defined. The other takes into account the impact of the\nproducer's own bid. The risks associated with the bidding are controlled by the\nparameters setting. The proposed approaches have been tested on a numerical\nexample.\n", "category": [5, 3]}
{"abstract": "  A number of recent emerging applications call for studying data streams,\npotentially infinite flows of information updated in real-time. When multiple\nco-evolving data streams are observed, an important task is to determine how\nthese streams depend on each other, accounting for dynamic dependence patterns\nwithout imposing any restrictive probabilistic law governing this dependence.\nIn this paper we argue that flexible least squares (FLS), a penalized version\nof ordinary least squares that accommodates for time-varying regression\ncoefficients, can be deployed successfully in this context. Our motivating\napplication is statistical arbitrage, an investment strategy that exploits\npatterns detected in financial data streams. We demonstrate that FLS is\nalgebraically equivalent to the well-known Kalman filter equations, and take\nadvantage of this equivalence to gain a better understanding of FLS and suggest\na more efficient algorithm. Promising experimental results obtained from a\nFLS-based algorithmic trading system for the S&P 500 Futures Index are\nreported.\n", "category": [5, 6, 6]}
{"abstract": "  It will be discussed the statistics of the extreme values in time series\ncharacterized by finite-term correlations with non-exponential decay.\nPrecisely, it will be considered the results of numerical analyses concerning\nthe return intervals of extreme values of the fluctuations of resistance and\ndefect-fraction displayed by a resistor with granular structure in a\nnonequilibrium stationary state. The resistance and defect-fraction are\ncalculated as a function of time by Monte Carlo simulations using a resistor\nnetwork approach. It will be shown that when the auto-correlation function of\nthe fluctuations displays a non-exponential and non-power-law decay, the\ndistribution of the return intervals of extreme values is a stretched\nexponential, with exponent largely independent of the threshold. Recently, a\nstretched exponential distribution of the return intervals of extreme values\nhas been identified in long-term correlated time series by Bunde et al. (2003)\nand Altmann and Kantz (2005). Thus, the present results show that the stretched\nexponential distribution of the return intervals is not an exclusive feature of\nlong-term correlated time series.\n", "category": [3, 3, 5]}
{"abstract": "  A dangerously brief history of the developments of the main ideas in\neconomics, as observed by a physicist, is given. This was published in\n'Econophysics of Stock and Other Markets', Eds. A. Chatterjee, B. K.\nChakrabarti, New Economic Windows Series, Springer, Milan, 2006, pp~219-224.\n", "category": [5, 3]}
{"abstract": "  Quantum game theory, whatever opinions may be held due to its abstract\nphysical formalism, have already found various applications even outside the\northodox physics domain. In this paper we introduce the concept of a quantum\nauction, its advantages and drawbacks. Then we describe the models that have\nalready been put forward. A general model involves Wigner formalism and\ninfinite dimensional Hilbert spaces - we envisage that the implementation might\nnot be an easy task. But a restricted model advocated by the Hewlett-Packard\ngroup seems to be much easier to implement. Simulations involving humans have\nalready been performed. We will focus on problems related to combinatorial\nauctions and technical assumptions that are made. Quantum approach offers at\nleast two important developments. Powerful quantum algorithms for finding\nsolutions would extend the range of possible applications. Quantum strategies,\nbeing qubits, can be teleported but are immune from cloning - therefore extreme\nprivacy of agent's activity could in principle be guaranteed. Then we point out\nsome key problem that have to be solved before commercial use would be\npossible. With present technology, optical networks, single photon sources and\ndetectors seems to be sufficient for experimental realization in the near\nfuture. We conclude by describing potential customers, estimating the potential\nmarket size and possible timing.\n", "category": [5, 3]}
{"abstract": "  This work suggests modifications to a previously introduced class of\nheterogeneous agent models that allow for the inclusion of different types of\nagent motivations and behaviours in a unified way. The agents operate within a\nhighly simplified environment where they are only able to be long or short one\nunit of the asset. The price of the asset is influenced by both an external\ninformation stream and the demand of the agents. The current strategy of each\nagent is defined by a pair of moving thresholds straddling the current price.\nWhen the price crosses either of the thresholds for a particular agent, that\nagent switches position and a new pair of thresholds is generated.\n  Different kinds of threshold motion correspond to different sources of\nmotivation, running the gamut from purely rational information-processing,\nthrough rational (but often undesirable) behaviour induced by perverse\nincentives and moral hazards, to purely psychological effects. As with the\nprevious class of models, the fact that the simplest model of this kind\nprecisely conforms to the Efficient Market Hypothesis allows causal\nrelationships to be established between properties at the agent level and\nviolations of EMH price statistics at the global level.\n", "category": [5, 3]}
{"abstract": "  We have conducted an agent-based simulation of chain bankruptcy. The\npropagation of credit risk on a network, i.e., chain bankruptcy, is the key to\nnderstanding largesized bankruptcies. In our model, decrease of revenue by the\nloss of accounts payable is modeled by an interaction term, and bankruptcy is\ndefined as a capital deficit. Model parameters were estimated using financial\ndata for 1,077 listed Japanese firms. Simulations of chain bankruptcy on the\nreal transaction network consisting of those 1,077 firms were made with the\nestimated model parameters. Given an initial bankrupt firm, a list of chain\nbankrupt firms was obtained. This model can be used to detect high-risk links\nin a transaction network, for the management of chain bankruptcy.\n", "category": [5, 3]}
{"abstract": "  In this paper we describe market in projective geometry language and give\ndefinition of a matrix of market rate, which is related to the matrix rate of\nreturn and the matrix of judgements in the Analytic Hierarchy Process (AHP). We\nuse these observations to extend the AHP model to projective geometry formalism\nand generalise it to intransitive case. We give financial interpretations of\nsuch generalised model and propose its simplification. The unification of the\nAHP model and projective aspect of portfolio theory suggests a wide spectrum of\nnew applications such extended model.\n", "category": [5, 3, 3]}
{"abstract": "  The present study deals with the analysis and mapping of Swiss franc interest\nrates. Interest rates depend on time and maturity, defining term structure of\nthe interest rate curves (IRC). In the present study IRC are considered in a\ntwo-dimensional feature space - time and maturity. Geostatistical models and\nmachine learning algorithms (multilayer perceptron and Support Vector Machines)\nwere applied to produce interest rate maps. IR maps can be used for the\nvisualisation and patterns perception purposes, to develop and to explore\neconomical hypotheses, to produce dynamic asses-liability simulations and for\nthe financial risk assessments. The feasibility of an application of interest\nrates mapping approach for the IRC forecasting is considered as well.\n", "category": [5, 3]}
{"abstract": "  A continuous-time financial portfolio selection model with expected utility\nmaximization typically boils down to solving a (static) convex stochastic\noptimization problem in terms of the terminal wealth, with a budget constraint.\nIn literature the latter is solved by assuming {\\it a priori} that the problem\nis well-posed (i.e., the supremum value is finite) and a Lagrange multiplier\nexists (and as a consequence the optimal solution is attainable). In this paper\nit is first shown, via various counter-examples, neither of these two\nassumptions needs to hold, and an optimal solution does not necessarily exist.\nThese anomalies in turn have important interpretations in and impacts on the\nportfolio selection modeling and solutions. Relations among the non-existence\nof the Lagrange multiplier, the ill-posedness of the problem, and the\nnon-attainability of an optimal solution are then investigated. Finally,\nexplicit and easily verifiable conditions are derived which lead to finding the\nunique optimal solution.\n", "category": [5, 2, 2, 2]}
{"abstract": "  This paper deals with a high-order accurate implicit finite-difference\napproach to the pricing of barrier options. In this way various types of\nbarrier options are priced, including barrier options paying rebates, and\noptions on dividend-paying-stocks. Moreover, the barriers may be monitored\neither continuously or discretely. In addition to the high-order accuracy of\nthe scheme, and the stretching effect of the coordinate transformation, the\nmain feature of this approach lies on a probability-based optimal determination\nof boundary conditions. This leads to much faster and accurate results when\ncompared with similar pricing approaches. The strength of the present scheme is\nparticularly demonstrated in the valuation of discretely monitored barrier\noptions where it yields values closest to those obtained from the only\nsemi-analytical valuation method available.\n", "category": [5, 2, 2]}
{"abstract": "  Financial markets investors are involved in many games -- they must interact\nwith other agents to achieve their goals. Among them are those directly\nconnected with their activity on markets but one cannot neglect other aspects\nthat influence human decisions and their performance as investors.\nDistinguishing all subgames is usually beyond hope and resource consuming. In\nthis paper we study how investors facing many different games, gather\ninformation and form their decision despite being unaware of the complete\nstructure of the game. To this end we apply reinforcement learning methods to\nthe Information Theory Model of Markets (ITMM). Following Mengel, we can try to\ndistinguish a class $\\Gamma$ of games and possible actions (strategies)\n$a^{i}_{m_{i}}$ for $i-$th agent. Any agent divides the whole class of games\ninto analogy subclasses she/he thinks are analogous and therefore adopts the\nsame strategy for a given subclass. The criteria for partitioning are based on\nprofit and costs analysis. The analogy classes and strategies are updated at\nvarious stages through the process of learning. This line of research can be\ncontinued in various directions.\n", "category": [5, 3, 3]}
{"abstract": "  The paper is devoted to modeling optimal exercise strategies of the behavior\nof investors and issuers working with convertible bonds. This implies solution\nof the problems of stock price modeling, payoff computation and min-max\noptimization.\n  Stock prices (underlying asset) were modeled under the assumption of the\ngeometric Brownian motion of their values. The Monte Carlo method was used for\ncalculating the real payoff which is the objective function. The min-max\noptimization problem was solved using the derivative-free Downhill Simplex\nmethod.\n  The performed numerical experiments allowed to formulate recommendations for\nthe choice of appropriate size of the initial simplex in the Downhill Simplex\nMethod, the number of generated trajectories of underlying asset, the size of\nthe problem and initial trajectories of the behavior of investors and issuers.\n", "category": [5, 2]}
{"abstract": "  One dimensional stylized model taking into account spatial activity of firms\nwith uniformly distributed customers is proposed. The spatial selling area of\neach firm is defined by a short interval cut out from selling space (large\ninterval). In this representation, the firm size is directly associated with\nthe size of its selling interval.\n  The recursive synchronous dynamics of economic evolution is discussed where\nthe growth rate is proportional to the firm size incremented by the term\nincluding the overlap of the selling area with areas of competing firms. Other\nwords, the overlap of selling areas inherently generate a negative feedback\noriginated from the pattern of demand. Numerical simulations focused on the\nobtaining of the firm size distributions uncovered that the range of free\nparameters where the Pareto's law holds corresponds to the range for which the\npair correlation between the nearest neighbor firms attains its minimum.\n", "category": [5, 3, 3]}
{"abstract": "  The problem of filtering information from large correlation matrices is of\ngreat importance in many applications. We have recently proposed the use of the\nKullback-Leibler distance to measure the performance of filtering algorithms in\nrecovering the underlying correlation matrix when the variables are described\nby a multivariate Gaussian distribution. Here we use the Kullback-Leibler\ndistance to investigate the performance of filtering methods based on Random\nMatrix Theory and on the shrinkage technique. We also present some results on\nthe application of the Kullback-Leibler distance to multivariate data which are\nnon Gaussian distributed.\n", "category": [3, 3, 5]}
{"abstract": "  The present paper aims at locating the breakings of the integration process\nof an international system observed during about 50 years in the 19th century.\nA historical study could link them to special events, which operated as\nexogenous shocks on this process. The indicator of integration used is the\nspread between the highest and the lowest among the London, Hamburg and Paris\ngold-silver prices. Three algorithms are combined to study this integration: a\nperiodization obtained with the SOM algorithm is confronted to the estimation\nof a two-regime Markov switching model, in order to give an interpretation of\nthe changes of regime; in the same time change-points are identified over the\nwhole period providing a more precise interpretation of the various types of\nregulation.\n", "category": [5, 6]}
{"abstract": "  This paper develops a two-dimensional structural framework for valuing credit\ndefault swaps and corporate bonds in the presence of default contagion.\nModelling the values of related firms as correlated geometric Brownian motions\nwith exponential default barriers, analytical formulae are obtained for both\ncredit default swap spreads and corporate bond yields. The credit dependence\nstructure is influenced by both a longer-term correlation structure as well as\nby the possibility of default contagion. In this way, the model is able to\ngenerate a diverse range of shapes for the term structure of credit spreads\nusing realistic values for input parameters.\n", "category": [5, 2]}
{"abstract": "  We study a new ensemble of random correlation matrices related to\nmultivariate Student (or more generally elliptic) random variables. We\nestablish the exact density of states of empirical correlation matrices that\ngeneralizes the Marcenko-Pastur result. The comparison between the theoretical\ndensity of states in the Student case and empirical financial data is\nsurprisingly good, even if we are still able to detect systematic deviations.\nFinally, we compute explicitely the Kullback-Leibler entropies of empirical\nStudent matrices, which are found to be independent of the true correlation\nmatrix, as in the Gaussian case. We provide numerically exact values for these\nKullback-Leibler entropies.\n", "category": [5, 3, 3]}
{"abstract": "  We model a closed economic system with interactions that generates the\nfeatures of empirical wealth distribution across all wealth brackets, namely a\nGibbsian trend in the lower and middle wealth range and a Pareto trend in the\nhigher range, by simply limiting the an agents' interaction to only agents with\nnearly the same wealth. To do this, we introduce a parameter BETA that limits\nthe range on the wealth of a partner with which an agent is allowed to\ninteract. We show that this wealth-limited interaction is enough to distribute\nwealth in a purely power law trend. If the interaction is not wealth limited,\nthe wealth distribution is expectedly Gibbsian. The value of BETA where the\ntransition from a purely Gibbsian law to a purely power law distribution\nhappens depends on whether the choice of interaction partner is mutual nor not.\nFor a non-mutual choice, where the richer agent gets to decide, the transition\nhappens at BETA=1.0. For a mutual choice, the transition is at BETA= 0.60. In\norder to generate a mixed Gibbs-Pareto distribution, we apply another\nwealth-based rule that depends on the parameter w_limit. An agent whose wealth\nis below w_limit can choose any partner to interact with, while an agent whose\nwealth is above w_limit is subject to the wealth-limited range in his choice of\npartner. A Gibbs-Pareto distribution appears if both these wealth-based rules\nare applied.\n", "category": [5, 3, 3]}
{"abstract": "  We study a minimalist kinetic model for economies. A system of agents with\nlocal trading rules display emergent demand behaviour. We examine the resulting\nwealth distribution to look for non-thermal behaviour. We compare and contrast\nthis model with other similar models.\n", "category": [5, 3]}
{"abstract": "  The so called \"globalization\" process (i.e. the inexorable integration of\nmarkets, currencies, nation-states, technologies and the intensification of\nconsciousness of the world as a whole) has a behavior exactly equivalent to a\nsystem that is tending to a maximum entropy state. This globalization process\nobeys a collective welfare principle in where the maximum payoff is given by\nthe equilibrium of the system and its stability by the maximization of the\nwelfare of the collective besides the individual welfare. This let us predict\nthe apparition of big common markets and strong common currencies. They will\nreach the \"equilibrium\" by decreasing its number until they reach a state\ncharacterized by only one common currency and only one big common community\naround the world.\n", "category": [5, 3]}
{"abstract": "  We propose the point process model as the Poissonian-like stochastic sequence\nwith slowly diffusing mean rate and adjust the parameters of the model to the\nempirical data of trading activity for 26 stocks traded on NYSE. The proposed\nscaled stochastic differential equation provides the universal description of\nthe trading activities with the same parameters applicable for all stocks.\n", "category": [5, 3, 3]}
{"abstract": "  We apply the potential force estimation method to artificial time series of\nmarket price produced by a deterministic dealer model. We find that dealers'\nfeedback of linear prediction of market price based on the latest mean price\nchanges plays the central role in the market's potential force. When markets\nare dominated by dealers with positive feedback the resulting potential force\nis repulsive, while the effect of negative feedback enhances the attractive\npotential force.\n", "category": [5, 3, 3]}
{"abstract": "  The problem of estimation error in portfolio optimization is discussed, in\nthe limit where the portfolio size N and the sample size T go to infinity such\nthat their ratio is fixed. The estimation error strongly depends on the ratio\nN/T and diverges for a critical value of this parameter. This divergence is the\nmanifestation of an algorithmic phase transition, it is accompanied by a number\nof critical phenomena, and displays universality. As the structure of a large\nnumber of multidimensional regression and modelling problems is very similar to\nportfolio optimization, the scope of the above observations extends far beyond\nfinance, and covers a large number of problems in operations research, machine\nlearning, bioinformatics, medical science, economics, and technology.\n", "category": [5, 3]}
{"abstract": "  Employing data on the assessed value of land in 1974--2007 Japan, we exhibit\na quasistatically varying log-normal distribution in the middle scale region.\nIn the derivation, a Non-Gibrat's law under the detailed quasi-balance is\nadopted together with two approximations. The resultant distribution is\npower-law with the varying exponent in the large scale region and the\nquasistatic log-normal distribution with the varying standard deviation in the\nmiddle scale region. In the distribution, not only the change of the exponent\nbut also the change of the standard deviation depends on the parameter of the\ndetailed quasi-balance. These results are consistently confirmed by the\nempirical data.\n", "category": [5, 3]}
{"abstract": "  The Mutual Fund Theorem (MFT) is considered in a general semimartingale\nfinancial market S with a finite time horizon T, where agents maximize expected\nutility of terminal wealth. It is established that:\n  1) Let N be the wealth process of the num\\'eraire portfolio (i.e. the optimal\nportfolio for the log utility). If any path-independent option with maturity T\nwritten on the num\\'eraire portfolio can be replicated by trading \\emph{only}\nin N, then the (MFT) holds true for general utility functions, and the\nnum\\'eraire portfolio may serve as mutual fund. This generalizes Merton's\nclassical result on Black-Scholes markets.\n  Conversely, under a supplementary weak completeness assumption, we show that\nthe validity of the (MFT) for general utility functions implies the same\nreplicability property for options on the num\\'eraire portfolio described\nabove.\n  2) If for a given class of utility functions (i.e. investors) the (MFT) holds\ntrue in all complete Brownian financial markets S, then all investors use the\nsame utility function U, which must be of HARA type. This is a result in the\nspirit of the classical work by Cass and Stiglitz.\n", "category": [5, 2]}
{"abstract": "  In order to investigate whether government regulations against corruption can\naffect the economic growth of a country, we analyze the dependence between\nGross Domestic Product (GDP) per capita growth rates and changes in the\nCorruption Perceptions Index (CPI). For the period 1999-2004 on average for all\ncountries in the world, we find that an increase of CPI by one unit leads to an\nincrease of the annual GDP per capita by 1.7 %. By regressing only European\ntransition countries, we find that $\\Delta$CPI = 1 generates increase of the\nannual GDP per capita by 2.4 %. We also analyze the relation between foreign\ndirect investments received by different countries and CPI, and we find a\nstatistically significant power-law functional dependence between foreign\ndirect investment per capita and the country corruption level measured by the\nCPI. We introduce a new measure to quantify the relative corruption between\ncountries based on their respective wealth as measured by GDP per capita.\n", "category": [5, 3, 3]}
{"abstract": "  We use high-frequency data of 1364 Chinese A-share stocks traded on the\nShanghai Stock Exchange and Shenzhen Stock Exchange to investigate the intraday\npatterns in the bid-ask spreads. The daily periodicity in the spread time\nseries is confirmed by Lomb analysis and the intraday bid-ask spreads are found\nto exhibit $L$-shaped pattern with idiosyncratic fine structure. The intraday\nspread of individual stocks relaxes as a power law within the first hour of the\ncontinuous double auction from 9:30AM to 10:30AM with exponents\n$\\beta_{\\rm{SHSE}}=0.19\\pm0.069$ for the Shanghai market and\n$\\beta_{\\rm{SZSE}}=0.18\\pm0.067$ for the Shenzhen market. The power-law\nrelaxation exponent $\\beta$ of individual stocks is roughly normally\ndistributed. There is evidence showing that the accumulation of information\nwidening the spread is an endogenous process.\n", "category": [5, 3]}
{"abstract": "  The condition for stationary increments, not scaling, detemines long time\npair autocorrelations. An incorrect assumption of stationary increments\ngenerates spurious stylized facts, fat tails and a Hurst exponent H_s=1/2, when\nthe increments are nonstationary, as they are in FX markets. The\nnonstationarity arises from systematic uneveness in noise traders' behavior.\nSpurious results arise mathematically from using a log increment with a\n'sliding window'. We explain why a hard to beat market demands martingale\ndynamics , and martingales with nonlinear variance generate nonstationary\nincrements. The nonstationarity is exhibited directly for Euro/Dollar FX data.\nWe observe that the Hurst exponent H_s generated by the using the sliding\nwindow technique on a time series plays the same role as does Mandelbrot's\nJoseph exponent. Finally, Mandelbrot originally assumed that the 'badly\nbehaved' second moment of cotton returns is due to fat tails, but that\nnonconvergent behavior is instead direct evidence for nonstationary increments.\nSummarizing, the evidence for scaling and fat tails as the basis for\neconophysics and financial economics is provided neither by FX markets nor by\ncotton price data.\n", "category": [5, 3, 3]}
{"abstract": "  We extend the fundamental theorem of asset pricing to a model where the risky\nstock is subject to proportional transaction costs in the form of bid-ask\nspreads and the bank account has different interest rates for borrowing and\nlending. We show that such a model is free of arbitrage if and only if one can\nembed in it a friction-free model that is itself free of arbitrage, in the\nsense that there exists an artificial friction-free price for the stock between\nits bid and ask prices and an artificial interest rate between the borrowing\nand lending interest rates such that, if one discounts this stock price by this\ninterest rate, then the resulting process is a martingale under some\nnon-degenerate probability measure. Restricting ourselves to the simple case of\na finite number of time steps and a finite number of possible outcomes for the\nstock price, the proof follows by combining classical arguments based on\nfinite-dimensional separation theorems with duality results from linear\noptimisation.\n", "category": [5, 2]}
{"abstract": "  We consider a financial contract that delivers a single cash flow given by\nthe terminal value of a cumulative gains process. The problem of modelling and\npricing such an asset and associated derivatives is important, for example, in\nthe determination of optimal insurance claims reserve policies, and in the\npricing of reinsurance contracts. In the insurance setting, the aggregate\nclaims play the role of the cumulative gains, and the terminal cash flow\nrepresents the totality of the claims payable for the given accounting period.\nA similar example arises when we consider the accumulation of losses in a\ncredit portfolio, and value a contract that pays an amount equal to the\ntotality of the losses over a given time interval. An explicit expression for\nthe value process is obtained. The price of an Arrow-Debreu security on the\ncumulative gains process is determined, and is used to obtain a closed-form\nexpression for the price of a European-style option on the value of the asset.\nThe results obtained make use of various remarkable properties of the gamma\nbridge process, and are applicable to a wide variety of financial products\nbased on cumulative gains processes such as aggregate claims, credit portfolio\nlosses, defined-benefit pension schemes, emissions, and rainfall.\n", "category": [5, 2]}
{"abstract": "  Mathematical models for financial asset prices which include, for example,\nstochastic volatility or jumps are incomplete in that derivative securities are\ngenerally not replicable by trading in the underlying. In earlier work (2004)\nthe first author provided a geometric condition under which trading in the\nunderlying and a finite number of vanilla options completes the market. We\ncomplement this result in several ways. First, we show that the geometric\ncondition is not necessary and a weaker, necessary and sufficient, condition is\npresented. While this condition is generally not directly verifiable, we show\nthat it simplifies to matrix non-degeneracy in a single point when the pricing\nfunctions are real analytic functions. In particular, any stochastic volatility\nmodel is then completed with an arbitrary European type option. Further, we\nshow that adding path-dependent options such as a variance swap to the set of\nprimary assets, instead of plain vanilla options, also completes the market.\n", "category": [5, 2]}
{"abstract": "  We propose a class of discrete-time stochastic models for the pricing of\ninflation-linked assets. The paper begins with an axiomatic scheme for asset\npricing and interest rate theory in a discrete-time setting. The first axiom\nintroduces a \"risk-free\" asset, and the second axiom determines the\nintertemporal pricing relations that hold for dividend-paying assets. The\nnominal and real pricing kernels, in terms of which the price index can be\nexpressed, are then modelled by introducing a Sidrauski-type utility function\ndepending on (a) the aggregate rate of consumption, and (b) the aggregate rate\nof real liquidity benefit conferred by the money supply. Consumption and money\nsupply policies are chosen such that the expected joint utility obtained over a\nspecified time horizon is maximised subject to a budget constraint that takes\ninto account the \"value\" of the liquidity benefit associated with the money\nsupply. For any choice of the bivariate utility function, the resulting model\ndetermines a relation between the rate of consumption, the price level, and the\nmoney supply. The model also produces explicit expressions for the real and\nnominal pricing kernels, and hence establishes a basis for the valuation of\ninflation-linked securities.\n", "category": [5, 2]}
{"abstract": "  The latest generation of volatility derivatives goes beyond variance and\nvolatility swaps and probes our ability to price realized variance and sojourn\ntimes along bridges for the underlying stock price process. In this paper, we\ngive an operator algebraic treatment of this problem based on Dyson expansions\nand moment methods and discuss applications to exotic volatility derivatives.\nThe methods are quite flexible and allow for a specification of the underlying\nprocess which is semi-parametric or even non-parametric, including\nstate-dependent local volatility, jumps, stochastic volatility and regime\nswitching. We find that volatility derivatives are particularly well suited to\nbe treated with moment methods, whereby one extrapolates the distribution of\nthe relevant path functionals on the basis of a few moments. We consider a\nnumber of exotics such as variance knockouts, conditional corridor variance\nswaps, gamma swaps and variance swaptions and give valuation formulas in\ndetail.\n", "category": [5, 2, 2]}
{"abstract": "  This paper proposes the k-generalized distribution as a model for describing\nthe distribution and dispersion of income within a population. Formulas for the\nshape, moments and standard tools for inequality measurement - such as the\nLorenz curve and the Gini coefficient - are given. A method for parameter\nestimation is also discussed. The model is shown to fit extremely well the data\non personal income distribution in Australia and the United States.\n", "category": [5, 3]}
{"abstract": "  The new notion of maturity-independent risk measures is introduced and\ncontrasted with the existing risk measurement concepts. It is shown, by means\nof two examples, one set on a finite probability space and the other in a\ndiffusion framework, that, surprisingly, some of the widely utilized risk\nmeasures cannot be used to build maturity-independent counterparts. We\nconstruct a large class of maturity-independent risk measures and give\nrepresentative examples in both continuous- and discrete-time financial models.\n", "category": [5, 2, 2]}
{"abstract": "  The t copula is often used in risk management as it allows for modelling tail\ndependence between risks and it is simple to simulate and calibrate. However,\nthe use of a standard t copula is often criticized due to its restriction of\nhaving a single parameter for the degrees of freedom (dof) that may limit its\ncapability to model the tail dependence structure in a multivariate case. To\novercome this problem, grouped t copula was proposed recently, where risks are\ngrouped a priori in such a way that each group has a standard t copula with its\nspecific dof parameter. In this paper we propose the use of a grouped t copula,\nwhere each group consists of one risk factor only, so that a priori grouping is\nnot required. The copula characteristics in the bivariate case are studied. We\nexplain simulation and calibration procedures, including a simulation study on\nfinite sample properties of the maximum likelihood estimators and Kendall's tau\napproximation. This new copula can be significantly different from the standard\nt copula in terms of risk measures such as tail dependence, value at risk and\nexpected shortfall.\n  Keywords: grouped t copula, tail dependence, risk management.\n", "category": [2, 5, 5]}
{"abstract": "  A stochastic theory for the toppling activity in sandpile models is\ndeveloped, based on a simple mean-field assumption about the toppling process.\nThe theory describes the process as an anti-persistent Gaussian walk, where the\ndiffusion coefficient is proportional to the activity. It is formulated as a\ngeneralization of the It\\^{o} stochastic differential equation with an\nanti-persistent fractional Gaussian noise source. An essential element of the\ntheory is re-scaling to obtain a proper thermodynamic limit, and it captures\nall temporal features of the toppling process obtained by numerical simulation\nof the Bak-Tang-Wiesenfeld sandpile in this limit.\n", "category": [5, 3, 3]}
{"abstract": "  A new class of risk measures called cash sub-additive risk measures is\nintroduced to assess the risk of future financial, nonfinancial and insurance\npositions. The debated cash additive axiom is relaxed into the cash sub\nadditive axiom to preserve the original difference between the numeraire of the\ncurrent reserve amounts and future positions. Consequently, cash sub-additive\nrisk measures can model stochastic and/or ambiguous interest rates or\ndefaultable contingent claims. Practical examples are presented and in such\ncontexts cash additive risk measures cannot be used. Several representations of\nthe cash sub-additive risk measures are provided. The new risk measures are\ncharacterized by penalty functions defined on a set of sub-linear probability\nmeasures and can be represented using penalty functions associated with cash\nadditive risk measures defined on some extended spaces. The issue of the\noptimal risk transfer is studied in the new framework using inf-convolution\ntechniques. Examples of dynamic cash sub-additive risk measures are provided\nvia BSDEs where the generator can locally depend on the level of the cash\nsub-additive risk measure.\n", "category": [5, 2]}
{"abstract": "  The purpose of this paper is to analyze and compute the early exercise\nboundary for a class of nonlinear Black--Scholes equations with a nonlinear\nvolatility which can be a function of the second derivative of the option price\nitself. A motivation for studying the nonlinear Black--Scholes equation with a\nnonlinear volatility arises from option pricing models taking into account e.g.\nnontrivial transaction costs, investor's preferences, feedback and illiquid\nmarkets effects and risk from a volatile (unprotected) portfolio. We present a\nnew method how to transform the free boundary problem for the early exercise\nboundary position into a solution of a time depending nonlinear parabolic\nequation defined on a fixed domain. We furthermore propose an iterative\nnumerical scheme that can be used to find an approximation of the free\nboundary. We present results of numerical approximation of the early exercise\nboundary for various types of nonlinear Black--Scholes equations and we discuss\ndependence of the free boundary on various model parameters.\n", "category": [5, 2]}
{"abstract": "  The Random Parameters model was proposed to explain the structure of the\ncovariance matrix in problems where most, but not all, of the eigenvalues of\nthe covariance matrix can be explained by Random Matrix Theory. In this\narticle, we explore other properties of the model, like the scaling of its PDF\nas one take larger scales. Special attention is given to the multifractal\nstructure of the model time series, which revealed a scaling structure\ncompatible with the known stylized facts for a reasonable choice of the\nparameter values.\n", "category": [5, 3]}
{"abstract": "  A recently introduced Importance Sampling strategy based on a least squares\noptimization is applied to the Monte Carlo simulation of Libor Market Models.\nSuch Least Squares Importance Sampling (LSIS) allows the automatic optimization\nof the sampling distribution within a trial class by means of a quick\npresimulation algorithm of straightforward implementation. With several\nnumerical examples we show that LSIS can be extremely effective in reducing the\nvariance of Monte Carlo estimators often resulting, especially when combined\nwith stratified sampling, in computational speed-ups of orders of magnitude.\n", "category": [5, 3, 3]}
{"abstract": "  In order to pursue the issue of the relation between the financial\ncross-correlations and the conventional Random Matrix Theory we analyse several\ncharacteristics of the stock market correlation matrices like the distribution\nof eigenvalues, the cross-correlations among signs of the returns, the\nvolatility cross-correlations, and the multifractal characteristics of the\nprincipal values. The results indicate that the stock market dynamics is not\nsimply decomposable into 'market', 'sectors', and the Wishart random bulk. This\nclearly is seen when the time series used to construct the correlation matrices\nare sufficiently long and thus the measurement noise suppressed. Instead, a\nhierarchically convoluted and highly nonlinear organization of the market\nemerges and indicates that the relevant information about the whole market is\nencoded already in its constituents.\n", "category": [5, 3]}
{"abstract": "  The existence of forbidden patterns, i.e., certain missing sequences in a\ngiven time series, is a recently proposed instrument of potential application\nin the study of time series. Forbidden patterns are related to the permutation\nentropy, which has the basic properties of classic chaos indicators, thus\nallowing to separate deterministic (usually chaotic) from random series;\nhowever, it requires less values of the series to be calculated, and it is\nsuitable for using with small datasets. In this Letter, the appearance of\nforbidden patterns is studied in different economical indicators like stock\nindices (Dow Jones Industrial Average and Nasdaq Composite), NYSE stocks (IBM\nand Boeing) and others (10-year Bond interest rate), to find evidences of\ndeterministic behavior in their evolutions.\n", "category": [5, 3, 3]}
{"abstract": "  An evolutionarily stable strategy (ESS) is an equilibrium strategy that is\nimmune to invasions by rare alternative (``mutant'') strategies. Unlike Nash\nequilibria, ESS do not always exist in finite games. In this paper we address\nthe question of what happens when the size of the game increases: does an ESS\nexist for ``almost every large'' game? Letting the entries in the $n\\times n$\ngame matrix be independently randomly chosen according to a distribution $F$,\nwe study the number of ESS with support of size $2.$ In particular, we show\nthat, as $n\\to \\infty$, the probability of having such an ESS: (i) converges to\n1 for distributions $F$ with ``exponential and faster decreasing tails'' (e.g.,\nuniform, normal, exponential); and (ii) converges to $1-1/\\sqrt{e}$ for\ndistributions $F$ with ``slower than exponential decreasing tails'' (e.g.,\nlognormal, Pareto, Cauchy). Our results also imply that the expected number of\nvertices of the convex hull of $n$ random points in the plane converges to\ninfinity for the distributions in (i), and to 4 for the distributions in (ii).\n", "category": [2, 7]}
{"abstract": "  Suppose that a target function is monotonic, namely, weakly increasing, and\nan available original estimate of this target function is not weakly\nincreasing. Rearrangements, univariate and multivariate, transform the original\nestimate to a monotonic estimate that always lies closer in common metrics to\nthe target function. Furthermore, suppose an original simultaneous confidence\ninterval, which covers the target function with probability at least\n$1-\\alpha$, is defined by an upper and lower end-point functions that are not\nweakly increasing. Then the rearranged confidence interval, defined by the\nrearranged upper and lower end-point functions, is shorter in length in common\nnorms than the original interval and also covers the target function with\nprobability at least $1-\\alpha$. We demonstrate the utility of the improved\npoint and interval estimates with an age-height growth chart example.\n", "category": [2, 7, 2, 6, 6]}
{"abstract": "  Counterfactual distributions are important ingredients for policy analysis\nand decomposition analysis in empirical economics. In this article we develop\nmodeling and inference tools for counterfactual distributions based on\nregression methods. The counterfactual scenarios that we consider consist of\nceteris paribus changes in either the distribution of covariates related to the\noutcome of interest or the conditional distribution of the outcome given\ncovariates. For either of these scenarios we derive joint functional central\nlimit theorems and bootstrap validity results for regression-based estimators\nof the status quo and counterfactual outcome distributions. These results allow\nus to construct simultaneous confidence sets for function-valued effects of the\ncounterfactual changes, including the effects on the entire distribution and\nquantile functions of the outcome as well as on related functionals. These\nconfidence sets can be used to test functional hypotheses such as no-effect,\npositive effect, or stochastic dominance. Our theory applies to general\ncounterfactual changes and covers the main regression methods including\nclassical, quantile, duration, and distribution regressions. We illustrate the\nresults with an empirical application to wage decompositions using data for the\nUnited States.\n  As a part of developing the main results, we introduce distribution\nregression as a comprehensive and flexible tool for modeling and estimating the\n\\textit{entire} conditional distribution. We show that distribution regression\nencompasses the Cox duration regression and represents a useful alternative to\nquantile regression. We establish functional central limit theorems and\nbootstrap validity results for the empirical distribution regression process\nand various related functionals.\n", "category": [6, 7, 6]}
{"abstract": "  Nonseparable panel models are important in a variety of economic settings,\nincluding discrete choice. This paper gives identification and estimation\nresults for nonseparable models under time homogeneity conditions that are like\n\"time is randomly assigned\" or \"time is an instrument.\" Partial identification\nresults for average and quantile effects are given for discrete regressors,\nunder static or dynamic conditions, in fully nonparametric and in\nsemiparametric models, with time effects. It is shown that the usual, linear,\nfixed-effects estimator is not a consistent estimator of the identified average\neffect, and a consistent estimator is given. A simple estimator of identified\nquantile treatment effects is given, providing a solution to the important\nproblem of estimating quantile treatment effects from panel data. Bounds for\noverall effects in static and dynamic models are given. The dynamic bounds\nprovide a partial identification solution to the important problem of\nestimating the effect of state dependence in the presence of unobserved\nheterogeneity. The impact of $T$, the number of time periods, is shown by\nderiving shrinkage rates for the identified set as $T$ grows. We also consider\nsemiparametric, discrete-choice models and find that semiparametric panel\nbounds can be much tighter than nonparametric bounds.\nComputationally-convenient methods for semiparametric models are presented. We\npropose a novel inference method that applies in panel data and other settings\nand show that it produces uniformly valid confidence regions in large samples.\nWe give empirical illustrations.\n", "category": [6, 7, 2, 6, 6]}
{"abstract": "  We consider median regression and, more generally, a possibly infinite\ncollection of quantile regressions in high-dimensional sparse models. In these\nmodels the overall number of regressors $p$ is very large, possibly larger than\nthe sample size $n$, but only $s$ of these regressors have non-zero impact on\nthe conditional quantile of the response variable, where $s$ grows slower than\n$n$. We consider quantile regression penalized by the $\\ell_1$-norm of\ncoefficients ($\\ell_1$-QR). First, we show that $\\ell_1$-QR is consistent at\nthe rate $\\sqrt{s/n} \\sqrt{\\log p}$. The overall number of regressors $p$\naffects the rate only through the $\\log p$ factor, thus allowing nearly\nexponential growth in the number of zero-impact regressors. The rate result\nholds under relatively weak conditions, requiring that $s/n$ converges to zero\nat a super-logarithmic speed and that regularization parameter satisfies\ncertain theoretical constraints. Second, we propose a pivotal, data-driven\nchoice of the regularization parameter and show that it satisfies these\ntheoretical constraints. Third, we show that $\\ell_1$-QR correctly selects the\ntrue minimal model as a valid submodel, when the non-zero coefficients of the\ntrue model are well separated from zero. We also show that the number of\nnon-zero coefficients in $\\ell_1$-QR is of same stochastic order as $s$.\nFourth, we analyze the rate of convergence of a two-step estimator that applies\nordinary quantile regression to the selected model. Fifth, we evaluate the\nperformance of $\\ell_1$-QR in a Monte-Carlo experiment, and illustrate its use\non an international economic growth application.\n", "category": [2, 7, 2, 6, 6]}
{"abstract": "  This work studies the large sample properties of the posterior-based\ninference in the curved exponential family under increasing dimension. The\ncurved structure arises from the imposition of various restrictions on the\nmodel, such as moment restrictions, and plays a fundamental role in\neconometrics and others branches of data analysis. We establish conditions\nunder which the posterior distribution is approximately normal, which in turn\nimplies various good properties of estimation and inference procedures based on\nthe posterior. In the process we also revisit and improve upon previous results\nfor the exponential family under increasing dimension by making use of\nconcentration of measure. We also discuss a variety of applications to\nhigh-dimensional versions of the classical econometric models including the\nmultinomial model with moment restrictions, seemingly unrelated regression\nequations, and single structural equation models. In our analysis, both the\nparameter dimension and the number of moments are increasing with the sample\nsize.\n", "category": [2, 7, 2, 6, 6]}
{"abstract": "  In most contemporary approaches to decision making, a decision problem is\ndescribed by a sets of states and set of outcomes, and a rich set of acts,\nwhich are functions from states to outcomes over which the decision maker (DM)\nhas preferences. Most interesting decision problems, however, do not come with\na state space and an outcome space. Indeed, in complex problems it is often far\nfrom clear what the state and outcome spaces would be. We present an\nalternative foundation for decision making, in which the primitive objects of\nchoice are syntactic programs. A representation theorem is proved in the spirit\nof standard representation theorems, showing that if the DM's preference\nrelation on objects of choice satisfies appropriate axioms, then there exist a\nset S of states, a set O of outcomes, a way of interpreting the objects of\nchoice as functions from S to O, a probability on S, and a utility function on\nO, such that the DM prefers choice a to choice b if and only if the expected\nutility of a is higher than that of b. Thus, the state space and outcome space\nare subjective, just like the probability and utility; they are not part of the\ndescription of the problem. In principle, a modeler can test for SEU behavior\nwithout having access to states or outcomes. We illustrate the power of our\napproach by showing that it can capture decision makers who are subject to\nframing effects.\n", "category": [0, 0, 7]}
{"abstract": "  An important issue with oversampled FIR analysis filter banks (FBs) is to\ndetermine inverse synthesis FBs, when they exist. Given any complex oversampled\nFIR analysis FB, we first provide an algorithm to determine whether there\nexists an inverse FIR synthesis system. We also provide a method to ensure the\nHermitian symmetry property on the synthesis side, which is serviceable to\nprocessing real-valued signals. As an invertible analysis scheme corresponds to\na redundant decomposition, there is no unique inverse FB. Given a particular\nsolution, we parameterize the whole family of inverses through a null space\nprojection. The resulting reduced parameter set simplifies design procedures,\nsince the perfect reconstruction constrained optimization problem is recast as\nan unconstrained optimization problem. The design of optimized synthesis FBs\nbased on time or frequency localization criteria is then investigated, using a\nsimple yet efficient gradient algorithm.\n", "category": [0, 0, 1, 2, 2]}
{"abstract": "  Arrow's theorem implies that a social choice function satisfying\nTransitivity, the Pareto Principle (Unanimity) and Independence of Irrelevant\nAlternatives (IIA) must be dictatorial. When non-strict preferences are\nallowed, a dictatorial social choice function is defined as a function for\nwhich there exists a single voter whose strict preferences are followed. This\ndefinition allows for many different dictatorial functions. In particular, we\nconstruct examples of dictatorial functions which do not satisfy Transitivity\nand IIA. Thus Arrow's theorem, in the case of non-strict preferences, does not\nprovide a complete characterization of all social choice functions satisfying\nTransitivity, the Pareto Principle, and IIA.\n  The main results of this article provide such a characterization for Arrow's\ntheorem, as well as for follow up results by Wilson. In particular, we\nstrengthen Arrow's and Wilson's result by giving an exact if and only if\ncondition for a function to satisfy Transitivity and IIA (and the Pareto\nPrinciple). Additionally, we derive formulas for the number of functions\nsatisfying these conditions.\n", "category": [2, 7]}
{"abstract": "  Quantile regression is an increasingly important empirical tool in economics\nand other sciences for analyzing the impact of a set of regressors on the\nconditional distribution of an outcome. Extremal quantile regression, or\nquantile regression applied to the tails, is of interest in many economic and\nfinancial applications, such as conditional value-at-risk, production\nefficiency, and adjustment bands in (S,s) models. In this paper we provide\nfeasible inference tools for extremal conditional quantile models that rely\nupon extreme value approximations to the distribution of self-normalized\nquantile regression statistics. The methods are simple to implement and can be\nof independent interest even in the non-regression case. We illustrate the\nresults with two empirical examples analyzing extreme fluctuations of a stock\nreturn and extremely low percentiles of live infants' birthweights in the range\nbetween 250 and 1500 grams.\n", "category": [6, 7, 2, 5, 6]}
{"abstract": "  Non-symmetric rectangular correlation matrices occur in many problems in\neconomics. We test the method of extracting statistically meaningful\ncorrelations between input and output variables of large dimensionality and\nbuild a toy model for artificially included correlations in large random time\nseries.The results are then applied to analysis of polish macroeconomic data\nand can be used as an alternative to classical cointegration approach.\n", "category": [3, 7, 5, 5, 6]}
{"abstract": "  We develop results for the use of Lasso and Post-Lasso methods to form\nfirst-stage predictions and estimate optimal instruments in linear instrumental\nvariables (IV) models with many instruments, $p$. Our results apply even when\n$p$ is much larger than the sample size, $n$. We show that the IV estimator\nbased on using Lasso or Post-Lasso in the first stage is root-n consistent and\nasymptotically normal when the first-stage is approximately sparse; i.e. when\nthe conditional expectation of the endogenous variables given the instruments\ncan be well-approximated by a relatively small set of variables whose\nidentities may be unknown. We also show the estimator is semi-parametrically\nefficient when the structural error is homoscedastic. Notably our results allow\nfor imperfect model selection, and do not rely upon the unrealistic \"beta-min\"\nconditions that are widely used to establish validity of inference following\nmodel selection. In simulation experiments, the Lasso-based IV estimator with a\ndata-driven penalty performs well compared to recently advocated\nmany-instrument-robust procedures. In an empirical example dealing with the\neffect of judicial eminent domain decisions on economic outcomes, the\nLasso-based IV estimator outperforms an intuitive benchmark.\n  In developing the IV results, we establish a series of new results for Lasso\nand Post-Lasso estimators of nonparametric conditional expectation functions\nwhich are of independent theoretical and practical interest. We construct a\nmodification of Lasso designed to deal with non-Gaussian, heteroscedastic\ndisturbances which uses a data-weighted $\\ell_1$-penalty function. Using\nmoderate deviation theory for self-normalized sums, we provide convergence\nrates for the resulting Lasso and Post-Lasso estimators that are as sharp as\nthe corresponding rates in the homoscedastic Gaussian case under the condition\nthat $\\log p = o(n^{1/3})$.\n", "category": [6, 7, 2, 6]}
{"abstract": "  This paper proposes a new approach to multi-sensor data fusion. It suggests\nthat aggregation of data from multiple sensors can be done more efficiently\nwhen we consider information about sensors' different characteristics. Similar\nto most research on effective sensors' characteristics, especially in control\nsystems, our focus is on sensors' accuracy and frequency response. A rule-based\nfuzzy system is presented for fusion of raw data obtained from the sensors that\nhave complement characteristics in accuracy and bandwidth. Furthermore, a fuzzy\npredictor system is suggested aiming for extreme accuracy which is a common\nneed in highly sensitive applications. Advantages of our proposed sensor fusion\nsystem are shown by simulation of a control system utilizing the fusion system\nfor output estimation.\n", "category": [1, 0]}
{"abstract": "  The Peirce quincuncial projection is a mapping of the surface of a sphere to\nthe interior of a square. It is a conformal map except for four points on the\nequator. These points of non-conformality cause significant artifacts in\nphotographic applications. In this paper, we propose an algorithm and\nuser-interface to mitigate these artifacts. Moreover, in order to facilitate an\ninteractive user-interface, we present a fast algorithm for calculating the\nPeirce quincuncial projection of spherical imagery. We then promote the Peirce\nquincuncial projection as a viable alternative to the more popular\nstereographic projection in some scenarios.\n", "category": [0, 0, 1]}
{"abstract": "  In this note, we propose to use sparse methods (e.g. LASSO, Post-LASSO,\nsqrt-LASSO, and Post-sqrt-LASSO) to form first-stage predictions and estimate\noptimal instruments in linear instrumental variables (IV) models with many\ninstruments in the canonical Gaussian case. The methods apply even when the\nnumber of instruments is much larger than the sample size. We derive asymptotic\ndistributions for the resulting IV estimators and provide conditions under\nwhich these sparsity-based IV estimators are asymptotically oracle-efficient.\nIn simulation experiments, a sparsity-based IV estimator with a data-driven\npenalty performs well compared to recently advocated many-instrument-robust\nprocedures. We illustrate the procedure in an empirical example using the\nAngrist and Krueger (1991) schooling data.\n", "category": [6, 7, 2, 6, 6]}
{"abstract": "  In this paper, we develop a new censored quantile instrumental variable\n(CQIV) estimator and describe its properties and computation. The CQIV\nestimator combines Powell (1986) censored quantile regression (CQR) to deal\nwith censoring, with a control variable approach to incorporate endogenous\nregressors. The CQIV estimator is obtained in two stages that are non-additive\nin the unobservables. The first stage estimates a non-additive model with\ninfinite dimensional parameters for the control variable, such as a quantile or\ndistribution regression model. The second stage estimates a non-additive\ncensored quantile regression model for the response variable of interest,\nincluding the estimated control variable to deal with endogeneity. For\ncomputation, we extend the algorithm for CQR developed by Chernozhukov and Hong\n(2002) to incorporate the estimation of the control variable. We give generic\nregularity conditions for asymptotic normality of the CQIV estimator and for\nthe validity of resampling methods to approximate its asymptotic distribution.\nWe verify these conditions for quantile and distribution regression estimation\nof the control variable. Our analysis covers two-stage (uncensored) quantile\nregression with non-additive first stage as an important special case. We\nillustrate the computation and applicability of the CQIV estimator with a\nMonte-Carlo numerical example and an empirical application on estimation of\nEngel curves for alcohol.\n", "category": [6, 7]}
{"abstract": "  Quantile regression (QR) is a principal regression method for analyzing the\nimpact of covariates on outcomes. The impact is described by the conditional\nquantile function and its functionals. In this paper we develop the\nnonparametric QR-series framework, covering many regressors as a special case,\nfor performing inference on the entire conditional quantile function and its\nlinear functionals. In this framework, we approximate the entire conditional\nquantile function by a linear combination of series terms with\nquantile-specific coefficients and estimate the function-valued coefficients\nfrom the data. We develop large sample theory for the QR-series coefficient\nprocess, namely we obtain uniform strong approximations to the QR-series\ncoefficient process by conditionally pivotal and Gaussian processes. Based on\nthese strong approximations, or couplings, we develop four resampling methods\n(pivotal, gradient bootstrap, Gaussian, and weighted bootstrap) that can be\nused for inference on the entire QR-series coefficient function.\n  We apply these results to obtain estimation and inference methods for linear\nfunctionals of the conditional quantile function, such as the conditional\nquantile function itself, its partial derivatives, average partial derivatives,\nand conditional average partial derivatives. Specifically, we obtain uniform\nrates of convergence and show how to use the four resampling methods mentioned\nabove for inference on the functionals. All of the above results are for\nfunction-valued parameters, holding uniformly in both the quantile index and\nthe covariate value, and covering the pointwise case as a by-product. We\ndemonstrate the practical utility of these results with an example, where we\nestimate the price elasticity function and test the Slutsky condition of the\nindividual demand for gasoline, as indexed by the individual unobserved\npropensity for gasoline consumption.\n", "category": [6, 7, 2, 6]}
{"abstract": "  In this chapter we discuss conceptually high dimensional sparse econometric\nmodels as well as estimation of these models using L1-penalization and\npost-L1-penalization methods. Focusing on linear and nonparametric regression\nframeworks, we discuss various econometric examples, present basic theoretical\nresults, and illustrate the concepts and methods with Monte Carlo simulations\nand an empirical application. In the application, we examine and confirm the\nempirical validity of the Solow-Swan model for international economic growth.\n", "category": [6, 7, 6]}
{"abstract": "  Biometric authentication systems that make use of signature verification\nmethods often render optimum performance only under limited and restricted\nconditions. Such methods utilize several training samples so as to achieve high\naccuracy. Moreover, several constraints are imposed on the end-user so that the\nsystem may work optimally, and as expected. For example, the user is made to\nsign within a small box, in order to limit their signature to a predefined set\nof dimensions, thus eliminating scaling. Moreover, the angular rotation with\nrespect to the referenced signature that will be inadvertently introduced as\nhuman error, hampers performance of biometric signature verification systems.\nTo eliminate this, traditionally, a user is asked to sign exactly on top of a\nreference line. In this paper, we propose a robust system that optimizes the\nsignature obtained from the user for a large range of variation in\nRotation-Scaling-Translation (RST) and resolves these error parameters in the\nuser signature according to the reference signature stored in the database.\n", "category": [0, 0, 0, 0, 1, 2]}
{"abstract": "  In recent times, communication through the internet has tremendously\nfacilitated the distribution of multimedia data. Although this is indubitably a\nboon, one of its repercussions is that it has also given impetus to the\nnotorious issue of online music piracy. Unethical attempts can also be made to\ndeliberately alter such copyrighted data and thus, misuse it. Copyright\nviolation by means of unauthorized distribution, as well as unauthorized\ntampering of copyrighted audio data is an important technological and research\nissue. Audio watermarking has been proposed as a solution to tackle this issue.\nThe main purpose of audio watermarking is to protect against possible threats\nto the audio data and in case of copyright violation or unauthorized tampering,\nauthenticity of such data can be disputed by virtue of audio watermarking.\n", "category": [0, 0, 0, 1, 2]}
{"abstract": "  Recognition systems are commonly designed to authenticate users at the access\ncontrol levels of a system. A number of voice recognition methods have been\ndeveloped using a pitch estimation process which are very vulnerable in low\nSignal to Noise Ratio (SNR) environments thus, these programs fail to provide\nthe desired level of accuracy and robustness. Also, most text independent\nspeaker recognition programs are incapable of coping with unauthorized attempts\nto gain access by tampering with the samples or reference database. The\nproposed text-independent voice recognition system makes use of multilevel\ncryptography to preserve data integrity while in transit or storage. Encryption\nand decryption follow a transform based approach layered with pseudorandom\nnoise addition whereas for pitch detection, a modified version of the\nautocorrelation pitch extraction algorithm is used. The experimental results\nshow that the proposed algorithm can decrypt the signal under test with\nexponentially reducing Mean Square Error over an increasing range of SNR.\nFurther, it outperforms the conventional algorithms in actual identification\ntasks even in noisy environments. The recognition rate thus obtained using the\nproposed method is compared with other conventional methods used for speaker\nidentification.\n", "category": [0, 0, 0, 1]}
{"abstract": "  In the domain of Biometrics, recognition systems based on iris, fingerprint\nor palm print scans etc. are often considered more dependable due to extremely\nlow variance in the properties of these entities with respect to time. However,\nover the last decade data processing capability of computers has increased\nmanifold, which has made real-time video content analysis possible. This shows\nthat the need of the hour is a robust and highly automated Face Detection and\nRecognition algorithm with credible accuracy rate. The proposed Face Detection\nand Recognition system using Discrete Wavelet Transform (DWT) accepts face\nframes as input from a database containing images from low cost devices such as\nVGA cameras, webcams or even CCTV's, where image quality is inferior. Face\nregion is then detected using properties of L*a*b* color space and only Frontal\nFace is extracted such that all additional background is eliminated. Further,\nthis extracted image is converted to grayscale and its dimensions are resized\nto 128 x 128 pixels. DWT is then applied to entire image to obtain the\ncoefficients. Recognition is carried out by comparison of the DWT coefficients\nbelonging to the test image with those of the registered reference image. On\ncomparison, Euclidean distance classifier is deployed to validate the test\nimage from the database. Accuracy for various levels of DWT Decomposition is\nobtained and hence, compared.\n", "category": [0, 0, 1]}
{"abstract": "  This article is about estimation and inference methods for high dimensional\nsparse (HDS) regression models in econometrics. High dimensional sparse models\narise in situations where many regressors (or series terms) are available and\nthe regression function is well-approximated by a parsimonious, yet unknown set\nof regressors. The latter condition makes it possible to estimate the entire\nregression function effectively by searching for approximately the right set of\nregressors. We discuss methods for identifying this set of regressors and\nestimating their coefficients based on $\\ell_1$-penalization and describe key\ntheoretical results. In order to capture realistic practical situations, we\nexpressly allow for imperfect selection of regressors and study the impact of\nthis imperfect selection on estimation and inference results. We focus the main\npart of the article on the use of HDS models and methods in the instrumental\nvariables model and the partially linear model. We present a set of novel\ninference results for these models and illustrate their use with applications\nto returns to schooling and growth regression.\n", "category": [6, 7, 6]}
{"abstract": "  We propose robust methods for inference on the effect of a treatment variable\non a scalar outcome in the presence of very many controls. Our setting is a\npartially linear model with possibly non-Gaussian and heteroscedastic\ndisturbances. Our analysis allows the number of controls to be much larger than\nthe sample size. To make informative inference feasible, we require the model\nto be approximately sparse; that is, we require that the effect of confounding\nfactors can be controlled for up to a small approximation error by conditioning\non a relatively small number of controls whose identities are unknown. The\nlatter condition makes it possible to estimate the treatment effect by\nselecting approximately the right set of controls. We develop a novel\nestimation and uniformly valid inference method for the treatment effect in\nthis setting, called the \"post-double-selection\" method. Our results apply to\nLasso-type methods used for covariate selection as well as to any other model\nselection method that is able to find a sparse model with good approximation\nproperties.\n  The main attractive feature of our method is that it allows for imperfect\nselection of the controls and provides confidence intervals that are valid\nuniformly across a large class of models. In contrast, standard post-model\nselection estimators fail to provide uniform inference even in simple cases\nwith a small, fixed number of controls. Thus our method resolves the problem of\nuniform inference after model selection for a large, interesting class of\nmodels. We illustrate the use of the developed methods with numerical\nsimulations and an application to the effect of abortion on crime rates.\n", "category": [6, 7, 6]}
{"abstract": "  Maximizing the revenue from selling _more than one_ good (or item) to a\nsingle buyer is a notoriously difficult problem, in stark contrast to the\none-good case. For two goods, we show that simple \"one-dimensional\" mechanisms,\nsuch as selling the goods separately, _guarantee_ at least 73% of the optimal\nrevenue when the valuations of the two goods are independent and identically\ndistributed, and at least $50\\%$ when they are independent. For the case of\n$k>2$ independent goods, we show that selling them separately guarantees at\nleast a $c/\\log^2 k$ fraction of the optimal revenue; and, for independent and\nidentically distributed goods, we show that selling them as one bundle\nguarantees at least a $c/\\log k$ fraction of the optimal revenue. Additional\nresults compare the revenues from the two simple mechanisms of selling the\ngoods separately and bundled, identify situations where bundling is optimal,\nand extend the analysis to multiple buyers.\n", "category": [0, 7]}
{"abstract": "  Multi agent consensus algorithms with update steps based on so-called\nbalanced asymmetric chains, are analyzed. For such algorithms it is shown that\n(i) the set of accumulation points of states is finite, (ii) the asymptotic\nunconditional occurrence of single consensus or multiple consensuses is\ndirectly related to the property of absolute infinite flow for the underlying\nupdate chain. The results are applied to well known consensus models.\n", "category": [2, 0, 1, 2]}
{"abstract": "  In a multi-agent system, unconditional (multiple) consensus is the property\nof reaching to (multiple) consensus irrespective of the instant and values at\nwhich states are initialized. For linear algorithms, occurrence of\nunconditional (multiple) consensus turns out to be equivalent to (class-)\nergodicity of the transition chain (A_n). For a wide class of chains, chains\nwith so-called balanced asymmetry property, necessary and sufficient conditions\nfor ergodicity and class-ergodicity are derived. The results are employed to\nanalyze the limiting behavior of agents' states in the JLM model, the Krause\nmodel, and the Cucker-Smale model. In particular, unconditional single or\nmultiple consensus occurs in all three models. Moreover, a necessary and\nsufficient condition for unconditional consensus in the JLM model and a\nsufficient condition for consensus in the Cucker-Smale model are obtained.\n", "category": [2, 0, 1, 2]}
{"abstract": "  This paper considers fixed effects estimation and inference in linear and\nnonlinear panel data models with random coefficients and endogenous regressors.\nThe quantities of interest -- means, variances, and other moments of the random\ncoefficients -- are estimated by cross sectional sample moments of GMM\nestimators applied separately to the time series of each individual. To deal\nwith the incidental parameter problem introduced by the noise of the\nwithin-individual estimators in short panels, we develop bias corrections.\nThese corrections are based on higher-order asymptotic expansions of the GMM\nestimators and produce improved point and interval estimates in moderately long\npanels. Under asymptotic sequences where the cross sectional and time series\ndimensions of the panel pass to infinity at the same rate, the uncorrected\nestimator has an asymptotic bias of the same order as the asymptotic variance.\nThe bias corrections remove the bias without increasing variance. An empirical\nexample on cigarette demand based on Becker, Grossman and Murphy (1994) shows\nsignificant heterogeneity in the price effect across U.S. states.\n", "category": [6, 7, 2, 6, 6]}
{"abstract": "  Matching cells over time has long been the most difficult step in cell\ntracking. In this paper, we approach this problem by recasting it as a\nclassification problem. We construct a feature set for each cell, and compute a\nfeature difference vector between a cell in the current frame and a cell in a\nprevious frame. Then we determine whether the two cells represent the same cell\nover time by training decision trees as our binary classifiers. With the output\nof decision trees, we are able to formulate an assignment problem for our cell\nassociation task and solve it using a modified version of the Hungarian\nalgorithm.\n", "category": [0, 0, 1, 4, 6]}
{"abstract": "  Levy processes, which have stationary independent increments, are ideal for\nmodelling the various types of noise that can arise in communication channels.\nIf a Levy process admits exponential moments, then there exists a parametric\nfamily of measure changes called Esscher transformations. If the parameter is\nreplaced with an independent random variable, the true value of which\nrepresents a \"message\", then under the transformed measure the original Levy\nprocess takes on the character of an \"information process\". In this paper we\ndevelop a theory of such Levy information processes. The underlying Levy\nprocess, which we call the fiducial process, represents the \"noise type\". Each\nsuch noise type is capable of carrying a message of a certain specification. A\nnumber of examples are worked out in detail, including information processes of\nthe Brownian, Poisson, gamma, variance gamma, negative binomial, inverse\nGaussian, and normal inverse Gaussian type. Although in general there is no\nadditive decomposition of information into signal and noise, one is led\nnevertheless for each noise type to a well-defined scheme for signal detection\nand enhancement relevant to a variety of practical situations.\n", "category": [2, 0, 1, 2, 2, 5]}
{"abstract": "  We consider a large class of social learning models in which a group of\nagents face uncertainty regarding a state of the world, share the same utility\nfunction, observe private signals, and interact in a general dynamic setting.\nWe introduce Social Learning Equilibria, a static equilibrium concept that\nabstracts away from the details of the given extensive form, but nevertheless\ncaptures the corresponding asymptotic equilibrium behavior. We establish\ngeneral conditions for agreement, herding, and information aggregation in\nequilibrium, highlighting a connection between agreement and information\naggregation.\n", "category": [2, 7, 6]}
{"abstract": "  This paper proposes a voice morphing system for people suffering from\nLaryngectomy, which is the surgical removal of all or part of the larynx or the\nvoice box, particularly performed in cases of laryngeal cancer. A primitive\nmethod of achieving voice morphing is by extracting the source's vocal\ncoefficients and then converting them into the target speaker's vocal\nparameters. In this paper, we deploy Gaussian Mixture Models (GMM) for mapping\nthe coefficients from source to destination. However, the use of the\ntraditional/conventional GMM-based mapping approach results in the problem of\nover-smoothening of the converted voice. Thus, we hereby propose a unique\nmethod to perform efficient voice morphing and conversion based on GMM,which\novercomes the traditional-method effects of over-smoothening. It uses a\ntechnique of glottal waveform separation and prediction of excitations and\nhence the result shows that not only over-smoothening is eliminated but also\nthe transformed vocal tract parameters match with the target. Moreover, the\nsynthesized speech thus obtained is found to be of a sufficiently high quality.\nThus, voice morphing based on a unique GMM approach has been proposed and also\ncritically evaluated based on various subjective and objective evaluation\nparameters. Further, an application of voice morphing for Laryngectomees which\ndeploys this unique approach has been recommended by this paper.\n", "category": [0, 0, 1]}
{"abstract": "  We consider a group of strategic agents who must each repeatedly take one of\ntwo possible actions. They learn which of the two actions is preferable from\ninitial private signals, and by observing the actions of their neighbors in a\nsocial network.\n  We show that the question of whether or not the agents learn efficiently\ndepends on the topology of the social network. In particular, we identify a\ngeometric \"egalitarianism\" condition on the social network that guarantees\nlearning in infinite networks, or learning with high probability in large\nfinite networks, in any equilibrium. We also give examples of non-egalitarian\nnetworks with equilibria in which learning fails.\n", "category": [0, 7, 2]}
{"abstract": "  We propose dual regression as an alternative to the quantile regression\nprocess for the global estimation of conditional distribution functions under\nminimal assumptions. Dual regression provides all the interpretational power of\nthe quantile regression process while avoiding the need for repairing the\nintersecting conditional quantile surfaces that quantile regression often\nproduces in practice. Our approach introduces a mathematical programming\ncharacterization of conditional distribution functions which, in its simplest\nform, is the dual program of a simultaneous estimator for linear location-scale\nmodels. We apply our general characterization to the specification and\nestimation of a flexible class of conditional distribution functions, and\npresent asymptotic theory for the corresponding empirical dual regression\nprocess.\n", "category": [6, 7, 6]}
{"abstract": "  In applications it is common that the exact form of a conditional expectation\nis unknown and having flexible functional forms can lead to improvements.\nSeries method offers that by approximating the unknown function based on $k$\nbasis functions, where $k$ is allowed to grow with the sample size $n$. We\nconsider series estimators for the conditional mean in light of: (i) sharp LLNs\nfor matrices derived from the noncommutative Khinchin inequalities, (ii) bounds\non the Lebesgue factor that controls the ratio between the $L^\\infty$ and\n$L_2$-norms of approximation errors, (iii) maximal inequalities for processes\nwhose entropy integrals diverge, and (iv) strong approximations to series-type\nprocesses.\n  These technical tools allow us to contribute to the series literature,\nspecifically the seminal work of Newey (1997), as follows. First, we weaken the\ncondition on the number $k$ of approximating functions used in series\nestimation from the typical $k^2/n \\to 0$ to $k/n \\to 0$, up to log factors,\nwhich was available only for spline series before. Second, we derive $L_2$\nrates and pointwise central limit theorems results when the approximation error\nvanishes. Under an incorrectly specified model, i.e. when the approximation\nerror does not vanish, analogous results are also shown. Third, under stronger\nconditions we derive uniform rates and functional central limit theorems that\nhold if the approximation error vanishes or not. That is, we derive the strong\napproximation for the entire estimate of the nonparametric function.\n  We derive uniform rates, Gaussian approximations, and uniform confidence\nbands for a wide collection of linear functionals of the conditional\nexpectation function.\n", "category": [6, 7]}
{"abstract": "  We provide a comprehensive semi-parametric study of Bayesian partially\nidentified econometric models. While the existing literature on Bayesian\npartial identification has mostly focused on the structural parameter, our\nprimary focus is on Bayesian credible sets (BCS's) of the unknown identified\nset and the posterior distribution of its support function. We construct a\n(two-sided) BCS based on the support function of the identified set. We prove\nthe Bernstein-von Mises theorem for the posterior distribution of the support\nfunction. This powerful result in turn infers that, while the BCS and the\nfrequentist confidence set for the partially identified parameter are\nasymptotically different, our constructed BCS for the identified set has an\nasymptotically correct frequentist coverage probability. Importantly, we\nillustrate that the constructed BCS for the identified set does not require a\nprior on the structural parameter. It can be computed efficiently for subset\ninference, especially when the target of interest is a sub-vector of the\npartially identified parameter, where projecting to a low-dimensional subset is\noften required. Hence, the proposed methods are useful in many applications.\n  The Bayesian partial identification literature has been assuming a known\nparametric likelihood function. However, econometric models usually only\nidentify a set of moment inequalities, and therefore using an incorrect\nlikelihood function may result in misleading inferences. In contrast, with a\nnonparametric prior on the unknown likelihood function, our proposed Bayesian\nprocedure only requires a set of moment conditions, and can efficiently make\ninference about both the partially identified parameter and its identified set.\nThis makes it widely applicable in general moment inequality models. Finally,\nthe proposed method is illustrated in a financial asset pricing problem.\n", "category": [6, 7, 2, 6]}
{"abstract": "  We derive a Gaussian approximation result for the maximum of a sum of\nhigh-dimensional random vectors. Specifically, we establish conditions under\nwhich the distribution of the maximum is approximated by that of the maximum of\na sum of the Gaussian random vectors with the same covariance matrices as the\noriginal vectors. This result applies when the dimension of random vectors\n($p$) is large compared to the sample size ($n$); in fact, $p$ can be much\nlarger than $n$, without restricting correlations of the coordinates of these\nvectors. We also show that the distribution of the maximum of a sum of the\nrandom vectors with unknown covariance matrices can be consistently estimated\nby the distribution of the maximum of a sum of the conditional Gaussian random\nvectors obtained by multiplying the original vectors with i.i.d. Gaussian\nmultipliers. This is the Gaussian multiplier (or wild) bootstrap procedure.\nHere too, $p$ can be large or even much larger than $n$. These distributional\napproximations, either Gaussian or conditional Gaussian, yield a high-quality\napproximation to the distribution of the original maximum, often with\napproximation error decreasing polynomially in the sample size, and hence are\nof interest in many applications. We demonstrate how our Gaussian\napproximations and the multiplier bootstrap can be used for modern\nhigh-dimensional estimation, multiple hypothesis testing, and adaptive\nspecification testing. All these results contain nonasymptotic bounds on\napproximation errors.\n", "category": [2, 7, 2, 6]}
{"abstract": "  Recent studies have shown that deep neural networks (DNNs) perform\nsignificantly better than shallow networks and Gaussian mixture models (GMMs)\non large vocabulary speech recognition tasks. In this paper, we argue that the\nimproved accuracy achieved by the DNNs is the result of their ability to\nextract discriminative internal representations that are robust to the many\nsources of variability in speech signals. We show that these representations\nbecome increasingly insensitive to small perturbations in the input with\nincreasing network depth, which leads to better speech recognition performance\nwith deeper networks. We also show that DNNs cannot extrapolate to test samples\nthat are substantially different from the training examples. If the training\ndata are sufficiently representative, however, internal features learned by the\nDNN are relatively stable with respect to speaker differences, bandwidth\ndifferences, and environment distortion. This enables DNN-based recognizers to\nperform as well or better than state-of-the-art systems based on GMMs or\nshallow networks without the need for explicit model adaptation or feature\nnormalization.\n", "category": [0, 0, 0, 1]}
{"abstract": "  Convergence properties of time inhomogeneous Markov chain based discrete and\ncontinuous time linear consensus algorithms are analyzed. Provided that a\nso-called infinite jet flow property is satisfied by the underlying chains,\nnecessary conditions for both consensus and multiple consensus are established.\nA recenet extension by Sonin of the classical Kolmogorov-Doeblin\ndecomposition-separation for homogeneous Markov chains to the inhomogeneous\ncase is then employed to show that the obtained necessary conditions are also\nsufficient when the chain is of Class P*, as defined by Touri and Nedic. It is\nalso shown that Sonin's theorem leads to a rediscovery and generalization of\nmost of the existing related consensus results in the literature.\n", "category": [2, 0, 1, 2]}
{"abstract": "  In this article, we review quantile models with endogeneity. We focus on\nmodels that achieve identification through the use of instrumental variables\nand discuss conditions under which partial and point identification are\nobtained. We discuss key conditions, which include monotonicity and\nfull-rank-type conditions, in detail. In providing this review, we update the\nidentification results of Chernozhukov and Hansen (2005, Econometrica). We\nillustrate the modeling assumptions through economically motivated examples. We\nalso briefly review the literature on estimation and inference.\n  Key Words: identification, treatment effects, structural models, instrumental\nvariables\n", "category": [6, 7]}
{"abstract": "  We develop uniformly valid confidence regions for regression coefficients in\na high-dimensional sparse median regression model with homoscedastic errors.\nOur methods are based on a moment equation that is immunized against\nnon-regular estimation of the nuisance part of the median regression function\nby using Neyman's orthogonalization. We establish that the resulting\ninstrumental median regression estimator of a target regression coefficient is\nasymptotically normally distributed uniformly with respect to the underlying\nsparse model and is semi-parametrically efficient. We also generalize our\nmethod to a general non-smooth Z-estimation framework with the number of target\nparameters $p_1$ being possibly much larger than the sample size $n$. We extend\nHuber's results on asymptotic normality to this setting, demonstrating uniform\nasymptotic normality of the proposed estimators over $p_1$-dimensional\nrectangles, constructing simultaneous confidence bands on all of the $p_1$\ntarget parameters, and establishing asymptotic validity of the bands uniformly\nover underlying approximately sparse models.\n  Keywords: Instrument; Post-selection inference; Sparsity; Neyman's Orthogonal\nScore test; Uniformly valid inference; Z-estimation.\n", "category": [2, 7, 6, 6]}
{"abstract": "  This paper considers generalized linear models in the presence of many\ncontrols. We lay out a general methodology to estimate an effect of interest\nbased on the construction of an instrument that immunize against model\nselection mistakes and apply it to the case of logistic binary choice model.\nMore specifically we propose new methods for estimating and constructing\nconfidence regions for a regression parameter of primary interest $\\alpha_0$, a\nparameter in front of the regressor of interest, such as the treatment variable\nor a policy variable. These methods allow to estimate $\\alpha_0$ at the\nroot-$n$ rate when the total number $p$ of other regressors, called controls,\npotentially exceed the sample size $n$ using sparsity assumptions. The sparsity\nassumption means that there is a subset of $s<n$ controls which suffices to\naccurately approximate the nuisance part of the regression function.\nImportantly, the estimators and these resulting confidence regions are valid\nuniformly over $s$-sparse models satisfying $s^2\\log^2 p = o(n)$ and other\ntechnical conditions. These procedures do not rely on traditional consistent\nmodel selection arguments for their validity. In fact, they are robust with\nrespect to moderate model selection mistakes in variable selection. Under\nsuitable conditions, the estimators are semi-parametrically efficient in the\nsense of attaining the semi-parametric efficiency bounds for the class of\nmodels in this paper.\n", "category": [6, 7, 2, 6]}
{"abstract": "  We consider the well known, and notoriously difficult, problem of a single\nrevenue-maximizing seller selling two or more heterogeneous goods to a single\nbuyer whose private values for the goods are drawn from a (possibly correlated)\nknown distribution, and whose valuation is additive over the goods. We show\nthat when there are two (or more) goods, _simple mechanisms_ -- such as selling\nthe goods separately or as a bundle -- _may yield only a negligible fraction of\nthe optimal revenue_. This resolves the open problem of Briest, Chawla,\nKleinberg, and Weinberg (JET 2015) who prove the result for at least three\ngoods in the related setup of a unit-demand buyer. We also introduce the menu\nsize as a simple measure of the complexity of mechanisms, and show that the\nrevenue may increase polynomially with _menu size_ and that no bounded menu\nsize can ensure any positive fraction of the optimal revenue. The menu size\nalso turns out to \"pin down\" the revenue properties of deterministic\nmechanisms.\n", "category": [0, 7]}
{"abstract": "  We consider the complexity of finding a correlated equilibrium of an\n$n$-player game in a model that allows the algorithm to make queries on\nplayers' payoffs at pure strategy profiles. Randomized regret-based dynamics\nare known to yield an approximate correlated equilibrium efficiently, namely,\nin time that is polynomial in the number of players $n$. Here we show that both\nrandomization and approximation are necessary: no efficient deterministic\nalgorithm can reach even an approximate correlated equilibrium, and no\nefficient randomized algorithm can reach an exact correlated equilibrium. The\nresults are obtained by bounding from below the number of payoff queries that\nare needed.\n", "category": [0, 0, 7]}
{"abstract": "  In this supplementary appendix we provide additional results, omitted proofs\nand extensive simulations that complement the analysis of the main text\n(arXiv:1201.0224).\n", "category": [2, 7, 6]}
{"abstract": "  Polarimetric synthetic aperture radar (PolSAR) has achieved a prominent\nposition as a remote imaging method. However, PolSAR images are contaminated by\nspeckle noise due to the coherent illumination employed during the data\nacquisition. This noise provides a granular aspect to the image, making its\nprocessing and analysis (such as in edge detection) hard tasks. This paper\ndiscusses seven methods for edge detection in multilook PolSAR images. In all\nmethods, the basic idea consists in detecting transition points in the finest\npossible strip of data which spans two regions. The edge is contoured using the\ntransitions points and a B-spline curve. Four stochastic distances, two\ndifferences of entropies, and the maximum likelihood criterion were used under\nthe scaled complex Wishart distribution; the first six stem from the h-phi\nclass of measures. The performance of the discussed detection methods was\nquantified and analyzed by the computational time and probability of correct\nedge detection, with respect to the number of looks, the backscatter matrix as\na whole, the SPAN, the covariance an the spatial resolution. The detection\nprocedures were applied to three real PolSAR images. Results provide evidence\nthat the methods based on the Bhattacharyya distance and the difference of\nShannon entropies outperform the other techniques.\n", "category": [2, 0, 1, 6]}
{"abstract": "  A successful approach to image quality assessment involves comparing the\nstructural information between a distorted and its reference image. However,\nextracting structural information that is perceptually important to our visual\nsystem is a challenging task. This paper addresses this issue by employing a\nsparse representation-based approach and proposes a new metric called the\n\\emph{sparse representation-based quality} (SPARQ) \\emph{index}. The proposed\nmethod learns the inherent structures of the reference image as a set of basis\nvectors, such that any structure in the image can be represented by a linear\ncombination of only a few of those basis vectors. This sparse strategy is\nemployed because it is known to generate basis vectors that are qualitatively\nsimilar to the receptive field of the simple cells present in the mammalian\nprimary visual cortex. The visual quality of the distorted image is estimated\nby comparing the structures of the reference and the distorted images in terms\nof the learnt basis vectors resembling cortical cells. Our approach is\nevaluated on six publicly available subject-rated image quality assessment\ndatasets. The proposed SPARQ index consistently exhibits high correlation with\nthe subjective ratings on all datasets and performs better or at par with the\nstate-of-the-art.\n", "category": [0, 0, 1]}
{"abstract": "  We introduce a new solution concept, called periodicity, for selecting\noptimal strategies in strategic form games. This periodicity solution concept\nyields new insight into non-trivial games. In mixed strategy strategic form\ngames, periodic solutions yield values for the utility function of each player\nthat are equal to the Nash equilibrium ones. In contrast to the Nash\nstrategies, here the payoffs of each player are robust against what the\nopponent plays. Sometimes, periodicity strategies yield higher utilities, and\nsometimes the Nash strategies do, but often the utilities of these two\nstrategies coincide. We formally define and study periodic strategies in two\nplayer perfect information strategic form games with pure strategies and we\nprove that every non-trivial finite game has at least one periodic strategy,\nwith non-trivial meaning non-degenerate payoffs. In some classes of games where\nmixed strategies are used, we identify quantitative features. Particularly\ninteresting are the implications for collective action games, since there the\ncollective action strategy can be incorporated in a purely non-cooperative\ncontext. Moreover, we address the periodicity issue when the players have a\ncontinuum set of strategies available.\n", "category": [0, 7]}
{"abstract": "  Random stepped frequency (RSF) radar, which transmits random-frequency\npulses, can suppress the range ambiguity, improve convert detection, and\npossess excellent electronic counter-countermeasures (ECCM) ability [1]. In\nthis paper, we apply a sparse recovery method to estimate the range and Doppler\nof targets. We also propose a cognitive mechanism for RSF radar to further\nenhance the performance of the sparse recovery method. The carrier frequencies\nof transmitted pulses are adaptively designed in response to the observed\ncircumstance. We investigate the criterion to design carrier frequencies, and\nefficient methods are then devised. Simulation results demonstrate that the\nadaptive frequency-design mechanism significantly improves the performance of\ntarget reconstruction in comparison with the non-adaptive mechanism.\n", "category": [1]}
{"abstract": "  Compressive sensing (CS) can effectively recover a signal when it is sparse\nin some discrete atoms. However, in some applications, signals are sparse in a\ncontinuous parameter space, e.g., frequency space, rather than discrete atoms.\nUsually, we divide the continuous parameter into finite discrete grid points\nand build a dictionary from these grid points. However, the actual targets may\nnot exactly lie on the grid points no matter how densely the parameter is\ngrided, which introduces mismatch between the predefined dictionary and the\nactual one. In this article, a novel method, namely adaptive matching pursuit\nwith constrained total least squares (AMP-CTLS), is proposed to find actual\natoms even if they are not included in the initial dictionary. In AMP-CTLS, the\ngrid and the dictionary are adaptively updated to better agree with\nmeasurements. The convergence of the algorithm is discussed, and numerical\nexperiments demonstrate the advantages of AMP-CTLS.\n", "category": [1, 0, 2]}
{"abstract": "  This paper concerns robust inference on average treatment effects following\nmodel selection. In the selection on observables framework, we show how to\nconstruct confidence intervals based on a doubly-robust estimator that are\nrobust to model selection errors and prove that they are valid uniformly over a\nlarge class of treatment effect models. The class allows for multivalued\ntreatments with heterogeneous effects (in observables), general\nheteroskedasticity, and selection amongst (possibly) more covariates than\nobservations. Our estimator attains the semiparametric efficiency bound under\nappropriate conditions. Precise conditions are given for any model selector to\nyield these results, and we show how to combine data-driven selection with\neconomic theory. For implementation, we give a specific proposal for selection\nbased on the group lasso, which is particularly well-suited to treatment\neffects data, and derive new results for high-dimensional, sparse multinomial\nlogistic regression. A simulation study shows our estimator performs very well\nin finite samples over a wide range of models. Revisiting the National\nSupported Work demonstration data, our method yields accurate estimates and\ntight confidence intervals.\n", "category": [2, 7, 6, 6]}
{"abstract": "  Sensor selection is an important design problem in large-scale sensor\nnetworks. Sensor selection can be interpreted as the problem of selecting the\nbest subset of sensors that guarantees a certain estimation performance. We\nfocus on observations that are related to a general non-linear model. The\nproposed framework is valid as long as the observations are independent, and\nits likelihood satisfies the regularity conditions. We use several functions of\nthe Cram\\'er-Rao bound (CRB) as a performance measure. We formulate the sensor\nselection problem as the design of a selection vector, which in its original\nform is a nonconvex l0-(quasi) norm optimization problem. We present relaxed\nsensor selection solvers that can be efficiently solved in polynomial time. We\nalso propose a projected subgradient algorithm that is attractive for\nlarge-scale problems and also show how the algorithm can be easily distributed.\nThe proposed framework is illustrated with a number of examples related to\nsensor placement design for localization.\n", "category": [0, 1, 2]}
{"abstract": "  We study the problem of nonparametric regression when the regressor is\nendogenous, which is an important nonparametric instrumental variables (NPIV)\nregression in econometrics and a difficult ill-posed inverse problem with\nunknown operator in statistics. We first establish a general upper bound on the\nsup-norm (uniform) convergence rate of a sieve estimator, allowing for\nendogenous regressors and weakly dependent data. This result leads to the\noptimal sup-norm convergence rates for spline and wavelet least squares\nregression estimators under weakly dependent data and heavy-tailed error terms.\nThis upper bound also yields the sup-norm convergence rates for sieve NPIV\nestimators under i.i.d. data: the rates coincide with the known optimal\n$L^2$-norm rates for severely ill-posed problems, and are power of $\\log(n)$\nslower than the optimal $L^2$-norm rates for mildly ill-posed problems. We then\nestablish the minimax risk lower bound in sup-norm loss, which coincides with\nour upper bounds on sup-norm rates for the spline and wavelet sieve NPIV\nestimators. This sup-norm rate optimality provides another justification for\nthe wide application of sieve NPIV estimators. Useful results on\nweakly-dependent random matrices are also provided.\n", "category": [2, 7, 6, 6]}
{"abstract": "  In this paper, we provide efficient estimators and honest confidence bands\nfor a variety of treatment effects including local average (LATE) and local\nquantile treatment effects (LQTE) in data-rich environments. We can handle very\nmany control variables, endogenous receipt of treatment, heterogeneous\ntreatment effects, and function-valued outcomes. Our framework covers the\nspecial case of exogenous receipt of treatment, either conditional on controls\nor unconditionally as in randomized control trials. In the latter case, our\napproach produces efficient estimators and honest bands for (functional)\naverage treatment effects (ATE) and quantile treatment effects (QTE). To make\ninformative inference possible, we assume that key reduced form predictive\nrelationships are approximately sparse. This assumption allows the use of\nregularization and selection methods to estimate those relations, and we\nprovide methods for post-regularization and post-selection inference that are\nuniformly valid (honest) across a wide-range of models. We show that a key\ningredient enabling honest inference is the use of orthogonal or doubly robust\nmoment conditions in estimating certain reduced form functional parameters. We\nillustrate the use of the proposed methods with an application to estimating\nthe effect of 401(k) eligibility and participation on accumulated assets.\n", "category": [2, 7, 6, 6, 6]}
{"abstract": "  We derive fixed effects estimators of parameters and average partial effects\nin (possibly dynamic) nonlinear panel data models with individual and time\neffects. They cover logit, probit, ordered probit, Poisson and Tobit models\nthat are important for many empirical applications in micro and macroeconomics.\nOur estimators use analytical and jackknife bias corrections to deal with the\nincidental parameter problem, and are asymptotically unbiased under asymptotic\nsequences where $N/T$ converges to a constant. We develop inference methods and\nshow that they perform well in numerical examples.\n", "category": [6, 7]}
{"abstract": "  This paper considers identification and estimation of ceteris paribus effects\nof continuous regressors in nonseparable panel models with time homogeneity.\nThe effects of interest are derivatives of the average and quantile structural\nfunctions of the model. We find that these derivatives are identified with two\ntime periods for \"stayers\", i.e. for individuals with the same regressor values\nin two time periods. We show that the identification results carry over to\nmodels that allow location and scale time effects. We propose nonparametric\nseries methods and a weighted bootstrap scheme to estimate and make inference\non the identified effects. The bootstrap proposed allows uniform inference for\nfunction-valued parameters such as quantile effects uniformly over a region of\nquantile indices and/or regressor values. An empirical application to Engel\ncurve estimation with panel data illustrates the results.\n", "category": [6, 7]}
{"abstract": "  This work proposes new inference methods for a regression coefficient of\ninterest in a (heterogeneous) quantile regression model. We consider a\nhigh-dimensional model where the number of regressors potentially exceeds the\nsample size but a subset of them suffice to construct a reasonable\napproximation to the conditional quantile function. The proposed methods are\n(explicitly or implicitly) based on orthogonal score functions that protect\nagainst moderate model selection mistakes, which are often inevitable in the\napproximately sparse model considered in the present paper. We establish the\nuniform validity of the proposed confidence regions for the quantile regression\ncoefficient. Importantly, these methods directly apply to more than one\nvariable and a continuum of quantile indices. In addition, the performance of\nthe proposed methods is illustrated through Monte-Carlo experiments and an\nempirical example, dealing with risk factors in childhood malnutrition.\n", "category": [2, 7, 6]}
{"abstract": "  Biondi et al. (2012) develop an analytical model to examine the emergent\ndynamic properties of share market price formation over time, capable to\ncapture important stylized facts. These latter properties prove to be sensitive\nto regulatory regimes for fundamental information provision, as well as to\nmarket confidence conditions among actual and potential investors. Regimes\nbased upon mark-to-market (fair value) measurement of traded security, while\ngenerating higher linear correlation between market prices and fundamental\nsignals, also involve higher market instability and volatility. These regimes\nalso incur more relevant episodes of market exuberance and vagary in some\nregions of the market confidence space, where lower market liquidity further\noccurs.\n", "category": [5, 7, 3, 5, 5, 5]}
{"abstract": "  This paper considers the problem of testing many moment inequalities where\nthe number of moment inequalities, denoted by $p$, is possibly much larger than\nthe sample size $n$. There is a variety of economic applications where solving\nthis problem allows to carry out inference on causal and structural parameters,\na notable example is the market structure model of Ciliberto and Tamer (2009)\nwhere $p=2^{m+1}$ with $m$ being the number of firms that could possibly enter\nthe market. We consider the test statistic given by the maximum of $p$\nStudentized (or $t$-type) inequality-specific statistics, and analyze various\nways to compute critical values for the test statistic. Specifically, we\nconsider critical values based upon (i) the union bound combined with a\nmoderate deviation inequality for self-normalized sums, (ii) the multiplier and\nempirical bootstraps, and (iii) two-step and three-step variants of (i) and\n(ii) by incorporating the selection of uninformative inequalities that are far\nfrom being binding and a novel selection of weakly informative inequalities\nthat are potentially binding but do not provide first order information. We\nprove validity of these methods, showing that under mild conditions, they lead\nto tests with the error in size decreasing polynomially in $n$ while allowing\nfor $p$ being much larger than $n$, indeed $p$ can be of order $\\exp (n^{c})$\nfor some $c > 0$. Importantly, all these results hold without any restriction\non the correlation structure between $p$ Studentized statistics, and also hold\nuniformly with respect to suitably large classes of underlying distributions.\nMoreover, in the online supplement, we show validity of a test based on the\nblock multiplier bootstrap in the case of dependent data under some general\nmixing conditions.\n", "category": [2, 7, 6, 6]}
{"abstract": "  The increasing importance of renewable energy, especially solar and wind\npower, has led to new forces in the formation of electricity prices. Hence,\nthis paper introduces an econometric model for the hourly time series of\nelectricity prices of the European Power Exchange (EPEX) which incorporates\nspecific features like renewable energy. The model consists of several\nsophisticated and established approaches and can be regarded as a periodic\nVAR-TARCH with wind power, solar power, and load as influences on the time\nseries. It is able to map the distinct and well-known features of electricity\nprices in Germany. An efficient iteratively reweighted lasso approach is used\nfor the estimation. Moreover, it is shown that several existing models are\noutperformed by the procedure developed in this paper.\n", "category": [6, 7, 5, 5, 6]}
{"abstract": "  We study the class of potential games that are also graphical games with\nrespect to a given graph $G$ of connections between the players. We show that,\nup to strategic equivalence, this class of games can be identified with the set\nof Markov random fields on $G$.\n  From this characterization, and from the Hammersley-Clifford theorem, it\nfollows that the potentials of such games can be decomposed to local\npotentials. We use this decomposition to strongly bound the number of strategy\nchanges of a single player along a better response path. This result extends to\ngeneralized graphical potential games, which are played on infinite graphs.\n", "category": [2, 0, 7]}
{"abstract": "  We consider a network of evolving opinions. It includes multiple individuals\nwith first-order opinion dynamics defined in continuous time and evolving based\non a general exogenously defined time-varying underlying graph. In such a\nnetwork, for an arbitrary fixed initial time, a subset of individuals forms an\neminence grise coalition, abbreviated as EGC, if the individuals in that subset\nare capable of leading the entire network to agreeing on any desired opinion,\nthrough a cooperative choice of their own initial opinions. In this endeavor,\nthe coalition members are assumed to have access to full profile of the\nunderlying graph of the network as well as the initial opinions of all other\nindividuals. While the complete coalition of individuals always qualifies as an\nEGC, we establish the existence of a minimum size EGC for an arbitrary\ntime-varying network; also, we develop a non-trivial set of upper and lower\nbounds on that size. As a result, we show that, even when the underlying graph\ndoes not guarantee convergence to a global or multiple consensus, a generally\nrestricted coalition of agents can steer public opinion towards a desired\nglobal consensus without affecting any of the predefined graph interactions,\nprovided they can cooperatively adjust their own initial opinions. Geometric\ninsights into the structure of EGC's are given. The results are also extended\nto the discrete time case where the relation with Decomposition-Separation\nTheorem is also made explicit.\n", "category": [2, 0, 1, 2]}
{"abstract": "  The problem on the Cramer-Rao Lower Bounds (CRLBs) for the joint time delay\nand Doppler stretch estimation of an extended target is considered in this\npaper. The integral representations of the CRLBs for both the time delay and\nthe Doppler stretch are derived. To facilitate computation and analysis, series\nrepresentations and approximations of the CRLBs are introduced. According to\nthese series representations, the impact of several waveform parameters on the\nestimation accuracy is investigated, which reveals that the CRLB of the Doppler\nstretch is inversely proportional to the effective time-bandwidth product of\nthe waveform. This conclusion generalizes a previous result in the narrowband\ncase. The popular wideband ambiguity function (WBAF) based delay-Doppler\nestimator is evaluated and compared with the CRLBs through numerical\nexperiments. Our results indicate that the WBAF estimator, originally derived\nfrom a single scatterer model, is not suitable for the parameter estimation of\nan extended target.\n", "category": [1]}
{"abstract": "  During the Great Recession, Democrats in the United States argued that\ngovernment spending could be utilized to \"grease the wheels\" of the economy in\norder to create wealth and to increase employment; Republicans, on the other\nhand, contended that government spending is wasteful and discouraged\ninvestment, thereby increasing unemployment. Today, in 2020, we find ourselves\nin the midst of another crisis where government spending and fiscal stimulus is\nagain being considered as a solution. In the present paper, we address this\nquestion by formulating an optimal control problem generalizing the model of\nRadner & Shepp (1996). The model allows for the company to borrow continuously\nfrom the government. We prove that there exists an optimal strategy; rigorous\nverification proofs for its optimality are provided. We proceed to prove that\ngovernment loans increase the expected net value of a company. We also examine\nthe consequences of different profit-taking behaviors among firms who receive\nfiscal stimulus.\n", "category": [7, 5, 5]}
{"abstract": "  This paper considers inference on functionals of semi/nonparametric\nconditional moment restrictions with possibly nonsmooth generalized residuals,\nwhich include all of the (nonlinear) nonparametric instrumental variables (IV)\nas special cases. These models are often ill-posed and hence it is difficult to\nverify whether a (possibly nonlinear) functional is root-$n$ estimable or not.\nWe provide computationally simple, unified inference procedures that are\nasymptotically valid regardless of whether a functional is root-$n$ estimable\nor not. We establish the following new useful results: (1) the asymptotic\nnormality of a plug-in penalized sieve minimum distance (PSMD) estimator of a\n(possibly nonlinear) functional; (2) the consistency of simple sieve variance\nestimators for the plug-in PSMD estimator, and hence the asymptotic chi-square\ndistribution of the sieve Wald statistic; (3) the asymptotic chi-square\ndistribution of an optimally weighted sieve quasi likelihood ratio (QLR) test\nunder the null hypothesis; (4) the asymptotic tight distribution of a\nnon-optimally weighted sieve QLR statistic under the null; (5) the consistency\nof generalized residual bootstrap sieve Wald and QLR tests; (6) local power\nproperties of sieve Wald and QLR tests and of their bootstrap versions; (7)\nasymptotic properties of sieve Wald and SQLR for functionals of increasing\ndimension. Simulation studies and an empirical illustration of a nonparametric\nquantile IV regression are presented.\n", "category": [2, 7, 6]}
{"abstract": "  This paper establishes consistency of the weighted bootstrap for quadratic\nforms $\\left( n^{-1/2} \\sum_{i=1}^{n} Z_{i,n} \\right)^{T}\\left( n^{-1/2}\n\\sum_{i=1}^{n} Z_{i,n} \\right)$ where $(Z_{i,n})_{i=1}^{n}$ are mean zero,\nindependent $\\mathbb{R}^{d}$-valued random variables and $d=d(n)$ is allowed to\ngrow with the sample size $n$, slower than $n^{1/4}$. The proof relies on an\nadaptation of Lindeberg interpolation technique whereby we simplify the\noriginal problem to a Gaussian approximation problem. We apply our bootstrap\nresults to model-specification testing problems when the number of moments is\nallowed to grow with the sample size.\n", "category": [2, 7, 2, 6]}
{"abstract": "  We consider estimation and inference in panel data models with additive\nunobserved individual specific heterogeneity in a high dimensional setting. The\nsetting allows the number of time varying regressors to be larger than the\nsample size. To make informative estimation and inference feasible, we require\nthat the overall contribution of the time varying variables after eliminating\nthe individual specific heterogeneity can be captured by a relatively small\nnumber of the available variables whose identities are unknown. This\nrestriction allows the problem of estimation to proceed as a variable selection\nproblem. Importantly, we treat the individual specific heterogeneity as fixed\neffects which allows this heterogeneity to be related to the observed time\nvarying variables in an unspecified way and allows that this heterogeneity may\nbe non-zero for all individuals. Within this framework, we provide procedures\nthat give uniformly valid inference over a fixed subset of parameters in the\ncanonical linear fixed effects model and over coefficients on a fixed vector of\nendogenous variables in panel data instrumental variables models with fixed\neffects and many instruments. An input to developing the properties of our\nproposed procedures is the use of a variant of the Lasso estimator that allows\nfor a grouped data structure where data across groups are independent and\ndependence within groups is unrestricted. We provide formal conditions within\nthis structure under which the proposed Lasso variant selects a sparse model\nwith good approximation properties. We present simulation results in support of\nthe theoretical developments and illustrate the use of the methods in an\napplication aimed at estimating the effect of gun prevalence on crime rates.\n", "category": [6, 7]}
{"abstract": "  Using Gretl, I apply ARMA, Vector ARMA, VAR, state-space model with a Kalman\nfilter, transfer-function and intervention models, unit root tests,\ncointegration test, volatility models (ARCH, GARCH, ARCH-M, GARCH-M,\nTaylor-Schwert GARCH, GJR, TARCH, NARCH, APARCH, EGARCH) to analyze quarterly\ntime series of GDP and Government Consumption Expenditures & Gross Investment\n(GCEGI) from 1980 to 2013. The article is organized as: (I) Definition; (II)\nRegression Models; (III) Discussion. Additionally, I discovered a unique\ninteraction between GDP and GCEGI in both the short-run and the long-run and\nprovided policy makers with some suggestions. For example in the short run, GDP\nresponded positively and very significantly (0.00248) to GCEGI, while GCEGI\nreacted positively but not too significantly (0.08051) to GDP. In the long run,\ncurrent GDP responded negatively and permanently (0.09229) to a shock in past\nGCEGI, while current GCEGI reacted negatively yet temporarily (0.29821) to a\nshock in past GDP. Therefore, policy makers should not adjust current GCEGI\nbased merely on the condition of current and past GDP. Although increasing\nGCEGI does help GDP in the short-term, significantly abrupt increase in GCEGI\nmight not be good to the long-term health of GDP. Instead, a balanced,\nsustainable, and economically viable solution is recommended, so that the\nshort-term benefits to the current economy from increasing GCEGI often largely\nsecured by the long-term loan outweigh or at least equal to the negative effect\nto the future economy from the long-term debt incurred by the loan. Finally, I\nfound that non-normally distributed volatility models generally perform better\nthan normally distributed ones. More specifically, TARCH-GED performs the best\nin the group of non-normally distributed, while GARCH-M does the best in the\ngroup of normally distributed.\n", "category": [7, 5, 6]}
{"abstract": "  Factor structures or interactive effects are convenient devices to\nincorporate latent variables in panel data models. We consider fixed effect\nestimation of nonlinear panel single-index models with factor structures in the\nunobservables, which include logit, probit, ordered probit and Poisson\nspecifications. We establish that fixed effect estimators of model parameters\nand average partial effects have normal distributions when the two dimensions\nof the panel grow large, but might suffer of incidental parameter bias. We show\nhow models with factor structures can also be applied to capture important\nfeatures of network data such as reciprocity, degree heterogeneity, homophily\nin latent variables and clustering. We illustrate this applicability with an\nempirical example to the estimation of a gravity equation of international\ntrade between countries using a Poisson model with multiple factors.\n", "category": [6, 7]}
{"abstract": "  We study how long-lived rational agents learn from repeatedly observing a\nprivate signal and each others' actions. With normal signals, a group of any\nsize learns more slowly than just four agents who directly observe each others'\nprivate signals in each period. Similar results apply to general signal\nstructures. We identify rational groupthink---in which agents ignore their\nprivate signals and choose the same action for long periods of time---as the\ncause of this failure of information aggregation.\n", "category": [0, 7, 2]}
{"abstract": "  Photo retouching enables photographers to invoke dramatic visual impressions\nby artistically enhancing their photos through stylistic color and tone\nadjustments. However, it is also a time-consuming and challenging task that\nrequires advanced skills beyond the abilities of casual photographers. Using an\nautomated algorithm is an appealing alternative to manual work but such an\nalgorithm faces many hurdles. Many photographic styles rely on subtle\nadjustments that depend on the image content and even its semantics. Further,\nthese adjustments are often spatially varying. Because of these\ncharacteristics, existing automatic algorithms are still limited and cover only\na subset of these challenges. Recently, deep machine learning has shown unique\nabilities to address hard problems that resisted machine algorithms for long.\nThis motivated us to explore the use of deep learning in the context of photo\nediting. In this paper, we explain how to formulate the automatic photo\nadjustment problem in a way suitable for this approach. We also introduce an\nimage descriptor that accounts for the local semantics of an image. Our\nexperiments demonstrate that our deep learning formulation applied using these\ndescriptors successfully capture sophisticated photographic styles. In\nparticular and unlike previous techniques, it can model local adjustments that\ndepend on the image semantics. We show on several examples that this yields\nresults that are qualitatively and quantitatively better than previous work.\n", "category": [0, 0, 0, 1]}
{"abstract": "  We propose new concepts of statistical depth, multivariate quantiles, ranks\nand signs, based on canonical transportation maps between a distribution of\ninterest on $R^d$ and a reference distribution on the $d$-dimensional unit\nball. The new depth concept, called Monge-Kantorovich depth, specializes to\nhalfspace depth in the case of spherical distributions, but, for more general\ndistributions, differs from the latter in the ability for its contours to\naccount for non convex features of the distribution of interest. We propose\nempirical counterparts to the population versions of those Monge-Kantorovich\ndepth contours, quantiles, ranks and signs, and show their consistency by\nestablishing a uniform convergence property for empirical transport maps, which\nis of independent interest.\n", "category": [2, 7, 6]}
{"abstract": "  In our paper we analyze the relationship between the day-ahead electricity\nprice of the Energy Exchange Austria (EXAA) and other day-ahead electricity\nprices in Europe. We focus on markets, which settle their prices after the\nEXAA, which enables traders to include the EXAA price into their calculations.\nFor each market we employ econometric models to incorporate the EXAA price and\ncompare them with their counterparts without the price of the Austrian\nexchange. By employing a forecasting study, we find that electricity price\nmodels can be improved when EXAA prices are considered.\n", "category": [5, 7, 5, 6, 6]}
{"abstract": "  The frequentist method of simulated minimum distance (SMD) is widely used in\neconomics to estimate complex models with an intractable likelihood. In other\ndisciplines, a Bayesian approach known as Approximate Bayesian Computation\n(ABC) is far more popular. This paper connects these two seemingly related\napproaches to likelihood-free estimation by means of a Reverse Sampler that\nuses both optimization and importance weighting to target the posterior\ndistribution. Its hybrid features enable us to analyze an ABC estimate from the\nperspective of SMD. We show that an ideal ABC estimate can be obtained as a\nweighted average of a sequence of SMD modes, each being the minimizer of the\ndeviations between the data and the model. This contrasts with the SMD, which\nis the mode of the average deviations. Using stochastic expansions, we provide\na general characterization of frequentist estimators and those based on\nBayesian computations including Laplace-type estimators. Their differences are\nillustrated using analytical examples and a simulation study of the dynamic\npanel model.\n", "category": [6, 7]}
{"abstract": "  In this note, we offer an approach to estimating causal/structural parameters\nin the presence of many instruments and controls based on methods for\nestimating sparse high-dimensional models. We use these high-dimensional\nmethods to select both which instruments and which control variables to use.\nThe approach we take extends BCCH2012, which covers selection of instruments\nfor IV models with a small number of controls, and extends BCH2014, which\ncovers selection of controls in models where the variable of interest is\nexogenous conditional on observables, to accommodate both a large number of\ncontrols and a large number of instruments. We illustrate the approach with a\nsimulation and an empirical example. Technical supporting material is available\nin a supplementary online appendix.\n", "category": [6, 7]}
{"abstract": "  Here we present an expository, general analysis of valid post-selection or\npost-regularization inference about a low-dimensional target parameter,\n$\\alpha$, in the presence of a very high-dimensional nuisance parameter,\n$\\eta$, which is estimated using modern selection or regularization methods.\nOur analysis relies on high-level, easy-to-interpret conditions that allow one\nto clearly see the structures needed for achieving valid post-regularization\ninference. Simple, readily verifiable sufficient conditions are provided for a\nclass of affine-quadratic models. We focus our discussion on estimation and\ninference procedures based on using the empirical analog of theoretical\nequations $$M(\\alpha, \\eta)=0$$ which identify $\\alpha$. Within this structure,\nwe show that setting up such equations in a manner such that the\northogonality/immunization condition $$\\partial_\\eta M(\\alpha, \\eta) = 0$$ at\nthe true parameter values is satisfied, coupled with plausible conditions on\nthe smoothness of $M$ and the quality of the estimator $\\hat \\eta$, guarantees\nthat inference on for the main parameter $\\alpha$ based on testing or point\nestimation methods discussed below will be regular despite selection or\nregularization biases occurring in estimation of $\\eta$. In particular, the\nestimator of $\\alpha$ will often be uniformly consistent at the root-$n$ rate\nand uniformly asymptotically normal even though estimators $\\hat \\eta$ will\ngenerally not be asymptotically linear and regular. The uniformity holds over\nlarge classes of models that do not impose highly implausible \"beta-min\"\nconditions. We also show that inference can be carried out by inverting tests\nformed from Neyman's $C(\\alpha)$ (orthogonal score) statistics.\n", "category": [2, 7, 6]}
{"abstract": "  A new algorithm for voice automatic syllabic splitting in the Portuguese\nlanguage is proposed, which is based on the envelope of the speech signal of\nthe input audio file. A computational implementation in MatlabTM is presented\nand made available at the URL\nhttp://www2.ee.ufpe.br/codec/divisao_silabica.html. Due to its\nstraightforwardness, the proposed method is very attractive for embedded\nsystems (e.g. i-phones). It can also be used as a screen to assist more\nsophisticated methods. Voice excerpts containing more than one syllable and\nidentified by the same envelope are named as super-syllables and they are\nsubsequently separated. The results indicate which samples corresponds to the\nbeginning and end of each detected syllable. Preliminary tests were performed\nto fifty words at an identification rate circa 70% (further improvements may be\nincorporated to treat particular phonemes). This algorithm is also useful in\nvoice command systems, as a tool in the teaching of Portuguese language or even\nfor patients with speech pathology.\n", "category": [0, 0, 0, 1]}
{"abstract": "  General guidelines for a new fast computation of blocklength 8m+4 DFTs are\npresented, which is based on a Laurent series involving matrices. Results of\nnon-trivial real multiplicative complexity are presented for blocklengths N=64,\nachieving lower multiplication counts than previously published FFTs. A\ndetailed description for the cases m=1 and m=2 is presented.\n", "category": [0, 0, 1]}
{"abstract": "  Common high-dimensional methods for prediction rely on having either a sparse\nsignal model, a model in which most parameters are zero and there are a small\nnumber of non-zero parameters that are large in magnitude, or a dense signal\nmodel, a model with no large parameters and very many small non-zero\nparameters. We consider a generalization of these two basic models, termed here\na \"sparse+dense\" model, in which the signal is given by the sum of a sparse\nsignal and a dense signal. Such a structure poses problems for traditional\nsparse estimators, such as the lasso, and for traditional dense estimation\nmethods, such as ridge estimation. We propose a new penalization-based method,\ncalled lava, which is computationally efficient. With suitable choices of\npenalty parameters, the proposed method strictly dominates both lasso and\nridge. We derive analytic expressions for the finite-sample risk function of\nthe lava estimator in the Gaussian sequence model. We also provide an deviation\nbound for the prediction risk in the Gaussian regression model with fixed\ndesign. In both cases, we provide Stein's unbiased estimator for lava's\nprediction risk. A simulation example compares the performance of lava to\nlasso, ridge, and elastic net in a regression example using feasible,\ndata-dependent penalty parameters and illustrates lava's improved performance\nrelative to these benchmarks.\n", "category": [6, 0, 7, 2]}
{"abstract": "  Finite field transforms have many applications and, in many cases, can be\nimplemented with a low computational complexity. In this paper, the Z Transform\nover a finite field is introduced and some of its properties are presented.\n", "category": [2, 0, 1]}
{"abstract": "  This paper presents a new approach for a vocoder design based on full\nfrequency masking by octaves in addition to a technique for spectral filling\nvia beta probability distribution. Some psycho-acoustic characteristics of\nhuman hearing - inaudibility masking in frequency and phase - are used as a\nbasis for the proposed algorithm. The results confirm that this technique may\nbe useful to save bandwidth in applications requiring intelligibility. It is\nrecommended for the legal eavesdropping of long voice conversations.\n", "category": [0, 1]}
{"abstract": "  This paper presents a novel method for controlling teams of unmanned aerial\nvehicles using Stochastic Optimal Control (SOC) theory. The approach consists\nof a centralized high-level planner that computes optimal state trajectories as\nvelocity sequences, and a platform-specific low-level controller which ensures\nthat these velocity sequences are met. The planning task is expressed as a\ncentralized path-integral control problem, for which optimal control\ncomputation corresponds to a probabilistic inference problem that can be solved\nby efficient sampling methods. Through simulation we show that our SOC approach\n(a) has significant benefits compared to deterministic control and other SOC\nmethods in multimodal problems with noise-dependent optimal solutions, (b) is\ncapable of controlling a large number of platforms in real-time, and (c) yields\ncollective emergent behaviour in the form of flight formations. Finally, we\nshow that our approach works for real platforms, by controlling a team of three\nquadrotors in outdoor conditions.\n", "category": [1, 0, 0, 0]}
{"abstract": "  This paper describes a flexible architecture for implementing a new fast\ncomputation of the discrete Fourier and Hartley transforms, which is based on a\nmatrix Laurent series. The device calculates the transforms based on a single\nbit selection operator. The hardware structure and synthesis are presented,\nwhich handled a 16-point fast transform in 65 nsec, with a Xilinx SPARTAN 3E\ndevice.\n", "category": [0, 0, 1]}
{"abstract": "  We study Markov decision problems where the agent does not know the\ntransition probability function mapping current states and actions to future\nstates. The agent has a prior belief over a set of possible transition\nfunctions and updates beliefs using Bayes' rule. We allow her to be\nmisspecified in the sense that the true transition probability function is not\nin the support of her prior. This problem is relevant in many economic settings\nbut is usually not amenable to analysis by the researcher. We make the problem\ntractable by studying asymptotic behavior. We propose an equilibrium notion and\nprovide conditions under which it characterizes steady state behavior. In the\nspecial case where the problem is static, equilibrium coincides with the\nsingle-agent version of Berk-Nash equilibrium (Esponda and Pouzo (2016)). We\nalso discuss subtle issues that arise exclusively in dynamic settings due to\nthe possibility of a negative value of experimentation.\n", "category": [5, 7]}
{"abstract": "  This paper considers the cluster synchronization problem of generic linear\ndynamical systems whose system models are distinct in different clusters. These\nnonidentical linear models render control design and coupling conditions highly\ncorrelated if static couplings are used for all individual systems. In this\npaper, a dynamic coupling structure, which incorporates a global weighting\nfactor and a vanishing auxiliary control variable, is proposed for each agent\nand is shown to be a feasible solution. Lower bounds on the global and local\nweighting factors are derived under the condition that every interaction\nsubgraph associated with each cluster admits a directed spanning tree. The\nspanning tree requirement is further shown to be a necessary condition when the\nclusters connect acyclically with each other. Simulations for two applications,\ncluster heading alignment of nonidentical ships and cluster phase\nsynchronization of nonidentical harmonic oscillators, illustrate essential\nparts of the derived theoretical results.\n", "category": [1, 0]}
{"abstract": "  This paper introduces the theory and hardware implementation of two new\nalgorithms for computing a single component of the discrete Fourier transform.\nIn terms of multiplicative complexity, both algorithms are more efficient, in\ngeneral, than the well known Goertzel Algorithm.\n", "category": [0, 0, 1, 6]}
{"abstract": "  This paper examines finite field trigonometry as a tool to construct\ntrigonometric digital transforms. In particular, by using properties of the\nk-cosine function over GF(p), the Finite Field Discrete Cosine Transform\n(FFDCT) is introduced. The FFDCT pair in GF(p) is defined, having blocklengths\nthat are divisors of (p+1)/2. A special case is the Mersenne FFDCT, defined\nwhen p is a Mersenne prime. In this instance blocklengths that are powers of\ntwo are possible and radix-2 fast algorithms can be used to compute the\ntransform.\n", "category": [0, 0, 1]}
{"abstract": "  We propose an automatic video inpainting algorithm which relies on the\noptimisation of a global, patch-based functional. Our algorithm is able to deal\nwith a variety of challenging situations which naturally arise in video\ninpainting, such as the correct reconstruction of dynamic textures, multiple\nmoving objects and moving background. Furthermore, we achieve this in an order\nof magnitude less execution time with respect to the state-of-the-art. We are\nalso able to achieve good quality results on high definition videos. Finally,\nwe provide specific algorithmic details to make implementation of our algorithm\nas easy as possible. The resulting algorithm requires no segmentation or manual\ninput other than the definition of the inpainting mask, and can deal with a\nwider variety of situations than is handled by previous work. 1. Introduction.\nAdvanced image and video editing techniques are increasingly common in the\nimage processing and computer vision world, and are also starting to be used in\nmedia entertainment. One common and difficult task closely linked to the world\nof video editing is image and video \" inpainting \". Generally speaking, this is\nthe task of replacing the content of an image or video with some other content\nwhich is visually pleasing. This subject has been extensively studied in the\ncase of images, to such an extent that commercial image inpainting products\ndestined for the general public are available, such as Photoshop's \" Content\nAware fill \" [1]. However, while some impressive results have been obtained in\nthe case of videos, the subject has been studied far less extensively than\nimage inpainting. This relative lack of research can largely be attributed to\nhigh time complexity due to the added temporal dimension. Indeed, it has only\nvery recently become possible to produce good quality inpainting results on\nhigh definition videos, and this only in a semi-automatic manner. Nevertheless,\nhigh-quality video inpainting has many important and useful applications such\nas film restoration, professional post-production in cinema and video editing\nfor personal use. For this reason, we believe that an automatic, generic video\ninpainting algorithm would be extremely useful for both academic and\nprofessional communities.\n", "category": [0, 0, 1, 2]}
{"abstract": "  In this paper, we address the global stabilization of chains of integrators\nby means of a bounded static feedback law whose p first time derivatives are\nbounded. Our construction is based on the technique of nested saturations\nintroduced by Teel. We show that the control amplitude and the maximum value of\nits p first derivatives can be imposed below any prescribed values. Our results\nare illustrated by the stabilization of the third order integrator on the\nfeedback and its first two derivatives.\n", "category": [1, 0]}
{"abstract": "  We propose a reconfigurable bit comparator implemented with a nanowire spin\nvalve whose two contacts are magnetostrictive with bistable magnetization.\nReference and input bits are \"written\" into the magnetization states of the two\ncontacts with electrically generated strain and the spin-valve's resistance is\nlowered if they match. Multiple comparators can be interfaced in parallel with\na magneto-tunneling junction to determine if an N-bit input stream matches an\nN-bit reference stream bit by bit. The system is robust against thermal noise\nat room temperature and a 16-bit comparator can operate at roughly 416 MHz\nwhile dissipating at most 420 aJ per cycle.\n", "category": [3, 0, 1]}
{"abstract": "  In this paper we study the problems of estimating heterogeneity in causal\neffects in experimental or observational studies and conducting inference about\nthe magnitude of the differences in treatment effects across subsets of the\npopulation. In applications, our method provides a data-driven approach to\ndetermine which subpopulations have large or small treatment effects and to\ntest hypotheses about the differences in these effects. For experiments, our\nmethod allows researchers to identify heterogeneity in treatment effects that\nwas not specified in a pre-analysis plan, without concern about invalidating\ninference due to multiple testing. In most of the literature on supervised\nmachine learning (e.g. regression trees, random forests, LASSO, etc.), the goal\nis to build a model of the relationship between a unit's attributes and an\nobserved outcome. A prominent role in these methods is played by\ncross-validation which compares predictions to actual outcomes in test samples,\nin order to select the level of complexity of the model that provides the best\npredictive power. Our method is closely related, but it differs in that it is\ntailored for predicting causal effects of a treatment rather than a unit's\noutcome. The challenge is that the \"ground truth\" for a causal effect is not\nobserved for any individual unit: we observe the unit with the treatment, or\nwithout the treatment, but not both at the same time. Thus, it is not obvious\nhow to use cross-validation to determine whether a causal effect has been\naccurately predicted. We propose several novel cross-validation criteria for\nthis problem and demonstrate through simulations the conditions under which\nthey perform better than standard methods for the problem of causal effects. We\nthen apply the method to a large-scale field experiment re-ranking results on a\nsearch engine.\n", "category": [6, 7]}
{"abstract": "  Non-standard distributional approximations have received considerable\nattention in recent years. They often provide more accurate approximations in\nsmall samples, and theoretical improvements in some cases. This paper shows\nthat the seemingly unrelated \"many instruments asymptotics\" and \"small\nbandwidth asymptotics\" share a common structure, where the object determining\nthe limiting distribution is a V-statistic with a remainder that is an\nasymptotically normal degenerate U-statistic. We illustrate how this general\nstructure can be used to derive new results by obtaining a new asymptotic\ndistribution of a series estimator of the partially linear model when the\nnumber of terms in the series approximation possibly grows as fast as the\nsample size, which we call \"many terms asymptotics\".\n", "category": [2, 7, 6]}
{"abstract": "  In this article, we present a novel scheme for segmenting the image boundary\n(with the background) in optoacoustic small animal in vivo imaging systems. The\nmethod utilizes a multiscale edge detection algorithm to generate a binary edge\nmap. A scale dependent morphological operation is employed to clean spurious\nedges. Thereafter, an ellipse is fitted to the edge map through constrained\nparametric transformations and iterative goodness of fit calculations. The\nmethod delimits the tissue edges through the curve fitting model, which has\nshown high levels of accuracy. Thus, this method enables segmentation of\noptoacoutic images with minimal human intervention, by eliminating need of\nscale selection for multiscale processing and seed point determination for\ncontour mapping.\n", "category": [3, 0, 1]}
{"abstract": "  This letter considers waveform design of orthogonal frequency division\nmultiplexing (OFDM) signal for radar applications, and aims at mitigating the\nenvelope fluctuation in OFDM. A novel method is proposed to reduce the\npeak-to-mean envelope power ratio (PMEPR), which is commonly used to evaluate\nthe fluctuation. The proposed method is based on the tone reservation approach,\nin which some bits or subcarriers of OFDM are allocated for decreasing PMEPR.\nWe introduce the coefficient of variation of envelopes (CVE) as the cost\nfunction for waveform optimization, and develop an iterative least squares\nalgorithm. Minimizing CVE leads to distinct PMEPR reduction, and it is\nguaranteed that the cost function monotonically decreases by applying the\niterative algorithm. Simulations demonstrate that the envelope is significantly\nsmoothed by the proposed method.\n", "category": [1, 0, 2]}
{"abstract": "  Optimal behavior in (competitive) situation is traditionally determined with\nthe help of utility functions that measure the payoff of different actions.\nGiven an ordering on the space of revenues (payoffs), the classical axiomatic\napproach of von Neumann and Morgenstern establishes the existence of suitable\nutility functions, and yields to game-theory as the most prominent\nmaterialization of a theory to determine optimal behavior. Although this\nappears to be a most natural approach to risk management too, applications in\ncritical infrastructures often violate the implicit assumption of actions\nleading to deterministic consequences. In that sense, the gameplay in a\ncritical infrastructure risk control competition is intrinsically random in the\nsense of actions having uncertain consequences. Mathematically, this takes us\nto utility functions that are probability-distribution-valued, in which case we\nloose the canonic (in fact every possible) ordering on the space of payoffs,\nand the original techniques of von Neumann and Morgenstern no longer apply.\n  This work introduces a new kind of game in which uncertainty applies to the\npayoff functions rather than the player's actions (a setting that has been\nwidely studied in the literature, yielding to celebrated notions like the\ntrembling hands equilibrium or the purification theorem). In detail, we show\nhow to fix the non-existence of a (canonic) ordering on the space of\nprobability distributions by only mildly restricting the full set to a subset\nthat can be totally ordered. Our vehicle to define the ordering and establish\nbasic game-theory is non-standard analysis and hyperreal numbers.\n", "category": [7, 2, 5, 6, 6]}
{"abstract": "  The linear regression model is widely used in empirical work in Economics,\nStatistics, and many other disciplines. Researchers often include many\ncovariates in their linear model specification in an attempt to control for\nconfounders. We give inference methods that allow for many covariates and\nheteroskedasticity. Our results are obtained using high-dimensional\napproximations, where the number of included covariates are allowed to grow as\nfast as the sample size. We find that all of the usual versions of Eicker-White\nheteroskedasticity consistent standard error estimators for linear models are\ninconsistent under this asymptotics. We then propose a new heteroskedasticity\nconsistent standard error formula that is fully automatic and robust to both\n(conditional)\\ heteroskedasticity of unknown form and the inclusion of possibly\nmany covariates. We apply our findings to three settings: parametric linear\nmodels with many covariates, linear panel models with many fixed effects, and\nsemiparametric semi-linear models with many technical regressors. Simulation\nevidence consistent with our theoretical results is also provided. The proposed\nmethods are also illustrated with an empirical application.\n", "category": [2, 7, 6, 6]}
{"abstract": "  In this work we design a receiver that iteratively passes soft information\nbetween the channel estimation and data decoding stages. The receiver\nincorporates sparsity-based parametric channel estimation. State-of-the-art\nsparsity-based iterative receivers simplify the channel estimation problem by\nrestricting the multipath delays to a grid. Our receiver does not impose such a\nrestriction. As a result it does not suffer from the leakage effect, which\ndestroys sparsity. Communication at near capacity rates in high SNR requires a\nlarge modulation order. Due to the close proximity of modulation symbols in\nsuch systems, the grid-based approximation is of insufficient accuracy. We show\nnumerically that a state-of-the-art iterative receiver with grid-based sparse\nchannel estimation exhibits a bit-error-rate floor in the high SNR regime. On\nthe contrary, our receiver performs very close to the perfect channel state\ninformation bound for all SNR values. We also demonstrate both theoretically\nand numerically that parametric channel estimation works well in dense\nchannels, i.e., when the number of multipath components is large and each\nindividual component cannot be resolved.\n", "category": [0, 1, 2, 6]}
{"abstract": "  The ill-posedness of the inverse problem of recovering a regression function\nin a nonparametric instrumental variable model leads to estimators that may\nsuffer from a very slow, logarithmic rate of convergence. In this paper, we\nshow that restricting the problem to models with monotone regression functions\nand monotone instruments significantly weakens the ill-posedness of the\nproblem. In stark contrast to the existing literature, the presence of a\nmonotone instrument implies boundedness of our measure of ill-posedness when\nrestricted to the space of monotone functions. Based on this result we derive a\nnovel non-asymptotic error bound for the constrained estimator that imposes\nmonotonicity of the regression function. For a given sample size, the bound is\nindependent of the degree of ill-posedness as long as the regression function\nis not too steep. As an implication, the bound allows us to show that the\nconstrained estimator converges at a fast, polynomial rate, independently of\nthe degree of ill-posedness, in a large, but slowly shrinking neighborhood of\nconstant functions. Our simulation study demonstrates significant finite-sample\nperformance gains from imposing monotonicity even when the regression function\nis rather far from being a constant. We apply the constrained estimator to the\nproblem of estimating gasoline demand functions from U.S. data.\n", "category": [6, 7, 2, 6]}
{"abstract": "  Nonparametric methods play a central role in modern empirical work. While\nthey provide inference procedures that are more robust to parametric\nmisspecification bias, they may be quite sensitive to tuning parameter choices.\nWe study the effects of bias correction on confidence interval coverage in the\ncontext of kernel density and local polynomial regression estimation, and prove\nthat bias correction can be preferred to undersmoothing for minimizing coverage\nerror and increasing robustness to tuning parameter choice. This is achieved\nusing a novel, yet simple, Studentization, which leads to a new way of\nconstructing kernel-based bias-corrected confidence intervals. In addition, for\npractical cases, we derive coverage error optimal bandwidths and discuss\neasy-to-implement bandwidth selectors. For interior points, we show that the\nMSE-optimal bandwidth for the original point estimator (before bias correction)\ndelivers the fastest coverage error decay rate after bias correction when\nsecond-order (equivalent) kernels are employed, but is otherwise suboptimal\nbecause it is too \"large\". Finally, for odd-degree local polynomial regression,\nwe show that, as with point estimation, coverage error adapts to boundary\npoints automatically when appropriate Studentization is used; however, the\nMSE-optimal bandwidth for the original point estimator is suboptimal. All the\nresults are established using valid Edgeworth expansions and illustrated with\nsimulated data. Our findings have important consequences for empirical work as\nthey indicate that bias-corrected confidence intervals, coupled with\nappropriate standard errors, have smaller coverage error and are less sensitive\nto tuning parameter choices in practically relevant cases where additional\nsmoothness is available.\n", "category": [2, 7, 6]}
{"abstract": "  This paper makes several important contributions to the literature about\nnonparametric instrumental variables (NPIV) estimation and inference on a\nstructural function $h_0$ and its functionals. First, we derive sup-norm\nconvergence rates for computationally simple sieve NPIV (series 2SLS)\nestimators of $h_0$ and its derivatives. Second, we derive a lower bound that\ndescribes the best possible (minimax) sup-norm rates of estimating $h_0$ and\nits derivatives, and show that the sieve NPIV estimator can attain the minimax\nrates when $h_0$ is approximated via a spline or wavelet sieve. Our optimal\nsup-norm rates surprisingly coincide with the optimal root-mean-squared rates\nfor severely ill-posed problems, and are only a logarithmic factor slower than\nthe optimal root-mean-squared rates for mildly ill-posed problems. Third, we\nuse our sup-norm rates to establish the uniform Gaussian process strong\napproximations and the score bootstrap uniform confidence bands (UCBs) for\ncollections of nonlinear functionals of $h_0$ under primitive conditions,\nallowing for mildly and severely ill-posed problems. Fourth, as applications,\nwe obtain the first asymptotic pointwise and uniform inference results for\nplug-in sieve t-statistics of exact consumer surplus (CS) and deadweight loss\n(DL) welfare functionals under low-level conditions when demand is estimated\nvia sieve NPIV. Empiricists could read our real data application of UCBs for\nexact CS and DL functionals of gasoline demand that reveals interesting\npatterns and is applicable to other markets.\n", "category": [6, 7]}
{"abstract": "  The problem of determining the intrinsic quality of a signal processing\nsystem with respect to the inference of an unknown deterministic parameter\n$\\theta$ is considered. While the Fisher information measure $F(\\theta)$ forms\na classical tool for such a problem, direct computation of the information\nmeasure can become difficult in various situations. For the estimation\ntheoretic performance analysis of nonlinear measurement systems, the form of\nthe likelihood function can make the calculation of the information measure\n$F(\\theta)$ challenging. In situations where no closed-form expression of the\nstatistical system model is available, the analytical derivation of $F(\\theta)$\nis not possible at all. Based on the Cauchy-Schwarz inequality, we derive an\nalternative information measure $S(\\theta)$. It provides a lower bound on the\nFisher information $F(\\theta)$ and has the property of being evaluated with the\nmean, the variance, the skewness and the kurtosis of the system model at hand.\nThese entities usually exhibit good mathematical tractability or can be\ndetermined at low-complexity by real-world measurements in a calibrated setup.\nWith various examples, we show that $S(\\theta)$ provides a good conservative\napproximation for $F(\\theta)$ and outline different estimation theoretic\nproblems where the presented information bound turns out to be useful.\n", "category": [0, 1, 2]}
{"abstract": "  This paper proposes a video encryption algorithm using RSA and Pseudo Noise\n(PN) sequence, aimed at applications requiring sensitive video information\ntransfers. The system is primarily designed to work with files encoded using\nthe Audio Video Interleaved (AVI) codec, although it can be easily ported for\nuse with Moving Picture Experts Group (MPEG) encoded files. The audio and video\ncomponents of the source separately undergo two layers of encryption to ensure\na reasonable level of security. Encryption of the video component involves\napplying the RSA algorithm followed by the PN-based encryption. Similarly, the\naudio component is first encrypted using PN and further subjected to encryption\nusing the Discrete Cosine Transform. Combining these techniques, an efficient\nsystem, invulnerable to security breaches and attacks with favorable values of\nparameters such as encryption/decryption speed, encryption/decryption ratio and\nvisual degradation; has been put forth. For applications requiring encryption\nof sensitive data wherein stringent security requirements are of prime concern,\nthe system is found to yield negligible similarities in visual perception\nbetween the original and the encrypted video sequence. For applications wherein\nvisual similarity is not of major concern, we limit the encryption task to a\nsingle level of encryption which is accomplished by using RSA, thereby\nquickening the encryption process. Although some similarity between the\noriginal and encrypted video is observed in this case, it is not enough to\ncomprehend the happenings in the video.\n", "category": [0, 0, 1]}
{"abstract": "  We consider a model of matching in trading networks in which firms can enter\ninto bilateral contracts. In trading networks, stable outcomes, which are\nimmune to deviations of arbitrary sets of firms, may not exist. We define a new\nsolution concept called trail stability. Trail-stable outcomes are immune to\nconsecutive, pairwise deviations between linked firms. We show that any trading\nnetwork with bilateral contracts has a trail-stable outcome whenever firms'\nchoice functions satisfy the full substitutability condition. For trail-stable\noutcomes, we prove results on the lattice structure, the rural hospitals\ntheorem, strategy-proofness, and comparative statics of firm entry and exit. We\nalso introduce weak trail stability which is implied by trail stability under\nfull substitutability. We describe relationships between the solution concepts.\n", "category": [0, 7, 5]}
{"abstract": "  Segmentation of biomedical images is essential for studying and\ncharacterizing anatomical structures, detection and evaluation of pathological\ntissues. Segmentation has been further shown to enhance the reconstruction\nperformance in many tomographic imaging modalities by accounting for\nheterogeneities of the excitation field and tissue properties in the imaged\nregion. This is particularly relevant in optoacoustic tomography, where\ndiscontinuities in the optical and acoustic tissue properties, if not properly\naccounted for, may result in deterioration of the imaging performance.\nEfficient segmentation of optoacoustic images is often hampered by the\nrelatively low intrinsic contrast of large anatomical structures, which is\nfurther impaired by the limited angular coverage of some commonly employed\ntomographic imaging configurations. Herein, we analyze the performance of\nactive contour models for boundary segmentation in cross-sectional optoacoustic\ntomography. The segmented mask is employed to construct a two compartment model\nfor the acoustic and optical parameters of the imaged tissues, which is\nsubsequently used to improve accuracy of the image reconstruction routines. The\nperformance of the suggested segmentation and modeling approach are showcased\nin tissue-mimicking phantoms and small animal imaging experiments.\n", "category": [3, 0, 1, 3]}
{"abstract": "  The first ever human vs. computer no-limit Texas hold 'em competition took\nplace from April 24-May 8, 2015 at River's Casino in Pittsburgh, PA. In this\narticle I present my thoughts on the competition design, agent architecture,\nand lessons learned.\n", "category": [0, 0, 0, 7]}
{"abstract": "  In this paper, we extend the deep long short-term memory (DLSTM) recurrent\nneural networks by introducing gated direct connections between memory cells in\nadjacent layers. These direct links, called highway connections, enable\nunimpeded information flow across different layers and thus alleviate the\ngradient vanishing problem when building deeper LSTMs. We further introduce the\nlatency-controlled bidirectional LSTMs (BLSTMs) which can exploit the whole\nhistory while keeping the latency under control. Efficient algorithms are\nproposed to train these novel networks using both frame and sequence\ndiscriminative criteria. Experiments on the AMI distant speech recognition\n(DSR) task indicate that we can train deeper LSTMs and achieve better\nimprovement from sequence training with highway LSTMs (HLSTMs). Our novel model\nobtains $43.9/47.7\\%$ WER on AMI (SDM) dev and eval sets, outperforming all\nprevious works. It beats the strong DNN and DLSTM baselines with $15.7\\%$ and\n$5.3\\%$ relative improvement respectively.\n", "category": [0, 0, 0, 0, 1]}
{"abstract": "  In this paper, we investigate the use of prediction-adaptation-correction\nrecurrent neural networks (PAC-RNNs) for low-resource speech recognition. A\nPAC-RNN is comprised of a pair of neural networks in which a {\\it correction}\nnetwork uses auxiliary information given by a {\\it prediction} network to help\nestimate the state probability. The information from the correction network is\nalso used by the prediction network in a recurrent loop. Our model outperforms\nother state-of-the-art neural networks (DNNs, LSTMs) on IARPA-Babel tasks.\nMoreover, transfer learning from a language that is similar to the target\nlanguage can help improve performance further.\n", "category": [0, 0, 0, 1]}
{"abstract": "  Oversubscribed treatments are often allocated using randomized waiting lists.\nApplicants are ranked randomly, and treatment offers are made following that\nranking until all seats are filled. To estimate causal effects, researchers\noften compare applicants getting and not getting an offer. We show that those\ntwo groups are not statistically comparable. Therefore, the estimator arising\nfrom that comparison is inconsistent. We propose a new estimator, and show that\nit is consistent. Finally, we revisit an application, and we show that using\nour estimator can lead to sizably different results from those obtained using\nthe commonly used estimator.\n", "category": [6, 7]}
{"abstract": "  The problem of endogeneity in statistics and econometrics is often handled by\nintroducing instrumental variables (IV) which fulfill the mean independence\nassumption, i.e. the unobservable is mean independent of the instruments. When\nfull independence of IV's and the unobservable is assumed, nonparametric IV\nregression models and nonparametric demand models lead to nonlinear integral\nequations with unknown integral kernels. We prove convergence rates for the\nmean integrated square error of the iteratively regularized Newton method\napplied to these problems. Compared to related results we derive stronger\nconvergence results that rely on weaker nonlinearity restrictions. We\ndemonstrate in numerical simulations for a nonparametric IV regression that the\nmethod produces better results than the standard model.\n", "category": [6, 7, 2, 6]}
{"abstract": "  The game-theoretic risk management framework put forth in the precursor work\n\"Towards a Theory of Games with Payoffs that are Probability-Distributions\"\n(arXiv:1506.07368 [q-fin.EC]) is herein extended by algorithmic details on how\nto compute equilibria in games where the payoffs are probability distributions.\nOur approach is \"data driven\" in the sense that we assume empirical data\n(measurements, simulation, etc.) to be available that can be compiled into\ndistribution models, which are suitable for efficient decisions about\npreferences, and setting up and solving games using these as payoffs. While\npreferences among distributions turn out to be quite simple if nonparametric\nmethods (kernel density estimates) are used, computing Nash-equilibria in games\nusing such models is discovered as inefficient (if not impossible). In fact, we\ngive a counterexample in which fictitious play fails to converge for the\n(specifically unfortunate) choice of payoff distributions in the game, and\nintroduce a suitable tail approximation of the payoff densities to tackle the\nissue. The overall procedure is essentially a modified version of fictitious\nplay, and is herein described for standard and multicriteria games, to\niteratively deliver an (approximate) Nash-equilibrium. An exact method using\nlinear programming is also given.\n", "category": [7, 0, 2, 5, 5, 6]}
{"abstract": "  The Kalman filter and its extensions are used in a vast number of aerospace\nand navigation applications for nonlinear state estimation of time series. In\nthe literature, different approaches have been proposed to exploit the\nstructure of the state and measurement models to reduce the computational\ndemand of the algorithms. In this tutorial, we survey existing code\noptimization methods and present them using unified notation that allows them\nto be used with various Kalman filter extensions. We develop the optimization\nmethods to cover a wider range of models, show how different structural\noptimizations can be combined, and present new applications for the existing\noptimizations. Furthermore, we present an example that shows that the\nexploitation of the structure of the problem can lead to improved estimation\naccuracy while reducing the computational load. This tutorial is intended for\npersons who are familiar with Kalman filtering and want to get insights for\nreducing the computational demand of different Kalman filter extensions.\n", "category": [1, 0, 2, 2]}
{"abstract": "  The problem of determining the achievable sensitivity with digitization\nexhibiting minimal complexity is addressed. In this case, measurements are\nexclusively available in hard-limited form. Assessing the achievable\nsensitivity via the Cram\\'{e}r-Rao lower bound requires characterization of the\nlikelihood function, which is intractable for multivariate binary\ndistributions. In this context, the Fisher matrix of the exponential family and\na lower bound for arbitrary probabilistic models are discussed. The\nconservative approximation for Fisher's information matrix rests on a surrogate\nexponential family distribution connected to the actual data-generating system\nby two compact equivalences. Without characterizing the likelihood and its\nsupport, this probabilistic notion enables designing estimators that\nconsistently achieve the sensitivity as defined by the inverse of the\nconservative information matrix. For parameter estimation with multivariate\nbinary samples, a quadratic exponential surrogate distribution tames\nstatistical complexity such that a quantitative assessment of an achievable\nsensitivity level becomes tractable. This fact is exploited for the performance\nanalysis concerning parameter estimation with an array of low-complexity binary\nsensors in comparison to an ideal system featuring infinite amplitude\nresolution. Additionally, data-driven assessment by estimating a conservative\napproximation for the Fisher matrix under recursive binary sampling as\nimplemented in $\\Sigma\\Delta$-modulating analog-to-digital converters is\ndemonstrated.\n", "category": [0, 1, 2]}
{"abstract": "  The partial (ceteris paribus) effects of interest in nonlinear and\ninteractive linear models are heterogeneous as they can vary dramatically with\nthe underlying observed or unobserved covariates. Despite the apparent\nimportance of heterogeneity, a common practice in modern empirical work is to\nlargely ignore it by reporting average partial effects (or, at best, average\neffects for some groups). While average effects provide very convenient scalar\nsummaries of typical effects, by definition they fail to reflect the entire\nvariety of the heterogeneous effects. In order to discover these effects much\nmore fully, we propose to estimate and report sorted effects -- a collection of\nestimated partial effects sorted in increasing order and indexed by\npercentiles. By construction the sorted effect curves completely represent and\nhelp visualize the range of the heterogeneous effects in one plot. They are as\nconvenient and easy to report in practice as the conventional average partial\neffects. They also serve as a basis for classification analysis, where we\ndivide the observational units into most or least affected groups and summarize\ntheir characteristics. We provide a quantification of uncertainty (standard\nerrors and confidence bands) for the estimated sorted effects and related\nclassification analysis, and provide confidence sets for the most and least\naffected groups. The derived statistical results rely on establishing key, new\nmathematical results on Hadamard differentiability of a multivariate sorting\noperator and a related classification operator, which are of independent\ninterest. We apply the sorted effects method and classification analysis to\ndemonstrate several striking patterns in the gender wage gap.\n", "category": [6, 7]}
{"abstract": "  We propose a general framework for regularization in M-estimation problems\nunder time dependent (absolutely regular-mixing) data which encompasses many of\nthe existing estimators. We derive non-asymptotic concentration bounds for the\nregularized M-estimator. Our results exhibit a variance-bias trade-off, with\nthe variance term being governed by a novel measure of the complexity of the\nparameter set. We also show that the mixing structure affect the variance term\nby scaling the number of observations; depending on the decay rate of the\nmixing coefficients, this scaling can even affect the asymptotic behavior.\nFinally, we propose a data-driven method for choosing the tuning parameters of\nthe regularized estimator which yield the same (up to constants) concentration\nbound as one that optimally balances the (squared) bias and variance terms. We\nillustrate the results with several canonical examples.\n", "category": [2, 7, 6]}
{"abstract": "  Many material and biological samples in scientific imaging are characterized\nby non-local repeating structures. These are studied using scanning electron\nmicroscopy and electron tomography. Sparse sampling of individual pixels in a\n2D image acquisition geometry, or sparse sampling of projection images with\nlarge tilt increments in a tomography experiment, can enable high speed data\nacquisition and minimize sample damage caused by the electron beam.\n  In this paper, we present an algorithm for electron tomographic\nreconstruction and sparse image interpolation that exploits the non-local\nredundancy in images. We adapt a framework, termed plug-and-play (P&P) priors,\nto solve these imaging problems in a regularized inversion setting. The power\nof the P&P approach is that it allows a wide array of modern denoising\nalgorithms to be used as a \"prior model\" for tomography and image\ninterpolation. We also present sufficient mathematical conditions that ensure\nconvergence of the P&P approach, and we use these insights to design a new\nnon-local means denoising algorithm. Finally, we demonstrate that the algorithm\nproduces higher quality reconstructions on both simulated and real electron\nmicroscope data, along with improved convergence properties compared to other\nmethods.\n", "category": [0, 1]}
{"abstract": "  We propose a bootstrap-based calibrated projection procedure to build\nconfidence intervals for single components and for smooth functions of a\npartially identified parameter vector in moment (in)equality models. The method\ncontrols asymptotic coverage uniformly over a large class of data generating\nprocesses. The extreme points of the calibrated projection confidence interval\nare obtained by extremizing the value of the function of interest subject to a\nproper relaxation of studentized sample analogs of the moment (in)equality\nconditions. The degree of relaxation, or critical level, is calibrated so that\nthe function of theta, not theta itself, is uniformly asymptotically covered\nwith prespecified probability. This calibration is based on repeatedly checking\nfeasibility of linear programming problems, rendering it computationally\nattractive.\n  Nonetheless, the program defining an extreme point of the confidence interval\nis generally nonlinear and potentially intricate. We provide an algorithm,\nbased on the response surface method for global optimization, that approximates\nthe solution rapidly and accurately, and we establish its rate of convergence.\nThe algorithm is of independent interest for optimization problems with simple\nobjectives and complicated constraints. An empirical application estimating an\nentry game illustrates the usefulness of the method. Monte Carlo simulations\nconfirm the accuracy of the solution algorithm, the good statistical as well as\ncomputational performance of calibrated projection (including in comparison to\nother methods), and the algorithm's potential to greatly accelerate computation\nof other confidence intervals.\n", "category": [2, 7, 6]}
{"abstract": "  In this paper, we propose a doubly robust method to present the heterogeneity\nof the average treatment effect with respect to observed covariates of\ninterest. We consider a situation where a large number of covariates are needed\nfor identifying the average treatment effect but the covariates of interest for\nanalyzing heterogeneity are of much lower dimension. Our proposed estimator is\ndoubly robust and avoids the curse of dimensionality. We propose a uniform\nconfidence band that is easy to compute, and we illustrate its usefulness via\nMonte Carlo experiments and an application to the effects of smoking on birth\nweights.\n", "category": [6, 7]}
{"abstract": "  The key-leakage-storage region is derived for a generalization of a classic\ntwo-terminal key agreement model. The additions to the model are that the\nencoder observes a hidden, or noisy, version of the identifier, and that the\nencoder and decoder can perform multiple measurements. To illustrate the\nbehavior of the region, the theory is applied to binary identifiers and noise\nmodeled via binary symmetric channels. In particular, the key-leakage-storage\nregion is simplified by applying Mrs. Gerber's lemma twice in different\ndirections to a Markov chain. The growth in the region as the number of\nmeasurements increases is quantified. The amount by which the privacy-leakage\nrate reduces for a hidden identifier as compared to a noise-free (visible)\nidentifier at the encoder is also given. If the encoder incorrectly models the\nsource as visible, it is shown that substantial secrecy leakage may occur and\nthe reliability of the reconstructed key might decrease.\n", "category": [0, 0, 0, 1, 2, 2]}
{"abstract": "  Random pulse repetition interval (PRI) waveform arouses great interests in\nthe field of modern radars due to its ability to alleviate range and Doppler\nambiguities as well as enhance electronic counter-countermeasures (ECCM)\ncapabilities. Theoretical results pertaining to the statistical characteristics\nof ambiguity function (AF) are derived in this work, indicating that the range\nand Doppler ambiguities can be effectively suppressed by increasing the number\nof pulses and the range of PRI jitters. This provides an important guidance in\nterms of waveform design. As is well known, the significantly lifted sidelobe\npedestal induced by PRI randomization will degrade the performance of weak\ntarget detection. Proceeding from that, we propose to employ orthogonal\nmatching pursuit (OMP) to overcome this issue. Simulation results demonstrate\nthat the OMP method can effectively lower the sidelobe pedestal of strong\ntarget and improve the performance of weak target estimation.\n", "category": [6, 1]}
{"abstract": "  The potential of demand side as a frequency reserve proposes interesting\nopportunity in handling imbalances due to intermittent renewable energy\nsources. This paper proposes a novel approach for computing the parameters of a\nstochastic battery model representing the aggregation of Thermostatically\nControlled Loads (TCLs). A hysteresis based non-disruptive control is used\nusing priority stack algorithm to track the reference regulation signal. The\nparameters of admissible ramp-rate and the charge limits of the battery are\ndynamically calculated using the information from TCLs that is the status\n(on/off), availability and relative temperature distance till the switching\nboundary. The approach builds on and improves on the existing research work by\nproviding a straight-forward mechanism for calculation of stochastic parameters\nof equivalent battery model. The effectiveness of proposed approach is\ndemonstrated by a test case having a large number of residential TCLs tracking\na scaled down real frequency regulation signal.\n", "category": [0, 1]}
{"abstract": "  A standard operational requirement in power systems is that the voltage\nmagnitudes lie within prespecified bounds. Conventional engineering wisdom\nsuggests that such a tightly-regulated profile, imposed for system design\npurposes and good operation of the network, should also guarantee a secure\nsystem, operating far from static bifurcation instabilities such as voltage\ncollapse. In general however, these two objectives are distinct and must be\nseparately enforced. We formulate an optimization problem which maximizes the\ndistance to voltage collapse through injections of reactive power, subject to\npower flow and operational voltage constraints. By exploiting a linear\napproximation of the power flow equations we arrive at a convex reformulation\nwhich can be efficiently solved for the optimal injections. We also address the\nplanning problem of allocating the resources by recasting our problem in a\nsparsity-promoting framework that allows us to choose a desired trade-off\nbetween optimality of injections and the number of required actuators. Finally,\nwe present a distributed algorithm to solve the optimization problem, showing\nthat it can be implemented on-line as a feedback controller. We illustrate the\nperformance of our results with the IEEE30 bus network.\n", "category": [2, 0, 1]}
{"abstract": "  We discuss efficient Bayesian estimation of dynamic covariance matrices in\nmultivariate time series through a factor stochastic volatility model. In\nparticular, we propose two interweaving strategies (Yu and Meng, Journal of\nComputational and Graphical Statistics, 20(3), 531-570, 2011) to substantially\naccelerate convergence and mixing of standard MCMC approaches. Similar to\nmarginal data augmentation techniques, the proposed acceleration procedures\nexploit non-identifiability issues which frequently arise in factor models. Our\nnew interweaving strategies are easy to implement and come at almost no extra\ncomputational cost; nevertheless, they can boost estimation efficiency by\nseveral orders of magnitude as is shown in extensive simulation studies. To\nconclude, the application of our algorithm to a 26-dimensional exchange rate\ndata set illustrates the superior performance of the new approach for\nreal-world data.\n", "category": [6, 7, 6, 6]}
{"abstract": "  Boosting is one of the most significant developments in machine learning.\nThis paper studies the rate of convergence of $L_2$Boosting, which is tailored\nfor regression, in a high-dimensional setting. Moreover, we introduce so-called\n\\textquotedblleft post-Boosting\\textquotedblright. This is a post-selection\nestimator which applies ordinary least squares to the variables selected in the\nfirst stage by $L_2$Boosting. Another variant is \\textquotedblleft Orthogonal\nBoosting\\textquotedblright\\ where after each step an orthogonal projection is\nconducted. We show that both post-$L_2$Boosting and the orthogonal boosting\nachieve the same rate of convergence as LASSO in a sparse, high-dimensional\nsetting. We show that the rate of convergence of the classical $L_2$Boosting\ndepends on the design matrix described by a sparse eigenvalue constant. To show\nthe latter results, we derive new approximation results for the pure greedy\nalgorithm, based on analyzing the revisiting behavior of $L_2$Boosting. We also\nintroduce feasible rules for early stopping, which can be easily implemented\nand used in applied work. Our results also allow a direct comparison between\nLASSO and boosting which has been missing from the literature. Finally, we\npresent simulation studies and applications to illustrate the relevance of our\ntheoretical results and to provide insights into the practical aspects of\nboosting. In these simulation studies, post-$L_2$Boosting clearly outperforms\nLASSO.\n", "category": [6, 0, 7, 2, 6, 6]}
{"abstract": "  In this paper, we consider a high-dimensional quantile regression model where\nthe sparsity structure may differ between two sub-populations. We develop\n$\\ell_1$-penalized estimators of both regression coefficients and the threshold\nparameter. Our penalized estimators not only select covariates but also\ndiscriminate between a model with homogeneous sparsity and a model with a\nchange point. As a result, it is not necessary to know or pretest whether the\nchange point is present, or where it occurs. Our estimator of the change point\nachieves an oracle property in the sense that its asymptotic distribution is\nthe same as if the unknown active sets of regression coefficients were known.\nImportantly, we establish this oracle property without a perfect covariate\nselection, thereby avoiding the need for the minimum level condition on the\nsignals of active covariates. Dealing with high-dimensional quantile regression\nwith an unknown change point calls for a new proof technique since the quantile\nloss function is non-smooth and furthermore the corresponding objective\nfunction is non-convex with respect to the change point. The technique\ndeveloped in this paper is applicable to a general M-estimation framework with\na change point, which may be of independent interest. The proposed methods are\nthen illustrated via Monte Carlo experiments and an application to tipping in\nthe dynamics of racial segregation.\n", "category": [6, 7]}
{"abstract": "  Behavior initiation is a form of leadership and is an important aspect of\nsocial organization that affects the processes of group formation, dynamics,\nand decision-making in human societies and other social animal species. In this\nwork, we formalize the \"Coordination Initiator Inference Problem\" and propose a\nsimple yet powerful framework for extracting periods of coordinated activity\nand determining individuals who initiated this coordination, based solely on\nthe activity of individuals within a group during those periods. The proposed\napproach, given arbitrary individual time series, automatically (1) identifies\ntimes of coordinated group activity, (2) determines the identities of\ninitiators of those activities, and (3) classifies the likely mechanism by\nwhich the group coordination occurred, all of which are novel computational\ntasks. We demonstrate our framework on both simulated and real-world data:\ntrajectories tracking of animals as well as stock market data. Our method is\ncompetitive with existing global leadership inference methods but provides the\nfirst approaches for local leadership and coordination mechanism\nclassification. Our results are consistent with ground-truthed biological data\nand the framework finds many known events in financial data which are not\notherwise reflected in the aggregate NASDAQ index. Our method is easily\ngeneralizable to any coordinated time-series data from interacting entities.\n", "category": [0, 0, 0, 7, 3]}
{"abstract": "  The package High-dimensional Metrics (\\Rpackage{hdm}) is an evolving\ncollection of statistical methods for estimation and quantification of\nuncertainty in high-dimensional approximately sparse models. It focuses on\nproviding confidence intervals and significance testing for (possibly many)\nlow-dimensional subcomponents of the high-dimensional parameter vector.\nEfficient estimators and uniformly valid confidence intervals for regression\ncoefficients on target variables (e.g., treatment or policy variable) in a\nhigh-dimensional approximately sparse regression model, for average treatment\neffect (ATE) and average treatment effect for the treated (ATET), as well for\nextensions of these parameters to the endogenous setting are provided. Theory\ngrounded, data-driven methods for selecting the penalization parameter in Lasso\nregressions under heteroscedastic and non-Gaussian errors are implemented.\nMoreover, joint/ simultaneous confidence intervals for regression coefficients\nof a high-dimensional sparse regression are implemented, including a joint\nsignificance test for Lasso regression. Data sets which have been used in the\nliterature and might be useful for classroom demonstration and for testing new\nestimators are included. \\R and the package \\Rpackage{hdm} are open-source\nsoftware projects and can be freely downloaded from CRAN:\n\\texttt{http://cran.r-project.org}.\n", "category": [6, 7, 6]}
{"abstract": "  Visual observation of Cumulus Oocyte Complexes provides only limited\ninformation about its functional competence, whereas the molecular evaluations\nmethods are cumbersome or costly. Image analysis of mammalian oocytes can\nprovide attractive alternative to address this challenge. However, it is\ncomplex, given the huge number of oocytes under inspection and the subjective\nnature of the features inspected for identification. Supervised machine\nlearning methods like random forest with annotations from expert biologists can\nmake the analysis task standardized and reduces inter-subject variability. We\npresent a semi-automatic framework for predicting the class an oocyte belongs\nto, based on multi-object parametric segmentation on the acquired microscopic\nimage followed by a feature based classification using random forests.\n", "category": [0, 1, 3]}
{"abstract": "  In a randomized control trial, the precision of an average treatment effect\nestimator can be improved either by collecting data on additional individuals,\nor by collecting additional covariates that predict the outcome variable. We\npropose the use of pre-experimental data such as a census, or a household\nsurvey, to inform the choice of both the sample size and the covariates to be\ncollected. Our procedure seeks to minimize the resulting average treatment\neffect estimator's mean squared error, subject to the researcher's budget\nconstraint. We rely on a modification of an orthogonal greedy algorithm that is\nconceptually simple and easy to implement in the presence of a large number of\npotential covariates, and does not require any tuning parameters. In two\nempirical applications, we show that our procedure can lead to substantial\ngains of up to 58%, measured either in terms of reductions in data collection\ncosts or in terms of improvements in the precision of the treatment effect\nestimator.\n", "category": [6, 7]}
{"abstract": "  In this paper we focus on subsampling stationary random processes that reside\non the vertices of undirected graphs. Second-order stationary graph signals are\nobtained by filtering white noise and they admit a well-defined power spectrum.\nEstimating the graph power spectrum forms a central component of stationary\ngraph signal processing and related inference tasks. We show that by sampling a\nsignificantly smaller subset of vertices and using simple least squares, we can\nreconstruct the power spectrum of the graph signal from the subsampled\nobservations, without any spectral priors. In addition, a near-optimal greedy\nalgorithm is developed to design the subsampling scheme.\n", "category": [0, 1, 2]}
{"abstract": "  In this paper, we propose a novelmethod to search for precise locations of\npaired note onset and offset in a singing voice signal. In comparison with the\nexisting onset detection algorithms,our approach differs in two key respects.\nFirst, we employ Correntropy, a generalized correlation function inspired from\nReyni's entropy, as a detection function to capture the instantaneous flux\nwhile preserving insensitiveness to outliers. Next, a novel peak picking\nalgorithm is specially designed for this detection function. By calculating the\nfitness of a pre-defined inverse hyperbolic kernel to a detection function, it\nis possible to find an onset and its corresponding offset simultaneously.\nExperimental results show that the proposed method achieves performance\nsignificantly better than or comparable to other state-of-the-art techniques\nfor onset detection in singing voice.\n", "category": [0, 1]}
{"abstract": "  We study factor models augmented by observed covariates that have explanatory\npowers on the unknown factors. In financial factor models, the unknown factors\ncan be reasonably well explained by a few observable proxies, such as the\nFama-French factors. In diffusion index forecasts, identified factors are\nstrongly related to several directly measurable economic variables such as\nconsumption-wealth variable, financial ratios, and term spread. With those\ncovariates, both the factors and loadings are identifiable up to a rotation\nmatrix even only with a finite dimension. To incorporate the explanatory power\nof these covariates, we propose a smoothed principal component analysis (PCA):\n(i) regress the data onto the observed covariates, and (ii) take the principal\ncomponents of the fitted data to estimate the loadings and factors. This allows\nus to accurately estimate the percentage of both explained and unexplained\ncomponents in factors and thus to assess the explanatory power of covariates.\nWe show that both the estimated factors and loadings can be estimated with\nimproved rates of convergence compared to the benchmark method. The degree of\nimprovement depends on the strength of the signals, representing the\nexplanatory power of the covariates on the factors. The proposed estimator is\nrobust to possibly heavy-tailed distributions. We apply the model to forecast\nUS bond risk premia, and find that the observed macroeconomic characteristics\ncontain strong explanatory powers of the factors. The gain of forecast is more\nsubstantial when the characteristics are incorporated to estimate the common\nfactors than directly used for forecasts.\n", "category": [6, 7]}
{"abstract": "  Estimating the long-term effects of treatments is of interest in many fields.\nA common challenge in estimating such treatment effects is that long-term\noutcomes are unobserved in the time frame needed to make policy decisions. One\napproach to overcome this missing data problem is to analyze treatments effects\non an intermediate outcome, often called a statistical surrogate, if it\nsatisfies the condition that treatment and outcome are independent conditional\non the statistical surrogate. The validity of the surrogacy condition is often\ncontroversial. Here we exploit that fact that in modern datasets, researchers\noften observe a large number, possibly hundreds or thousands, of intermediate\noutcomes, thought to lie on or close to the causal chain between the treatment\nand the long-term outcome of interest. Even if none of the individual proxies\nsatisfies the statistical surrogacy criterion by itself, using multiple proxies\ncan be useful in causal inference. We focus primarily on a setting with two\nsamples, an experimental sample containing data about the treatment indicator\nand the surrogates and an observational sample containing information about the\nsurrogates and the primary outcome. We state assumptions under which the\naverage treatment effect be identified and estimated with a high-dimensional\nvector of proxies that collectively satisfy the surrogacy assumption, and\nderive the bias from violations of the surrogacy assumption, and show that even\nif the primary outcome is also observed in the experimental sample, there is\nstill information to be gained from using surrogates.\n", "category": [6, 7, 6]}
{"abstract": "  In this paper, a mathematical model based on the one-parameter Mittag-Leffler\nfunction is proposed to be used for the first time to describe the relation\nbetween unemployment rate and inflation rate, also known as the Phillips curve.\nThe Phillips curve is in the literature often represented by an\nexponential-like shape. On the other hand, Phillips in his fundamental paper\nused a power function in the model definition. Considering that the ordinary as\nwell as generalised Mittag-Leffler function behaves between a purely\nexponential function and a power function it is natural to implement it in the\ndefinition of the model used to describe the relation between the data\nrepresenting the Phillips curve. For the modelling purposes the data of two\ndifferent European economies, France and Switzerland, were used and an\n\"out-of-sample\" forecast was done to compare the performance of the\nMittag-Leffler model to the performance of the power-type and exponential-type\nmodel. The results demonstrate that the ability of the Mittag-Leffler function\nto fit data that manifest signs of stretched exponentials, oscillations or even\ndamped oscillations can be of use when describing economic relations and\nphenomenons, such as the Phillips curve.\n", "category": [7, 5]}
{"abstract": "  This article provides an elaborate overview of current research in extended\nobject tracking. We provide a clear definition of the extended object tracking\nproblem and discuss its delimitation to other types of object tracking. Next,\ndifferent aspects of extended object modelling are extensively discussed.\nSubsequently, we give a tutorial introduction to two basic and well used\nextended object tracking approaches - the random matrix approach and the Kalman\nfilter-based approach for star-convex shapes. The next part treats the tracking\nof multiple extended objects and elaborates how the large number of feasible\nassociation hypotheses can be tackled using both Random Finite Set (RFS) and\nNon-RFS multi-object trackers. The article concludes with a summary of current\napplications, where four example applications involving camera, X-band radar,\nlight detection and ranging (lidar), red-green-blue-depth (RGB-D) sensors are\nhighlighted.\n", "category": [0, 0, 1]}
{"abstract": "  In a unified framework, we provide estimators and confidence bands for a\nvariety of treatment effects when the outcome of interest, typically a\nduration, is subjected to right censoring. Our methodology accommodates\naverage, distributional, and quantile treatment effects under different\nidentifying assumptions including unconfoundedness, local treatment effects,\nand nonlinear differences-in-differences. The proposed estimators are easy to\nimplement, have close-form representation, are fully data-driven upon\nestimation of nuisance parameters, and do not rely on parametric distributional\nassumptions, shape restrictions, or on restricting the potential treatment\neffect heterogeneity across different subpopulations. These treatment effects\nresults are obtained as a consequence of more general results on two-step\nKaplan-Meier estimators that are of independent interest: we provide conditions\nfor applying (i) uniform law of large numbers, (ii) functional central limit\ntheorems, and (iii) we prove the validity of the ordinary nonparametric\nbootstrap in a two-step estimation procedure where the outcome of interest may\nbe randomly censored.\n", "category": [6, 7]}
{"abstract": "  There are many settings where researchers are interested in estimating\naverage treatment effects and are willing to rely on the unconfoundedness\nassumption, which requires that the treatment assignment be as good as random\nconditional on pre-treatment variables. The unconfoundedness assumption is\noften more plausible if a large number of pre-treatment variables are included\nin the analysis, but this can worsen the performance of standard approaches to\ntreatment effect estimation. In this paper, we develop a method for de-biasing\npenalized regression adjustments to allow sparse regression methods like the\nlasso to be used for sqrt{n}-consistent inference of average treatment effects\nin high-dimensional linear models. Given linearity, we do not need to assume\nthat the treatment propensities are estimable, or that the average treatment\neffect is a sparse contrast of the outcome model parameters. Rather, in\naddition standard assumptions used to make lasso regression on the outcome\nmodel consistent under 1-norm error, we only require overlap, i.e., that the\npropensity score be uniformly bounded away from 0 and 1. Procedurally, our\nmethod combines balancing weights with a regularized regression adjustment.\n", "category": [6, 7, 2, 6]}
{"abstract": "  In complicated/nonlinear parametric models, it is generally hard to know\nwhether the model parameters are point identified. We provide computationally\nattractive procedures to construct confidence sets (CSs) for identified sets of\nfull parameters and of subvectors in models defined through a likelihood or a\nvector of moment equalities or inequalities. These CSs are based on level sets\nof optimal sample criterion functions (such as likelihood or optimally-weighted\nor continuously-updated GMM criterions). The level sets are constructed using\ncutoffs that are computed via Monte Carlo (MC) simulations directly from the\nquasi-posterior distributions of the criterions. We establish new Bernstein-von\nMises (or Bayesian Wilks) type theorems for the quasi-posterior distributions\nof the quasi-likelihood ratio (QLR) and profile QLR in partially-identified\nregular models and some non-regular models. These results imply that our MC CSs\nhave exact asymptotic frequentist coverage for identified sets of full\nparameters and of subvectors in partially-identified regular models, and have\nvalid but potentially conservative coverage in models with reduced-form\nparameters on the boundary. Our MC CSs for identified sets of subvectors are\nshown to have exact asymptotic coverage in models with singularities. We also\nprovide results on uniform validity of our CSs over classes of DGPs that\ninclude point and partially identified models. We demonstrate good\nfinite-sample coverage properties of our procedures in two simulation\nexperiments. Finally, our procedures are applied to two non-trivial empirical\nexamples: an airline entry game and a model of trade flows.\n", "category": [6, 7, 2, 6]}
{"abstract": "  This study suggests a new prediction model for chaotic time series inspired\nby the brain emotional learning of mammals. We describe the structure and\nfunction of this model, which is referred to as BELPM (Brain Emotional\nLearning-Based Prediction Model). Structurally, the model mimics the connection\nbetween the regions of the limbic system, and functionally it uses weighted k\nnearest neighbors to imitate the roles of those regions. The learning algorithm\nof BELPM is defined using steepest descent (SD) and the least square estimator\n(LSE). Two benchmark chaotic time series, Lorenz and Henon, have been used to\nevaluate the performance of BELPM. The obtained results have been compared with\nthose of other prediction methods. The results show that BELPM has the\ncapability to achieve a reasonable accuracy for long-term prediction of chaotic\ntime series, using a limited amount of training data and a reasonably low\ncomputational time.\n", "category": [1, 0]}
{"abstract": "  We investigate the performance of mismatched data detection in large\nmultiple-input multiple-output (MIMO) systems, where the prior distribution of\nthe transmit signal used in the data detector differs from the true prior. To\nminimize the performance loss caused by this prior mismatch, we include a\ntuning stage into our recently-proposed large MIMO approximate message passing\n(LAMA) algorithm, which allows us to develop mismatched LAMA algorithms with\noptimal as well as sub-optimal tuning. We show that carefully-selected priors\noften enable simpler and computationally more efficient algorithms compared to\nLAMA with the true prior while achieving near-optimal performance. A\nperformance analysis of our algorithms for a Gaussian prior and a uniform prior\nwithin a hypercube covering the QAM constellation recovers classical and recent\nresults on linear and non-linear MIMO data detection, respectively.\n", "category": [0, 1, 2]}
{"abstract": "  Accurate statistical models of neural spike responses can characterize the\ninformation carried by neural populations. But the limited samples of spike\ncounts during recording usually result in model overfitting. Besides, current\nmodels assume spike counts to be Poisson-distributed, which ignores the fact\nthat many neurons demonstrate over-dispersed spiking behaviour. Although the\nNegative Binomial Generalized Linear Model (NB-GLM) provides a powerful tool\nfor modeling over-dispersed spike counts, the maximum likelihood-based standard\nNB-GLM leads to highly variable and inaccurate parameter estimates. Thus, we\npropose a hierarchical parametric empirical Bayes method to estimate the neural\nspike responses among neuronal population. Our method integrates both\nGeneralized Linear Models (GLMs) and empirical Bayes theory, which aims to (1)\nimprove the accuracy and reliability of parameter estimation, compared to the\nmaximum likelihood-based method for NB-GLM and Poisson-GLM; (2) effectively\ncapture the over-dispersion nature of spike counts from both simulated data and\nexperimental data; and (3) provide insight into both neural interactions and\nspiking behaviours of the neuronal populations. We apply our approach to study\nboth simulated data and experimental neural data. The estimation of simulation\ndata indicates that the new framework can accurately predict mean spike counts\nsimulated from different models and recover the connectivity weights among\nneural populations. The estimation based on retinal neurons demonstrate the\nproposed method outperforms both NB-GLM and Poisson-GLM in terms of the\npredictive log-likelihood of held-out data. Codes are available in\nhttps://doi.org/10.5281/zenodo.4704423\n", "category": [4, 1, 4, 6]}
{"abstract": "  By borrowing methods from complex system analysis, in this paper we analyze\nthe features of the complex relationship that links the development and the\nindustrialization of a country to economic inequality. In order to do this, we\nidentify industrialization as a combination of a monetary index, the GDP per\ncapita, and a recently introduced measure of the complexity of an economy, the\nFitness. At first we explore these relations on a global scale over the time\nperiod 1990--2008 focusing on two different dimensions of inequality: the\ncapital share of income and a Theil measure of wage inequality. In both cases,\nthe movement of inequality follows a pattern similar to the one theorized by\nKuznets in the fifties. We then narrow down the object of study ad we\nconcentrate on wage inequality within the United States. By employing data on\nwages and employment on the approximately 3100 US counties for the time\ninterval 1990--2014, we generalize the Fitness-Complexity algorithm for\ncounties and NAICS sectors, and we investigate wage inequality between\nindustrial sectors within counties. At this scale, in the early nineties we\nrecover a behavior similar to the global one. While, in more recent years, we\nuncover a trend reversal: wage inequality monotonically increases as\nindustrialization levels grow. Hence at a county level, at net of the social\nand institutional factors that differ among countries, we not only observe an\nupturn in inequality but also a change in the structure of the relation between\nwage inequality and development.\n", "category": [7, 5]}
{"abstract": "  In many settings people must give numerical scores to entities from a small\ndiscrete set. For instance, rating physical attractiveness from 1--5 on dating\nsites, or papers from 1--10 for conference reviewing. We study the problem of\nunderstanding when using a different number of options is optimal. We consider\nthe case when scores are uniform random and Gaussian. We study computationally\nwhen using 2, 3, 4, 5, and 10 options out of a total of 100 is optimal in these\nmodels (though our theoretical analysis is for a more general setting with $k$\nchoices from $n$ total options as well as a continuous underlying space). One\nmay expect that using more options would always improve performance in this\nmodel, but we show that this is not necessarily the case, and that using fewer\nchoices---even just two---can surprisingly be optimal in certain situations.\nWhile in theory for this setting it would be optimal to use all 100 options, in\npractice this is prohibitive, and it is preferable to utilize a smaller number\nof options due to humans' limited computational resources. Our results could\nhave many potential applications, as settings requiring entities to be ranked\nby humans are ubiquitous. There could also be applications to other fields such\nas signal or image processing where input values from a large set must be\nmapped to output values in a smaller set.\n", "category": [0, 0, 0, 1, 2, 2]}
{"abstract": "  This paper introduces a general and flexible framework for F0 and\naperiodicity (additive non periodic component) analysis, specifically intended\nfor high-quality speech synthesis and modification applications. The proposed\nframework consists of three subsystems: instantaneous frequency estimator and\ninitial aperiodicity detector, F0 trajectory tracker, and F0 refinement and\naperiodicity extractor. A preliminary implementation of the proposed framework\nsubstantially outperformed (by a factor of 10 in terms of RMS F0 estimation\nerror) existing F0 extractors in tracking ability of temporally varying F0\ntrajectories. The front end aperiodicity detector consists of a complex-valued\nwavelet analysis filter with a highly selective temporal and spectral envelope.\nThis front end aperiodicity detector uses a new measure that quantifies the\ndeviation from periodicity. The measure is less sensitive to slow FM and AM and\nclosely correlates with the signal to noise ratio.\n", "category": [0, 1, 1]}
{"abstract": "  We propose a solution of the multiple target tracking (MTT) problem based on\nsets of trajectories and the random finite set framework. A full Bayesian\napproach to MTT should characterise the distribution of the trajectories given\nthe measurements, as it contains all information about the trajectories. We\nattain this by considering multi-object density functions in which objects are\ntrajectories. For the standard tracking models, we also describe a conjugate\nfamily of multitrajectory density functions.\n", "category": [0, 0, 1]}
{"abstract": "  The issue addressed in this paper is that of testing for common breaks across\nor within equations of a multivariate system. Our framework is very general and\nallows integrated regressors and trends as well as stationary regressors. The\nnull hypothesis is that breaks in different parameters occur at common\nlocations and are separated by some positive fraction of the sample size unless\nthey occur across different equations. Under the alternative hypothesis, the\nbreak dates across parameters are not the same and also need not be separated\nby a positive fraction of the sample size whether within or across equations.\nThe test considered is the quasi-likelihood ratio test assuming normal errors,\nthough as usual the limit distribution of the test remains valid with\nnon-normal errors. Of independent interest, we provide results about the rate\nof convergence of the estimates when searching over all possible partitions\nsubject only to the requirement that each regime contains at least as many\nobservations as some positive fraction of the sample size, allowing break dates\nnot separated by a positive fraction of the sample size across equations.\nSimulations show that the test has good finite sample properties. We also\nprovide an application to issues related to level shifts and persistence for\nvarious measures of inflation to illustrate its usefulness.\n", "category": [2, 7, 5, 6, 6]}
{"abstract": "  For many application areas A/B testing, which partitions users of a system\ninto an A (control) and B (treatment) group to experiment between several\napplication designs, enables Internet companies to optimize their services to\nthe behavioral patterns of their users. Unfortunately, the A/B testing\nframework cannot be applied in a straightforward manner to applications like\nauctions where the users (a.k.a., bidders) submit bids before the partitioning\ninto the A and B groups is made. This paper combines auction theoretic modeling\nwith the A/B testing framework to develop methodology for A/B testing auctions.\nThe accuracy of our method %, assuming the auction is directly comparable to\nideal A/B testing where there is no interference between A and B. Our results\nare based on an extension and improved analysis of the inference method of\nChawla et al. (2014).\n", "category": [0, 7]}
{"abstract": "  This paper develops and implements a nonparametric test of Random Utility\nModels. The motivating application is to test the null hypothesis that a sample\nof cross-sectional demand distributions was generated by a population of\nrational consumers. We test a necessary and sufficient condition for this that\ndoes not rely on any restriction on unobserved heterogeneity or the number of\ngoods. We also propose and implement a control function approach to account for\nendogenous expenditure. An econometric result of independent interest is a test\nfor linear inequality constraints when these are represented as the vertices of\na polyhedron rather than its faces. An empirical application to the U.K.\nHousehold Expenditure Survey illustrates computational feasibility of the\nmethod in demand problems with 5 goods.\n", "category": [2, 7, 6]}
{"abstract": "  We propose two types of Quantile Graphical Models (QGMs) --- Conditional\nIndependence Quantile Graphical Models (CIQGMs) and Prediction Quantile\nGraphical Models (PQGMs). CIQGMs characterize the conditional independence of\ndistributions by evaluating the distributional dependence structure at each\nquantile index. As such, CIQGMs can be used for validation of the graph\nstructure in the causal graphical models (\\cite{pearl2009causality,\nrobins1986new, heckman2015causal}). One main advantage of these models is that\nwe can apply them to large collections of variables driven by non-Gaussian and\nnon-separable shocks. PQGMs characterize the statistical dependencies through\nthe graphs of the best linear predictors under asymmetric loss functions. PQGMs\nmake weaker assumptions than CIQGMs as they allow for misspecification. Because\nof QGMs' ability to handle large collections of variables and focus on specific\nparts of the distributions, we could apply them to quantify tail\ninterdependence. The resulting tail risk network can be used for measuring\nsystemic risk contributions that help make inroads in understanding\ninternational financial contagion and dependence structures of returns under\ndownside market movements.\n  We develop estimation and inference methods for QGMs focusing on the\nhigh-dimensional case, where the number of variables in the graph is large\ncompared to the number of observations. For CIQGMs, these methods and results\ninclude valid simultaneous choices of penalty functions, uniform rates of\nconvergence, and confidence regions that are simultaneously valid. We also\nderive analogous results for PQGMs, which include new results for penalized\nquantile regressions in high-dimensional settings to handle misspecification,\nmany controls, and a continuum of additional conditioning events.\n", "category": [2, 7, 6]}
{"abstract": "  We propose a novel deep learning model, which supports permutation invariant\ntraining (PIT), for speaker independent multi-talker speech separation,\ncommonly known as the cocktail-party problem. Different from most of the prior\narts that treat speech separation as a multi-class regression problem and the\ndeep clustering technique that considers it a segmentation (or clustering)\nproblem, our model optimizes for the separation regression error, ignoring the\norder of mixing sources. This strategy cleverly solves the long-lasting label\npermutation problem that has prevented progress on deep learning based\ntechniques for speech separation. Experiments on the equal-energy mixing setup\nof a Danish corpus confirms the effectiveness of PIT. We believe improvements\nbuilt upon PIT can eventually solve the cocktail-party problem and enable\nreal-world adoption of, e.g., automatic meeting transcription and multi-party\nhuman-computer interaction, where overlapping speech is common.\n", "category": [0, 0, 0, 1]}
{"abstract": "  Bayesian and frequentist criteria are fundamentally different, but often\nposterior and sampling distributions are asymptotically equivalent (e.g.,\nGaussian). For the corresponding limit experiment, we characterize the\nfrequentist size of a certain Bayesian hypothesis test of (possibly nonlinear)\ninequalities. If the null hypothesis is that the (possibly\ninfinite-dimensional) parameter lies in a certain half-space, then the Bayesian\ntest's size is $\\alpha$; if the null hypothesis is a subset of a half-space,\nthen size is above $\\alpha$ (sometimes strictly); and in other cases, size may\nbe above, below, or equal to $\\alpha$. Two examples illustrate our results:\ntesting stochastic dominance and testing curvature of a translog cost function.\n", "category": [2, 7, 6, 6]}
{"abstract": "  In this review, we present econometric and statistical methods for analyzing\nrandomized experiments. For basic experiments we stress randomization-based\ninference as opposed to sampling-based inference. In randomization-based\ninference, uncertainty in estimates arises naturally from the random assignment\nof the treatments, rather than from hypothesized sampling from a large\npopulation. We show how this perspective relates to regression analyses for\nrandomized experiments. We discuss the analyses of stratified, paired, and\nclustered randomized experiments, and we stress the general efficiency gains\nfrom stratification. We also discuss complications in randomized experiments\nsuch as non-compliance. In the presence of non-compliance we contrast\nintention-to-treat analyses with instrumental variables analyses allowing for\ngeneral treatment effect heterogeneity. We consider in detail estimation and\ninference for heterogeneous treatment effects in settings with (possibly many)\ncovariates. These methods allow researchers to explore heterogeneity by\nidentifying subpopulations with different treatment effects while maintaining\nthe ability to construct valid confidence intervals. We also discuss optimal\nassignment to treatment based on covariates in such settings. Finally, we\ndiscuss estimation and inference in experiments in settings with interactions\nbetween units, both in general network settings and in settings where the\npopulation is partitioned into groups with all interactions contained within\nthese groups.\n", "category": [6, 7]}
{"abstract": "  In this paper we discuss recent developments in econometrics that we view as\nimportant for empirical researchers working on policy evaluation questions. We\nfocus on three main areas, where in each case we highlight recommendations for\napplied work. First, we discuss new research on identification strategies in\nprogram evaluation, with particular focus on synthetic control methods,\nregression discontinuity, external validity, and the causal interpretation of\nregression methods. Second, we discuss various forms of supplementary analyses\nto make the identification strategies more credible. These include placebo\nanalyses as well as sensitivity and robustness analyses. Third, we discuss\nrecent advances in machine learning methods for causal effects. These advances\ninclude methods to adjust for differences between treated and control units in\nhigh-dimensional settings, and methods for identifying and estimating\nheterogeneous treatment effects.\n", "category": [6, 7]}
{"abstract": "  This paper proposes a straightforward algorithm to carry out inference in\nlarge time-varying parameter vector autoregressions (TVP-VARs) with mixture\ninnovation components for each coefficient in the system. We significantly\ndecrease the computational burden by approximating the latent indicators that\ndrive the time-variation in the coefficients with a latent threshold process\nthat depends on the absolute size of the shocks. The merits of our approach are\nillustrated with two applications. First, we forecast the US term structure of\ninterest rates and demonstrate forecast gains of the proposed mixture\ninnovation model relative to other benchmark models. Second, we apply our\napproach to US macroeconomic data and find significant evidence for\ntime-varying effects of a monetary policy tightening.\n", "category": [6, 7, 6, 6]}
{"abstract": "  In this paper, we analyze the benefits of including downlink pilots in a\ncell-free massive MIMO system. We derive an approximate per-user achievable\ndownlink rate for conjugate beamforming processing, which takes into account\nboth uplink and downlink channel estimation errors, and power control. A\nperformance comparison is carried out, in terms of per-user net throughput,\nconsidering cell-free massive MIMO operation with and without downlink\ntraining, for different network densities. We take also into account the\nperformance improvement provided by max-min fairness power control in the\ndownlink. Numerical results show that, exploiting downlink pilots, the\nperformance can be considerably improved in low density networks over the\nconventional scheme where the users rely on statistical channel knowledge only.\nIn high density networks, performance improvements are moderate.\n", "category": [0, 1, 2]}
{"abstract": "  We report the first ex post study of the economic impact of sea level rise.\nWe apply two econometric approaches to estimate the past effects of sea level\nrise on the economy of the USA, viz. Barro type growth regressions adjusted for\nspatial patterns and a matching estimator. Unit of analysis is 3063 counties of\nthe USA. We fit growth regressions for 13 time periods and we estimated\nnumerous varieties and robustness tests for both growth regressions and\nmatching estimator. Although there is some evidence that sea level rise has a\npositive effect on economic growth, in most specifications the estimated\neffects are insignificant. We therefore conclude that there is no stable,\nsignificant effect of sea level rise on economic growth. This finding\ncontradicts previous ex ante studies.\n", "category": [5, 7, 6]}
{"abstract": "  We study the problem of Bayesian learning in a dynamical system involving\nstrategic agents with asymmetric information. In a series of seminal papers in\nthe literature, this problem has been investigated under a simplifying model\nwhere myopically selfish players appear sequentially and act once in the game,\nbased on private noisy observations of the system state and public observation\nof past players' actions. It has been shown that there exist information\ncascades where users discard their private information and mimic the action of\ntheir predecessor. In this paper, we provide a framework for studying Bayesian\nlearning dynamics in a more general setting than the one described above. In\nparticular, our model incorporates cases where players are non-myopic and\nstrategically participate for the whole duration of the game, and cases where\nan endogenous process selects which subset of players will act at each time\ninstance. The proposed framework hinges on a sequential decomposition\nmethodology for finding structured perfect Bayesian equilibria (PBE) of a\ngeneral class of dynamic games with asymmetric information, where user-specific\nstates evolve as conditionally independent Markov processes and users make\nindependent noisy observations of their states. Using this methodology, we\nstudy a specific dynamic learning model where players make decisions about\npublic investment based on their estimates of everyone's types. We characterize\na set of informational cascades for this problem where learning stops for the\nteam as a whole. We show that in such cascades, all players' estimates of other\nplayers' types freeze even though each individual player asymptotically learns\nits own true type.\n", "category": [0, 0, 7]}
{"abstract": "  Many economic and causal parameters depend on nonparametric or high\ndimensional first steps. We give a general construction of locally\nrobust/orthogonal moment functions for GMM, where moment conditions have zero\nderivative with respect to first steps. We show that orthogonal moment\nfunctions can be constructed by adding to identifying moments the nonparametric\ninfluence function for the effect of the first step on identifying moments.\nOrthogonal moments reduce model selection and regularization bias, as is very\nimportant in many applications, especially for machine learning first steps.\n  We give debiased machine learning estimators of functionals of high\ndimensional conditional quantiles and of dynamic discrete choice parameters\nwith high dimensional state variables. We show that adding to identifying\nmoments the nonparametric influence function provides a general construction of\northogonal moments, including regularity conditions, and show that the\nnonparametric influence function is robust to additional unknown functions on\nwhich it depends. We give a general approach to estimating the unknown\nfunctions in the nonparametric influence function and use it to automatically\ndebias estimators of functionals of high dimensional conditional location\nlearners. We give a variety of new doubly robust moment equations and\ncharacterize double robustness. We give general and simple regularity\nconditions and apply these for asymptotic inference on functionals of high\ndimensional regression quantiles and dynamic discrete choice parameters with\nhigh dimensional state variables.\n", "category": [2, 7, 6]}
{"abstract": "  Most modern supervised statistical/machine learning (ML) methods are\nexplicitly designed to solve prediction problems very well. Achieving this goal\ndoes not imply that these methods automatically deliver good estimators of\ncausal parameters. Examples of such parameters include individual regression\ncoefficients, average treatment effects, average lifts, and demand or supply\nelasticities. In fact, estimates of such causal parameters obtained via naively\nplugging ML estimators into estimating equations for such parameters can behave\nvery poorly due to the regularization bias. Fortunately, this regularization\nbias can be removed by solving auxiliary prediction problems via ML tools.\nSpecifically, we can form an orthogonal score for the target low-dimensional\nparameter by combining auxiliary and main ML predictions. The score is then\nused to build a de-biased estimator of the target parameter which typically\nwill converge at the fastest possible 1/root(n) rate and be approximately\nunbiased and normal, and from which valid confidence intervals for these\nparameters of interest may be constructed. The resulting method thus could be\ncalled a \"double ML\" method because it relies on estimating primary and\nauxiliary predictive models. In order to avoid overfitting, our construction\nalso makes use of the K-fold sample splitting, which we call cross-fitting.\nThis allows us to use a very broad set of ML predictive methods in solving the\nauxiliary and main prediction problems, such as random forest, lasso, ridge,\ndeep neural nets, boosted trees, as well as various hybrids and aggregators of\nthese methods.\n", "category": [6, 7]}
{"abstract": "  In this article the package High-dimensional Metrics (\\texttt{hdm}) is\nintroduced. It is a collection of statistical methods for estimation and\nquantification of uncertainty in high-dimensional approximately sparse models.\nIt focuses on providing confidence intervals and significance testing for\n(possibly many) low-dimensional subcomponents of the high-dimensional parameter\nvector. Efficient estimators and uniformly valid confidence intervals for\nregression coefficients on target variables (e.g., treatment or policy\nvariable) in a high-dimensional approximately sparse regression model, for\naverage treatment effect (ATE) and average treatment effect for the treated\n(ATET), as well for extensions of these parameters to the endogenous setting\nare provided. Theory grounded, data-driven methods for selecting the\npenalization parameter in Lasso regressions under heteroscedastic and\nnon-Gaussian errors are implemented. Moreover, joint/ simultaneous confidence\nintervals for regression coefficients of a high-dimensional sparse regression\nare implemented. Data sets which have been used in the literature and might be\nuseful for classroom demonstration and for testing new estimators are included.\n", "category": [6, 7, 6]}
{"abstract": "  This paper considers inference on fixed effects in a linear regression model\nestimated from network data. An important special case of our setup is the\ntwo-way regression model. This is a workhorse technique in the analysis of\nmatched data sets, such as employer-employee or student-teacher panel data. We\nformalize how the structure of the network affects the accuracy with which the\nfixed effects can be estimated. This allows us to derive sufficient conditions\non the network for consistent estimation and asymptotically-valid inference to\nbe possible. Estimation of moments is also considered. We allow for general\nnetworks and our setup covers both the dense and sparse case. We provide\nnumerical results for the estimation of teacher value-added models and\nregressions with occupational dummies.\n", "category": [6, 7]}
{"abstract": "  In this paper we consider a time-division duplex cell-free massive\nmultiple-input multiple-output (MIMO) system where many distributed access\npoints (APs) simultaneously serve many users. A normalized conjugate\nbeamforming scheme, which satisfies short-term average power constraints at the\nAPs, is proposed and analyzed taking into account the effect of imperfect\nchannel information. We derive an approximate closed-form expression for the\nper-user achievable downlink rate of this scheme. We also provide, analytically\nand numerically, a performance comparison between the normalized conjugate\nbeamforming and the conventional conjugate beamforming scheme in [1] (which\nsatisfies long-term average power constraints). Normalized conjugate\nbeamforming scheme reduces the beamforming uncertainty gain, which comes from\nthe users' lack of the channel state information knowledge, and hence, it\nimproves the achievable downlink rate compared to the conventional conjugate\nbeamforming scheme.\n", "category": [0, 1, 2]}
{"abstract": "  Quantile and quantile effect functions are important tools for descriptive\nand causal analyses due to their natural and intuitive interpretation. Existing\ninference methods for these functions do not apply to discrete random\nvariables. This paper offers a simple, practical construction of simultaneous\nconfidence bands for quantile and quantile effect functions of possibly\ndiscrete random variables. It is based on a natural transformation of\nsimultaneous confidence bands for distribution functions, which are readily\navailable for many problems. The construction is generic and does not depend on\nthe nature of the underlying problem. It works in conjunction with parametric,\nsemiparametric, and nonparametric modeling methods for observed and\ncounterfactual distributions, and does not depend on the sampling scheme. We\napply our method to characterize the distributional impact of insurance\ncoverage on health care utilization and obtain the distributional decomposition\nof the racial test score gap. We find that universal insurance coverage\nincreases the number of doctor visits across the entire distribution, and that\nthe racial test score gap is small at early ages but grows with age due to\nsocio economic factors affecting child development especially at the top of the\ndistribution. These are new, interesting empirical findings that complement\nprevious analyses that focused on mean effects only. In both applications, the\noutcomes of interest are discrete rendering existing inference methods invalid\nfor obtaining uniform confidence bands for observed and counterfactual quantile\nfunctions and for their difference -- the quantile effects functions.\n", "category": [6, 7]}
{"abstract": "  First-best climate policy is a uniform carbon tax which gradually rises over\ntime. Civil servants have complicated climate policy to expand bureaucracies,\npoliticians to create rents. Environmentalists have exaggerated climate change\nto gain influence, other activists have joined the climate bandwagon. Opponents\nto climate policy have attacked the weaknesses in climate research. The climate\ndebate is convoluted and polarized as a result, and climate policy complex.\nClimate policy should become easier and more rational as the Paris Agreement\nhas shifted climate policy back towards national governments. Changing\npolitical priorities, austerity, and a maturing bureaucracy should lead to a\nmore constructive climate debate.\n", "category": [5, 7, 5]}
{"abstract": "  We address the curse of dimensionality in dynamic covariance estimation by\nmodeling the underlying co-volatility dynamics of a time series vector through\nlatent time-varying stochastic factors. The use of a global-local shrinkage\nprior for the elements of the factor loadings matrix pulls loadings on\nsuperfluous factors towards zero. To demonstrate the merits of the proposed\nframework, the model is applied to simulated data as well as to daily\nlog-returns of 300 S&P 500 members. Our approach yields precise correlation\nestimates, strong implied minimum variance portfolio performance and superior\nforecasting accuracy in terms of log predictive scores when compared to typical\nbenchmarks.\n", "category": [6, 7, 5, 6, 6]}
{"abstract": "  This paper studies a penalized statistical decision rule for the treatment\nassignment problem. Consider the setting of a utilitarian policy maker who must\nuse sample data to allocate a binary treatment to members of a population,\nbased on their observable characteristics. We model this problem as a\nstatistical decision problem where the policy maker must choose a subset of the\ncovariate space to assign to treatment, out of a class of potential subsets. We\nfocus on settings in which the policy maker may want to select amongst a\ncollection of constrained subset classes: examples include choosing the number\nof covariates over which to perform best-subset selection, and model selection\nwhen approximating a complicated class via a sieve. We adapt and extend results\nfrom statistical learning to develop the Penalized Welfare Maximization (PWM)\nrule. We establish an oracle inequality for the regret of the PWM rule which\nshows that it is able to perform model selection over the collection of\navailable classes. We then use this oracle inequality to derive relevant bounds\non maximum regret for PWM. An important consequence of our results is that we\nare able to formalize model-selection using a \"hold-out\" procedure, where the\npolicy maker would first estimate various policies using half of the data, and\nthen select the policy which performs the best when evaluated on the other half\nof the data.\n", "category": [2, 7, 6]}
{"abstract": "  In this paper, we are interested in learning the underlying graph structure\nbehind training data. Solving this basic problem is essential to carry out any\ngraph signal processing or machine learning task. To realize this, we assume\nthat the data is smooth with respect to the graph topology, and we parameterize\nthe graph topology using an edge sampling function. That is, the graph\nLaplacian is expressed in terms of a sparse edge selection vector, which\nprovides an explicit handle to control the sparsity level of the graph. We\nsolve the sparse graph learning problem given some training data in both the\nnoiseless and noisy settings. Given the true smooth data, the posed sparse\ngraph learning problem can be solved optimally and is based on simple rank\nordering. Given the noisy data, we show that the joint sparse graph learning\nand denoising problem can be simplified to designing only the sparse edge\nselection vector, which can be solved using convex optimization.\n", "category": [0, 1]}
{"abstract": "  We describe Microsoft's conversational speech recognition system, in which we\ncombine recent developments in neural-network-based acoustic and language\nmodeling to advance the state of the art on the Switchboard recognition task.\nInspired by machine learning ensemble techniques, the system uses a range of\nconvolutional and recurrent neural networks. I-vector modeling and lattice-free\nMMI training provide significant gains for all acoustic model architectures.\nLanguage model rescoring with multiple forward and backward running RNNLMs, and\nword posterior-based system combination provide a 20% boost. The best single\nsystem uses a ResNet architecture acoustic model with RNNLM rescoring, and\nachieves a word error rate of 6.9% on the NIST 2000 Switchboard task. The\ncombined system has an error rate of 6.2%, representing an improvement over\npreviously reported results on this benchmark task.\n", "category": [0, 1]}
{"abstract": "  The work done in this paper, proposes a complex Laplacian-based distributed\ncontrol scheme for convergence in the multi-agent network. The proposed scheme\nhas been designated as cascade formulation. The proposed technique exploits the\ntraditional method of organizing large scattered networks into smaller\ninterconnected clusters to optimize information flow within the network. The\ncomplex Laplacian-based approach results in a hierarchical structure, with\nformation of a meta-cluster leading other clusters in the network. The proposed\nformulation enables flexibility to constrain the eigen spectra of the overall\nclosed-loop dynamics, ensuring desired convergence rate and control input\nintensity. The sufficient conditions ensuring globally stable formation for\nproposed formulation are also asserted. Robustness of the proposed formulation\nto uncertainties like loss in communication links and actuator failure has also\nbeen discussed. The effectiveness of the proposed approach is illustrated by\nsimulating a finitely large network of thirty vehicles.\n", "category": [2, 0, 1]}
{"abstract": "  The moment conditions or estimating equations for instrumental variables\nquantile regression involve the discontinuous indicator function. We instead\nuse smoothed estimating equations (SEE), with bandwidth $h$. We show that the\nmean squared error (MSE) of the vector of the SEE is minimized for some $h>0$,\nleading to smaller asymptotic MSE of the estimating equations and associated\nparameter estimators. The same MSE-optimal $h$ also minimizes the higher-order\ntype I error of a SEE-based $\\chi^2$ test and increases size-adjusted power in\nlarge samples. Computation of the SEE estimator also becomes simpler and more\nreliable, especially with (more) endogenous regressors. Monte Carlo simulations\ndemonstrate all of these superior properties in finite samples, and we apply\nour estimator to JTPA data. Smoothing the estimating equations is not just a\ntechnical operation for establishing Edgeworth expansions and bootstrap\nrefinements; it also brings the real benefits of having more precise estimators\nand more powerful tests. Code for the estimator, simulations, and empirical\nexamples is available from the first author's website.\n", "category": [6, 7, 2, 6, 6]}
{"abstract": "  Using and extending fractional order statistic theory, we characterize the\n$O(n^{-1})$ coverage probability error of the previously proposed confidence\nintervals for population quantiles using $L$-statistics as endpoints in Hutson\n(1999). We derive an analytic expression for the $n^{-1}$ term, which may be\nused to calibrate the nominal coverage level to get\n$O\\bigl(n^{-3/2}[\\log(n)]^3\\bigr)$ coverage error. Asymptotic power is shown to\nbe optimal. Using kernel smoothing, we propose a related method for\nnonparametric inference on conditional quantiles. This new method compares\nfavorably with asymptotic normality and bootstrap methods in theory and in\nsimulations. Code is available from the second author's website for both\nunconditional and conditional methods, simulations, and empirical examples.\n", "category": [2, 7, 6, 6, 6]}
{"abstract": "  To help mitigate road congestion caused by the unrelenting growth of traffic\ndemand, many transit authorities have implemented managed lane policies.\nManaged lanes typically run parallel to a freeway's standard, general-purpose\n(GP) lanes, but are restricted to certain types of vehicles. It was originally\nthought that managed lanes would improve the use of existing infrastructure\nthrough incentivization of demand-management behaviors like carpooling, but\nimplementations have often been characterized by unpredicted phenomena that is\noften to detrimental system performance.\n  This paper presents several macroscopic traffic modeling tools we have used\nfor study of freeways equipped with managed lanes, or \"managed lane-freeway\nnetworks.\" The proposed framework is based on the widely-used first-order\nkinematic wave theory. In this model, the GP and the managed lanes are modeled\nas parallel links connected by nodes, where certain type of traffic may switch\nbetween GP and managed lane links. Two types of managed lane topologies are\nconsidered: full-access, where vehicles can switch between the GP and the\nmanaged lanes anywhere; and separated, where such switching is allowed only at\ncertain locations called gates.\n  We also describe methods to incorporate in three phenomena into our model\nthat are particular to managed lane-freeway networks. The inertia effect\nreflects drivers' inclination to stay in their lane as long as possible and\nswitch only if this would obviously improve their travel condition. The\nfriction effect reflects the empirically-observed driver fear of moving fast in\na managed lane while traffic in the adjacent GP lanes moves slowly due to\ncongestion. The smoothing effect describes how managed lanes can increase\nthroughput at bottlenecks by reducing lane changes. We present simple models\nfor each of these phenomena that fit within the general macroscopic theory.\n", "category": [0, 1, 3]}
{"abstract": "  Numerous linear and non-linear data-detection and precoding algorithms for\nwideband massive multi-user (MU) multiple-input multiple-output (MIMO) wireless\nsystems that rely on orthogonal frequency-division multiplexing (OFDM) or\nsingle-carrier frequency-division multiple access (SC-FDMA) require the\ncomputation of the Gram matrix for each active subcarrier. Computing the Gram\nmatrix for each active subcarrier, however, results in excessively high\ncomputational complexity. In this paper, we propose novel, approximate\nalgorithms that significantly reduce the complexity of Gram-matrix computation\nby simultaneously exploiting correlation across subcarriers and channel\nhardening. We show analytically that a small fraction of Gram-matrix\ncomputations in combination with approximate interpolation schemes are\nsufficient to achieve near-optimal error-rate performance at low computational\ncomplexity in massive MU-MIMO systems. We also demonstrate that the proposed\nmethods exhibit improved robustness against channel-estimation errors compared\nto exact Gram-matrix interpolation algorithms that typically require high\ncomputational complexity.\n", "category": [0, 1, 2]}
{"abstract": "  Covert communication, also known as low probability of detection (LPD)\ncommunication, prevents the adversary from knowing that a communication is\ntaking place. Recent work has demonstrated that, in a three-party scenario with\na transmitter (Alice), intended recipient (Bob), and adversary (Warden Willie),\nthe maximum number of bits that can be transmitted reliably from Alice to Bob\nwithout detection by Willie, when additive white Gaussian noise (AWGN) channels\nexist between all parties, is on the order of the square root of the number of\nchannel uses. In this paper, we begin consideration of network scenarios by\nstudying the case where there are additional \"friendly\" nodes present in the\nenvironment that can produce artificial noise to aid in hiding the\ncommunication. We establish achievability results by considering constructions\nwhere the system node closest to the warden produces artificial noise and\ndemonstrate a significant improvement in the throughput achieved covertly,\nwithout requiring close coordination between Alice and the noise-generating\nnode. Conversely, under mild restrictions on the communication strategy, we\ndemonstrate no higher covert throughput is possible. Extensions to the\nconsideration of the achievable covert throughput when multiple wardens\nrandomly located in the environment collaborate to attempt detection of the\ntransmitter are also considered.\n", "category": [0, 0, 1, 2]}
{"abstract": "  We propose generalized random forests, a method for non-parametric\nstatistical estimation based on random forests (Breiman, 2001) that can be used\nto fit any quantity of interest identified as the solution to a set of local\nmoment equations. Following the literature on local maximum likelihood\nestimation, our method considers a weighted set of nearby training examples;\nhowever, instead of using classical kernel weighting functions that are prone\nto a strong curse of dimensionality, we use an adaptive weighting function\nderived from a forest designed to express heterogeneity in the specified\nquantity of interest. We propose a flexible, computationally efficient\nalgorithm for growing generalized random forests, develop a large sample theory\nfor our method showing that our estimates are consistent and asymptotically\nGaussian, and provide an estimator for their asymptotic variance that enables\nvalid confidence intervals. We use our approach to develop new methods for\nthree statistical tasks: non-parametric quantile regression, conditional\naverage partial effect estimation, and heterogeneous treatment effect\nestimation via instrumental variables. A software implementation, grf for R and\nC++, is available from CRAN.\n", "category": [6, 7, 6]}
{"abstract": "  We consider a variable selection problem for the prediction of binary\noutcomes. We study the best subset selection procedure by which the covariates\nare chosen by maximizing Manski (1975, 1985)'s maximum score objective function\nsubject to a constraint on the maximal number of selected variables. We show\nthat this procedure can be equivalently reformulated as solving a mixed integer\noptimization problem, which enables computation of the exact or an approximate\nsolution with a definite approximation error bound. In terms of theoretical\nresults, we obtain non-asymptotic upper and lower risk bounds when the\ndimension of potential covariates is possibly much larger than the sample size.\nOur upper and lower risk bounds are minimax rate-optimal when the maximal\nnumber of selected variables is fixed and does not increase with the sample\nsize. We illustrate usefulness of the best subset binary prediction approach\nvia Monte Carlo simulations and an empirical application of the work-trip\ntransportation mode choice.\n", "category": [6, 7]}
{"abstract": "  Biological and advanced cyberphysical control systems often have limited,\nsparse, uncertain, and distributed communication and computing in addition to\nsensing and actuation. Fortunately, the corresponding plants and performance\nrequirements are also sparse and structured, and this must be exploited to make\nconstrained controller design feasible and tractable. We introduce a new\n\"system level\" (SL) approach involving three complementary SL elements. System\nLevel Parameterizations (SLPs) generalize state space and Youla\nparameterizations of all stabilizing controllers and the responses they\nachieve, and combine with System Level Constraints (SLCs) to parameterize the\nlargest known class of constrained stabilizing controllers that admit a convex\ncharacterization, generalizing quadratic invariance (QI). SLPs also lead to a\ngeneralization of detectability and stabilizability, suggesting the existence\nof a rich separation structure, that when combined with SLCs, is naturally\napplicable to structurally constrained controllers and systems. We further\nprovide a catalog of useful SLCs, most importantly including sparsity, delay,\nand locality constraints on both communication and computing internal to the\ncontroller, and external system performance. The resulting System Level\nSynthesis (SLS) problems that arise define the broadest known class of\nconstrained optimal control problems that can be solved using convex\nprogramming. An example illustrates how this system level approach can\nsystematically explore tradeoffs in controller performance, robustness, and\nsynthesis/implementation complexity.\n", "category": [1, 0, 2]}
{"abstract": "  Conversational speech recognition has served as a flagship speech recognition\ntask since the release of the Switchboard corpus in the 1990s. In this paper,\nwe measure the human error rate on the widely used NIST 2000 test set, and find\nthat our latest automated system has reached human parity. The error rate of\nprofessional transcribers is 5.9% for the Switchboard portion of the data, in\nwhich newly acquainted pairs of people discuss an assigned topic, and 11.3% for\nthe CallHome portion where friends and family members have open-ended\nconversations. In both cases, our automated system establishes a new state of\nthe art, and edges past the human benchmark, achieving error rates of 5.8% and\n11.0%, respectively. The key to our system's performance is the use of various\nconvolutional and LSTM acoustic model architectures, combined with a novel\nspatial smoothing method and lattice-free MMI acoustic training, multiple\nrecurrent neural network language modeling approaches, and a systematic use of\nsystem combination.\n", "category": [0, 1]}
{"abstract": "  This paper formulates the inverse power flow problem which is to infer the\nnodal admittance matrix (hence the network structure of the power system) from\nvoltage and current phasors measured at a number of buses. We show that the\nadmittance matrix can be uniquely identified from a sequence of measurements\ncorresponding to different steady states when every node in the system is\nequipped with a measurement device, and a Kron-reduced admittance matrix can be\ndetermined even if some nodes in the system are not monitored (hidden nodes).\nFurthermore, we propose effective algorithms based on graph theory to uncover\nthe actual admittance matrix of radial systems with hidden nodes. We provide\ntheoretical guarantees for the recovered admittance matrix and demonstrate that\nthe actual admittance matrix can be fully recovered even from the Kron-reduced\nadmittance matrix under some mild assumptions. Simulations on standard test\nsystems confirm that these algorithms are capable of providing accurate\nestimates of the admittance matrix from noisy sensor data.\n", "category": [1, 0, 2]}
{"abstract": "  We present the Stata commands probitfe and logitfe, which estimate probit and\nlogit panel data models with individual and/or time unobserved effects. Fixed\neffect panel data methods that estimate the unobserved effects can be severely\nbiased because of the incidental parameter problem (Neyman and Scott, 1948). We\ntackle this problem by using the analytical and jackknife bias corrections\nderived in Fernandez-Val and Weidner (2016) for panels where the two dimensions\n($N$ and $T$) are moderately large. We illustrate the commands with an\nempirical application to international trade and a Monte Carlo simulation\ncalibrated to this application.\n", "category": [6, 7]}
{"abstract": "  The Counterfactual package implements the estimation and inference methods of\nChernozhukov, Fern\\'andez-Val and Melly (2013) for counterfactual analysis. The\ncounterfactual distributions considered are the result of changing either the\nmarginal distribution of covariates related to the outcome variable of\ninterest, or the conditional distribution of the outcome given the covariates.\nThey can be applied to estimate quantile treatment effects and wage\ndecompositions. This paper serves as an introduction to the package and\ndisplays basic functionality of the commands contained within.\n", "category": [6, 7]}
{"abstract": "  The statistical behavior of the eigenvalues of the sample covariance matrix\n(SCM) plays a key role in determining the performance of adaptive beamformers\n(ABF) in presence of noise. This paper presents a method to compute the\napproximate eigenvalue density function (EDF) for the SCM of a \\cin{} field\nwhen only a finite number of shapshots are available. The EDF of the ensemble\ncovariance matrix (ECM) is modeled as an atomic density with many fewer atoms\nthan the SCM size. The model results in substantial computational savings over\nmore direct methods of computing the EDF. The approximate EDF obtained from\nthis method agrees closely with histograms of eigenvalues obtained from\nsimulation.\n", "category": [0, 1]}
{"abstract": "  The R package quantreg.nonpar implements nonparametric quantile regression\nmethods to estimate and make inference on partially linear quantile models.\nquantreg.nonpar obtains point estimates of the conditional quantile function\nand its derivatives based on series approximations to the nonparametric part of\nthe model. It also provides pointwise and uniform confidence intervals over a\nregion of covariate values and/or quantile indices for the same functions using\nanalytical and resampling methods. This paper serves as an introduction to the\npackage and displays basic functionality of the functions contained within.\n", "category": [6, 7]}
{"abstract": "  The array polynomial is the z-transform of the array weights for a narrowband\nplanewave beamformer using a uniform linear array (ULA). Evaluating the array\npolynomial on the unit circle in the complex plane yields the beampattern. The\nlocations of the polynomial zeros on the unit circle indicate the nulls of the\nbeampattern. For planewave signals measured with a ULA, the locations of the\nensemble MVDR polynomial zeros are constrained on the unit circle. However,\nsample matrix inversion (SMI) MVDR polynomial zeros generally do not fall on\nthe unit circle. The proposed unit circle MVDR (UC MVDR) projects the zeros of\nthe SMI MVDR polynomial radially on the unit circle. This satisfies the\nconstraint on the zeros of ensemble MVDR polynomial. Numerical simulations show\nthat the UC MVDR beamformer suppresses interferers better than the SMI MVDR and\nthe diagonal loaded MVDR beamformer and also improves the white noise gain\n(WNG).\n", "category": [0, 1, 2]}
{"abstract": "  This paper develops inferential methods for a very general class of ill-posed\nmodels in econometrics encompassing the nonparametric instrumental variable\nregression, various functional regressions, and the density deconvolution. We\nfocus on uniform confidence sets for the parameter of interest estimated with\nTikhonov regularization, as in Darolles, Fan, Florens, and Renault (2011).\nSince it is impossible to have inferential methods based on the central limit\ntheorem, we develop two alternative approaches relying on the concentration\ninequality and bootstrap approximations. We show that expected diameters and\ncoverage properties of resulting sets have uniform validity over a large class\nof models, i.e., constructed confidence sets are honest. Monte Carlo\nexperiments illustrate that introduced confidence sets have reasonable width\nand coverage properties. Using U.S. data, we provide uniform confidence sets\nfor Engel curves for various commodities.\n", "category": [2, 7, 6, 6, 6]}
{"abstract": "  Detecting and classifying targets in video streams from surveillance cameras\nis a cumbersome, error-prone and expensive task. Often, the incurred costs are\nprohibitive for real-time monitoring. This leads to data being stored locally\nor transmitted to a central storage site for post-incident examination. The\nrequired communication links and archiving of the video data are still\nexpensive and this setup excludes preemptive actions to respond to imminent\nthreats. An effective way to overcome these limitations is to build a smart\ncamera that transmits alerts when relevant video sequences are detected. Deep\nneural networks (DNNs) have come to outperform humans in visual classifications\ntasks. The concept of DNNs and Convolutional Networks (ConvNets) can easily be\nextended to make use of higher-dimensional input data such as multispectral\ndata. We explore this opportunity in terms of achievable accuracy and required\ncomputational effort. To analyze the precision of DNNs for scene labeling in an\nurban surveillance scenario we have created a dataset with 8 classes obtained\nin a field experiment. We combine an RGB camera with a 25-channel VIS-NIR\nsnapshot sensor to assess the potential of multispectral image data for target\nclassification. We evaluate several new DNNs, showing that the spectral\ninformation fused together with the RGB frames can be used to improve the\naccuracy of the system or to achieve similar accuracy with a 3x smaller\ncomputation effort. We achieve a very high per-pixel accuracy of 99.1%. Even\nfor scarcely occurring, but particularly interesting classes, such as cars, 75%\nof the pixels are labeled correctly with errors occurring only around the\nborder of the objects. This high accuracy was obtained with a training set of\nonly 30 labeled images, paving the way for fast adaptation to various\napplication scenarios.\n", "category": [0, 0, 0, 1, 1]}
{"abstract": "  Product codes are a concatenated error-correction scheme that has been often\nconsidered for applications requiring very low bit-error rates, which demand\nthat the error floor be decreased as much as possible. In this work, we\nconsider product codes constructed from polynomial algebraic codes, and propose\na novel low-complexity post-processing technique that is able to improve the\nerror-correction performance by orders of magnitude. We provide lower bounds\nfor the error rate achievable under post processing, and present simulation\nresults indicating that these bounds are tight.\n", "category": [0, 1, 2]}
{"abstract": "  This paper proposes new nonparametric diagnostic tools to assess the\nasymptotic validity of different treatment effects estimators that rely on the\ncorrect specification of the propensity score. We derive a particular\nrestriction relating the propensity score distribution of treated and control\ngroups, and develop specification tests based upon it. The resulting tests do\nnot suffer from the \"curse of dimensionality\" when the vector of covariates is\nhigh-dimensional, are fully data-driven, do not require tuning parameters such\nas bandwidths, and are able to detect a broad class of local alternatives\nconverging to the null at the parametric rate $n^{-1/2}$, with $n$ the sample\nsize. We show that the use of an orthogonal projection on the tangent space of\nnuisance parameters facilitates the simulation of critical values by means of a\nmultiplier bootstrap procedure, and can lead to power gains. The finite sample\nperformance of the tests is examined by means of a Monte Carlo experiment and\nan empirical application. Open-source software is available for implementing\nthe proposed tests.\n", "category": [6, 7]}
{"abstract": "  Hardware based image processing offers speed and convenience not found in\nsoftware-centric approaches. Here, we show theoretically that a two-dimensional\nperiodic array of dipole-coupled elliptical nanomagnets, delineated on a\npiezoelectric substrate, can act as a dynamical system for specific image\nprocessing functions. Each nanomagnet has two stable magnetization states that\nencode pixel color (black or white). An image containing black and white pixels\nis first converted to voltage states and then mapped into the magnetization\nstates of a nanomagnet array with magneto-tunneling junctions (MTJs). The same\nMTJs are employed to read out the processed pixel colors later. Dipole\ninteraction between the nanomagnets implements specific image processing tasks\nsuch as noise reduction and edge enhancement detection. These functions are\ntriggered by applying a global strain to the nanomagnets with a voltage dropped\nacross the piezoelectric substrate. An image containing an arbitrary number of\nblack and white pixels can be processed in few nanoseconds with very low energy\ncost.\n", "category": [3, 3, 0, 1, 1]}
{"abstract": "  We consider inference about coefficients on a small number of variables of\ninterest in a linear panel data model with additive unobserved individual and\ntime specific effects and a large number of additional time-varying confounding\nvariables. We allow the number of these additional confounding variables to be\nlarger than the sample size, and suppose that, in addition to unrestricted time\nand individual specific effects, these confounding variables are generated by a\nsmall number of common factors and high-dimensional weakly-dependent\ndisturbances. We allow that both the factors and the disturbances are related\nto the outcome variable and other variables of interest. To make informative\ninference feasible, we impose that the contribution of the part of the\nconfounding variables not captured by time specific effects, individual\nspecific effects, or the common factors can be captured by a relatively small\nnumber of terms whose identities are unknown. Within this framework, we provide\na convenient computational algorithm based on factor extraction followed by\nlasso regression for inference about parameters of interest and show that the\nresulting procedure has good asymptotic properties. We also provide a simple\nk-step bootstrap procedure that may be used to construct inferential statements\nabout parameters of interest and prove its asymptotic validity. The proposed\nbootstrap may be of substantive independent interest outside of the present\ncontext as the proposed bootstrap may readily be adapted to other contexts\ninvolving inference after lasso variable selection and the proof of its\nvalidity requires some new technical arguments. We also provide simulation\nevidence about performance of our procedure and illustrate its use in two\nempirical applications.\n", "category": [6, 7]}
{"abstract": "  Passive microseismic data are commonly buried in noise, which presents a\nsignificant challenge for signal detection and recovery. For recordings from a\nsurface sensor array where each trace contains a time-delayed arrival from the\nevent, we propose an autocorrelation-based stacking method that designs a\ndenoising filter from all the traces, as well as a multi-channel detection\nscheme. This approach circumvents the issue of time aligning the traces prior\nto stacking because every trace's autocorrelation is centered at zero in the\nlag domain. The effect of white noise is concentrated near zero lag, so the\nfilter design requires a predictable adjustment of the zero-lag value.\nTruncation of the autocorrelation is employed to smooth the impulse response of\nthe denoising filter. In order to extend the applicability of the algorithm, we\nalso propose a noise prewhitening scheme that addresses cases with colored\nnoise. The simplicity and robustness of this method are validated with\nsynthetic and real seismic traces.\n", "category": [3, 0, 1]}
{"abstract": "  This article proposes different tests for treatment effect heterogeneity when\nthe outcome of interest, typically a duration variable, may be right-censored.\nThe proposed tests study whether a policy 1) has zero distributional (average)\neffect for all subpopulations defined by covariate values, and 2) has\nhomogeneous average effect across different subpopulations. The proposed tests\nare based on two-step Kaplan-Meier integrals and do not rely on parametric\ndistributional assumptions, shape restrictions, or on restricting the potential\ntreatment effect heterogeneity across different subpopulations. Our framework\nis suitable not only to exogenous treatment allocation but can also account for\ntreatment noncompliance - an important feature in many applications. The\nproposed tests are consistent against fixed alternatives, and can detect\nnonparametric alternatives converging to the null at the parametric\n$n^{-1/2}$-rate, $n$ being the sample size. Critical values are computed with\nthe assistance of a multiplier bootstrap. The finite sample properties of the\nproposed tests are examined by means of a Monte Carlo study and an application\nabout the effect of labor market programs on unemployment duration. Open-source\nsoftware is available for implementing all proposed tests.\n", "category": [6, 7]}
{"abstract": "  This paper presents the design of a supervisory algorithm that monitors\nsafety at road intersections and overrides drivers with a safe input when\nnecessary. The design of the supervisor consists of two parts: safety\nverification and control design. Safety verification is the problem to\ndetermine if vehicles will be able to cross the intersection without colliding\nwith current drivers' inputs. We translate this safety verification problem\ninto a jobshop scheduling problem, which minimizes the maximum lateness and\nevaluates if the optimal cost is zero. The zero optimal cost corresponds to the\ncase in which all vehicles can cross each conflict area without collisions.\nComputing the optimal cost requires solving a Mixed Integer Nonlinear\nProgramming (MINLP) problem due to the nonlinear second-order dynamics of the\nvehicles. We therefore estimate this optimal cost by formulating two related\nMixed Integer Linear Programming (MILP) problems that assume simpler vehicle\ndynamics. We prove that these two MILP problems yield lower and upper bounds of\nthe optimal cost. We also quantify the worst case approximation errors of these\nMILP problems. We design the supervisor to override the vehicles with a safe\ncontrol input if the MILP problem that computes the upper bound yields a\npositive optimal cost. We theoretically demonstrate that the supervisor keeps\nthe intersection safe and is non-blocking. Computer simulations further\nvalidate that the algorithms can run in real time for problems of realistic\nsize.\n", "category": [2, 0, 0, 1]}
{"abstract": "  Graph-based methods pervade the inference toolkits of numerous disciplines\nincluding sociology, biology, neuroscience, physics, chemistry, and\nengineering. A challenging problem encountered in this context pertains to\ndetermining the attributes of a set of vertices given those of another subset\nat possibly different time instants. Leveraging spatiotemporal dynamics can\ndrastically reduce the number of observed vertices, and hence the cost of\nsampling. Alleviating the limited flexibility of existing approaches, the\npresent paper broadens the existing kernel-based graph function reconstruction\nframework to accommodate time-evolving functions over possibly time-evolving\ntopologies. This approach inherits the versatility and generality of\nkernel-based methods, for which no knowledge on distributions or second-order\nstatistics is required. Systematic guidelines are provided to construct two\nfamilies of space-time kernels with complementary strengths. The first\nfacilitates judicious control of regularization on a space-time frequency\nplane, whereas the second can afford time-varying topologies. Batch and online\nestimators are also put forth, and a novel kernel Kalman filter is developed to\nobtain these estimates at affordable computational cost. Numerical tests with\nreal data sets corroborate the merits of the proposed methods relative to\ncompeting alternatives.\n", "category": [0, 1, 6]}
{"abstract": "  This paper considers maximum likelihood (ML) estimation in a large class of\nmodels with hidden Markov regimes. We investigate consistency of the ML\nestimator and local asymptotic normality for the models under general\nconditions which allow for autoregressive dynamics in the observable process,\nMarkov regime sequences with covariate-dependent transition matrices, and\npossible model misspecification. A Monte Carlo study examines the finite-sample\nproperties of the ML estimator in correctly specified and misspecified models.\nAn empirical application is also discussed.\n", "category": [2, 7, 6]}
{"abstract": "  In this paper the problem of image restoration (denoising and inpainting) is\napproached using sparse approximation of local image blocks. The local image\nblocks are extracted by sliding square windows over the image. An adaptive\nblock size selection procedure for local sparse approximation is proposed,\nwhich affects the global recovery of underlying image. Ideally the adaptive\nlocal block selection yields the minimum mean square error (MMSE) in recovered\nimage. This framework gives us a clustered image based on the selected block\nsize, then each cluster is restored separately using sparse approximation. The\nresults obtained using the proposed framework are very much comparable with the\nrecently proposed image restoration techniques.\n", "category": [0, 0, 1, 1, 6]}
{"abstract": "  Extremal quantile regression, i.e. quantile regression applied to the tails\nof the conditional distribution, counts with an increasing number of economic\nand financial applications such as value-at-risk, production frontiers,\ndeterminants of low infant birth weights, and auction models. This chapter\nprovides an overview of recent developments in the theory and empirics of\nextremal quantile regression. The advances in the theory have relied on the use\nof extreme value approximations to the law of the Koenker and Bassett (1978)\nquantile regression estimator. Extreme value laws not only have been shown to\nprovide more accurate approximations than Gaussian laws at the tails, but also\nhave served as the basis to develop bias corrected estimators and inference\nmethods using simulation and suitable variations of bootstrap and subsampling.\nThe applicability of these methods is illustrated with two empirical examples\non conditional value-at-risk and financial contagion.\n", "category": [6, 7]}
{"abstract": "  The image biomarker standardisation initiative (IBSI) is an independent\ninternational collaboration which works towards standardising the extraction of\nimage biomarkers from acquired imaging for the purpose of high-throughput\nquantitative image analysis (radiomics). Lack of reproducibility and validation\nof high-throughput quantitative image analysis studies is considered to be a\nmajor challenge for the field. Part of this challenge lies in the scantiness of\nconsensus-based guidelines and definitions for the process of translating\nacquired imaging into high-throughput image biomarkers. The IBSI therefore\nseeks to provide image biomarker nomenclature and definitions, benchmark data\nsets, and benchmark values to verify image processing and image biomarker\ncalculations, as well as reporting guidelines, for high-throughput image\nanalysis.\n", "category": [0, 1]}
{"abstract": "  The Fisher Ideal index, developed to measure price inflation, is applied to\ndefine a population-weighted temperature trend. This method has the advantages\nthat the trend is representative for the population distribution throughout the\nsample but without conflating the trend in the population distribution and the\ntrend in the temperature. I show that the trend in the global area-weighted\naverage surface air temperature is different in key details from the\npopulation-weighted trend. I extend the index to include urbanization and the\nurban heat island effect. This substantially changes the trend again. I further\nextend the index to include international migration, but this has a minor\nimpact on the trend.\n", "category": [5, 7, 3, 6]}
{"abstract": "  Most of the previous approaches to lyrics-to-audio alignment used a\npre-developed automatic speech recognition (ASR) system that innately suffered\nfrom several difficulties to adapt the speech model to individual singers. A\nsignificant aspect missing in previous works is the self-learnability of\nrepetitive vowel patterns in the singing voice, where the vowel part used is\nmore consistent than the consonant part. Based on this, our system first learns\na discriminative subspace of vowel sequences, based on weighted symmetric\nnon-negative matrix factorization (WS-NMF), by taking the self-similarity of a\nstandard acoustic feature as an input. Then, we make use of canonical time\nwarping (CTW), derived from a recent computer vision technique, to find an\noptimal spatiotemporal transformation between the text and the acoustic\nsequences. Experiments with Korean and English data sets showed that deploying\nthis method after a pre-developed, unsupervised, singing source separation\nachieved more promising results than other state-of-the-art unsupervised\napproaches and an existing ASR-based system.\n", "category": [0, 0, 0, 0, 1]}
{"abstract": "  We extend the Granger-Johansen representation theorems for I(1) and I(2)\nvector autoregressive processes to accommodate processes that take values in an\narbitrary complex separable Hilbert space. This more general setting is of\ncentral relevance for statistical applications involving functional time\nseries. We first obtain a range of necessary and sufficient conditions for a\npole in the inverse of a holomorphic index-zero Fredholm operator pencil to be\nof first or second order. Those conditions form the basis for our development\nof I(1) and I(2) representations of autoregressive Hilbertian processes.\nCointegrating and attractor subspaces are characterized in terms of the\nbehavior of the autoregressive operator pencil in a neighborhood of one.\n", "category": [2, 7, 6]}
{"abstract": "  Previous results reported in the robotics literature show the relationship\nbetween time-delay control (TDC) and proportional-integral-derivative control\n(PID). In this paper, we show that incremental nonlinear dynamic inversion\n(INDI) - more familiar in the aerospace community - are in fact equivalent to\nTDC. This leads to a meaningful and systematic method for PI(D)-control tuning\nof robust nonlinear flight control systems via INDI. We considered a\nreformulation of the plant dynamics inversion which removes effector blending\nmodels from the resulting control law, resulting in robust model-free control\nlaws like PI(D)-control.\n", "category": [0, 1]}
{"abstract": "  There is a large literature on semiparametric estimation of average treatment\neffects under unconfounded treatment assignment in settings with a fixed number\nof covariates. More recently attention has focused on settings with a large\nnumber of covariates. In this paper we extend lessons from the earlier\nliterature to this new setting. We propose that in addition to reporting point\nestimates and standard errors, researchers report results from a number of\nsupplementary analyses to assist in assessing the credibility of their\nestimates.\n", "category": [6, 7]}
{"abstract": "  Which equilibria will arise in signaling games depends on how the receiver\ninterprets deviations from the path of play. We develop a micro-foundation for\nthese off-path beliefs, and an associated equilibrium refinement, in a model\nwhere equilibrium arises through non-equilibrium learning by populations of\npatient and long-lived senders and receivers. In our model, young senders are\nuncertain about the prevailing distribution of play, so they rationally send\nout-of-equilibrium signals as experiments to learn about the behavior of the\npopulation of receivers. Differences in the payoff functions of the types of\nsenders generate different incentives for these experiments. Using the Gittins\nindex (Gittins, 1979), we characterize which sender types use each signal more\noften, leading to a constraint on the receiver's off-path beliefs based on\n\"type compatibility\" and hence a learning-based equilibrium selection.\n", "category": [5, 7]}
{"abstract": "  This paper presents a new view of multi-user (MU) hybrid massive\nmultiple-input and multiple-output (MIMO) systems from array signal processing\nperspective. We first show that the instantaneous channel vectors corresponding\nto different users are asymptotically orthogonal if the angles of arrival\n(AOAs) of users are different. We then decompose the channel matrix into an\nangle domain basis matrix and a gain matrix. The former can be formulated by\nsteering vectors and the latter has the same size as the number of RF chains,\nwhich perfectly matches the structure of hybrid precoding. A novel hybrid\nchannel estimation is proposed by separately estimating the angle information\nand the gain matrix, which could significantly save the training overhead and\nsubstantially improve the channel estimation accuracy compared to the\nconventional beamspace approach. Moreover, with the aid of the angle domain\nmatrix, the MU massive MIMO system can be viewed as a type of non-orthogonal\nangle division multiple access (ADMA) to simultaneously serve multiple users at\nthe same frequency band. Finally, the performance of the proposed scheme is\nvalidated by computer simulation results.\n", "category": [0, 1, 2]}
{"abstract": "  In many areas, practitioners seek to use observational data to learn a\ntreatment assignment policy that satisfies application-specific constraints,\nsuch as budget, fairness, simplicity, or other functional form constraints. For\nexample, policies may be restricted to take the form of decision trees based on\na limited set of easily observable individual characteristics. We propose a new\napproach to this problem motivated by the theory of semiparametrically\nefficient estimation. Our method can be used to optimize either binary\ntreatments or infinitesimal nudges to continuous treatments, and can leverage\nobservational data where causal effects are identified using a variety of\nstrategies, including selection on observables and instrumental variables.\nGiven a doubly robust estimator of the causal effect of assigning everyone to\ntreatment, we develop an algorithm for choosing whom to treat, and establish\nstrong guarantees for the asymptotic utilitarian regret of the resulting\npolicy.\n", "category": [2, 0, 7, 6, 6]}
{"abstract": "  In the recent years more and more high-dimensional data sets, where the\nnumber of parameters $p$ is high compared to the number of observations $n$ or\neven larger, are available for applied researchers. Boosting algorithms\nrepresent one of the major advances in machine learning and statistics in\nrecent years and are suitable for the analysis of such data sets. While Lasso\nhas been applied very successfully for high-dimensional data sets in Economics,\nboosting has been underutilized in this field, although it has been proven very\npowerful in fields like Biostatistics and Pattern Recognition. We attribute\nthis to missing theoretical results for boosting. The goal of this paper is to\nfill this gap and show that boosting is a competitive method for inference of a\ntreatment effect or instrumental variable (IV) estimation in a high-dimensional\nsetting. First, we present the $L_2$Boosting with componentwise least squares\nalgorithm and variants which are tailored for regression problems which are the\nworkhorse for most Econometric problems. Then we show how $L_2$Boosting can be\nused for estimation of treatment effects and IV estimation. We highlight the\nmethods and illustrate them with simulations and empirical examples. For\nfurther results and technical details we refer to Luo and Spindler (2016, 2017)\nand to the online supplement of the paper.\n", "category": [6, 7, 6]}
{"abstract": "  We formulated and implemented a procedure to generate aliasing-free\nexcitation source signals. It uses a new antialiasing filter in the continuous\ntime domain followed by an IIR digital filter for response equalization. We\nintroduced a cosine-series-based general design procedure for the new\nantialiasing function. We applied this new procedure to implement the\nantialiased Fujisaki-Ljungqvist model. We also applied it to revise our\nprevious implementation of the antialiased Fant-Liljencrants model. A\ncombination of these signals and a lattice implementation of the time varying\nvocal tract model provides a reliable and flexible basis to test fo extractors\nand source aperiodicity analysis methods. MATLAB implementations of these\nantialiased excitation source models are available as part of our open source\ntools for speech science.\n", "category": [1, 0, 1]}
{"abstract": "  Interbank lending and borrowing occur when financial institutions seek to\nsettle and refinance their mutual positions over time and circumstances. This\ninteractive process involves money creation at the aggregate level.\nCoordination mismatch on interbank credit may trigger systemic crises. This\nhappened when, since summer 2007, interbank credit coordination did not longer\nwork smoothly across financial institutions, eventually requiring exceptional\nmonetary policies by central banks, and guarantee and bailout interventions by\ngovernments. Our article develops an interacting heterogeneous agents-based\nmodel of interbank credit coordination under minimal institutions. First, we\nexplore the link between interbank credit coordination and the money generation\nprocess. Contrary to received understanding, interbank credit has the capacity\nto make the monetary system unbound. Second, we develop simulation analysis on\nimperfect interbank credit coordination, studying impact of interbank dynamics\non financial stability and resilience at individual and aggregate levels.\nSystemically destabilizing forces prove to be related to the working of the\nbanking system over time, especially interbank coordination conditions and\ncircumstances.\n", "category": [5, 7, 3, 5, 5]}
{"abstract": "  We study a sequential-learning model featuring a network of naive agents with\nGaussian information structures. Agents apply a heuristic rule to aggregate\npredecessors' actions. They weigh these actions according the strengths of\ntheir social connections to different predecessors. We show this rule arises\nendogenously when agents wrongly believe others act solely on private\ninformation and thus neglect redundancies among observations. We provide a\nsimple linear formula expressing agents' actions in terms of network paths and\nuse this formula to characterize the set of networks where naive agents\neventually learn correctly. This characterization implies that, on all networks\nwhere later agents observe more than one neighbor, there exist\ndisproportionately influential early agents who can cause herding on incorrect\nactions. Going beyond existing social-learning results, we compute the\nprobability of such mislearning exactly. This allows us to compare likelihoods\nof incorrect herding, and hence expected welfare losses, across network\nstructures. The probability of mislearning increases when link densities are\nhigher and when networks are more integrated. In partially segregated networks,\ndivergent early signals can lead to persistent disagreement between groups.\n", "category": [5, 0, 7]}
{"abstract": "  This paper examines the limit properties of information criteria (such as\nAIC, BIC, HQIC) for distinguishing between the unit root model and the various\nkinds of explosive models. The explosive models include the local-to-unit-root\nmodel, the mildly explosive model and the regular explosive model. Initial\nconditions with different order of magnitude are considered. Both the OLS\nestimator and the indirect inference estimator are studied. It is found that\nBIC and HQIC, but not AIC, consistently select the unit root model when data\ncome from the unit root model. When data come from the local-to-unit-root\nmodel, both BIC and HQIC select the wrong model with probability approaching 1\nwhile AIC has a positive probability of selecting the right model in the limit.\nWhen data come from the regular explosive model or from the mildly explosive\nmodel in the form of $1+n^{\\alpha }/n$ with $\\alpha \\in (0,1)$, all three\ninformation criteria consistently select the true model. Indirect inference\nestimation can increase or decrease the probability for information criteria to\nselect the right model asymptotically relative to OLS, depending on the\ninformation criteria and the true model. Simulation results confirm our\nasymptotic results in finite sample.\n", "category": [2, 7, 6]}
{"abstract": "  In this paper we look into the problem of planning over hybrid domains, where\nchange can be both discrete and instantaneous, or continuous over time. In\naddition, it is required that each state on the trajectory induced by the\nexecution of plans complies with a given set of global constraints. We approach\nthe computation of plans for such domains as the problem of searching over a\ndeterministic state model. In this model, some of the successor states are\nobtained by solving numerically the so-called initial value problem over a set\nof ordinary differential equations (ODE) given by the current plan prefix.\nThese equations hold over time intervals whose duration is determined\ndynamically, according to whether zero crossing events take place for a set of\ninvariant conditions. The resulting planner, FS+, incorporates these features\ntogether with effective heuristic guidance. FS+ does not impose any of the\nsyntactic restrictions on process effects often found on the existing\nliterature on Hybrid Planning. A key concept of our approach is that a clear\nseparation is struck between planning and simulation time steps. The former is\nthe time allowed to observe the evolution of a given dynamical system before\ncommitting to a future course of action, whilst the later is part of the model\nof the environment. FS+ is shown to be a robust planner over a diverse set of\nhybrid domains, taken from the existing literature on hybrid planning and\nsystems.\n", "category": [0, 0, 1]}
{"abstract": "  This paper develops the theoretical framework and the equations of a new\nrobust Generalized Maximum-likelihood-type Unscented Kalman Filter (GM-UKF)\nthat is able to suppress observation and innovation outliers while filtering\nout non-Gaussian measurement noise. Because the errors of the real and reactive\npower measurements calculated using Phasor Measurement Units (PMUs) follow\nlong-tailed probability distributions, the conventional UKF provides strongly\nbiased state estimates since it relies on the weighted least squares estimator.\nBy contrast, the state estimates and residuals of our GM-UKF are proved to be\nroughly Gaussian, allowing the sigma points to reliably approximate the mean\nand the covariance matrices of the predicted and corrected state vectors. To\ndevelop our GM-UKF, we first derive a batch-mode regression form by processing\nthe predictions and observations simultaneously, where the statistical\nlinearization approach is used. We show that the set of equations so derived\nare equivalent to those of the unscented transformation. Then, a robust\nGM-estimator that minimizes a convex Huber cost function while using weights\ncalculated via Projection Statistics (PS's) is proposed. The PS's are applied\nto a two-dimensional matrix that consists of serially correlated predicted\nstate and innovation vectors to detect observation and innovation outliers.\nThese outliers are suppressed by the GM-estimator using the iteratively\nreweighted least squares algorithm. Finally, the asymptotic error covariance\nmatrix of the GM-UKF state estimates is derived from the total influence\nfunction. In the companion paper, extensive simulation results will be shown to\nverify the effectiveness and robustness of the proposed method.\n", "category": [2, 0, 1, 6]}
{"abstract": "  Deep learning models (DLMs) are state-of-the-art techniques in speech\nrecognition. However, training good DLMs can be time consuming especially for\nproduction-size models and corpora. Although several parallel training\nalgorithms have been proposed to improve training efficiency, there is no clear\nguidance on which one to choose for the task in hand due to lack of systematic\nand fair comparison among them. In this paper we aim at filling this gap by\ncomparing four popular parallel training algorithms in speech recognition,\nnamely asynchronous stochastic gradient descent (ASGD), blockwise model-update\nfiltering (BMUF), bulk synchronous parallel (BSP) and elastic averaging\nstochastic gradient descent (EASGD), on 1000-hour LibriSpeech corpora using\nfeed-forward deep neural networks (DNNs) and convolutional, long short-term\nmemory, DNNs (CLDNNs). Based on our experiments, we recommend using BMUF as the\ntop choice to train acoustic models since it is most stable, scales well with\nnumber of GPUs, can achieve reproducible results, and in many cases even\noutperforms single-GPU SGD. ASGD can be used as a substitute in some cases.\n", "category": [0, 0, 0, 1]}
{"abstract": "  This paper is the second of a two-part series that discusses the\nimplementation issues and test results of a robust Unscented Kalman Filter\n(UKF) for power system dynamic state estimation with non-Gaussian synchrophasor\nmeasurement noise. The tuning of the parameters of our Generalized\nMaximum-Likelihood-type robust UKF (GM-UKF) is presented and discussed in a\nsystematic way. Using simulations carried out on the IEEE 39-bus system, its\nperformance is evaluated under different scenarios, including i) the occurrence\nof two different types of noises following thick-tailed distributions, namely\nthe Laplace or Cauchy probability distributions for real and reactive power\nmeasurements; ii) the occurrence of observation and innovation outliers; iii)\nthe occurrence of PMU measurement losses due to communication failures; iv)\ncyber attacks; and v) strong system nonlinearities. It is also compared to the\nUKF and the Generalized Maximum-Likelihood-type robust iterated EKF (GM-IEKF).\nSimulation results reveal that the GM-UKF outperforms the GM-IEKF and the UKF\nin all scenarios considered. In particular, when the system is operating under\nstressed conditions, inducing system nonlinearities, the GM-IEKF and the UKF\ndiverge while our GM-UKF does converge. In addition, when the power measurement\nnoises obey a Cauchy distribution, our GM-UKF converges to a state estimate\nvector that exhibits a much higher statistical efficiency than that of the\nGM-IEKF; by contrast, the UKF fails to converge. Finally, potential\napplications and future work of the proposed GM-UKF are discussed in concluding\nremarks section.\n", "category": [0, 1]}
{"abstract": "  In this paper we propose the utterance-level Permutation Invariant Training\n(uPIT) technique. uPIT is a practically applicable, end-to-end, deep learning\nbased solution for speaker independent multi-talker speech separation.\nSpecifically, uPIT extends the recently proposed Permutation Invariant Training\n(PIT) technique with an utterance-level cost function, hence eliminating the\nneed for solving an additional permutation problem during inference, which is\notherwise required by frame-level PIT. We achieve this using Recurrent Neural\nNetworks (RNNs) that, during training, minimize the utterance-level separation\nerror, hence forcing separated frames belonging to the same speaker to be\naligned to the same output stream. In practice, this allows RNNs, trained with\nuPIT, to separate multi-talker mixed speech without any prior knowledge of\nsignal duration, number of speakers, speaker identity or gender. We evaluated\nuPIT on the WSJ0 and Danish two- and three-talker mixed-speech separation tasks\nand found that uPIT outperforms techniques based on Non-negative Matrix\nFactorization (NMF) and Computational Auditory Scene Analysis (CASA), and\ncompares favorably with Deep Clustering (DPCL) and the Deep Attractor Network\n(DANet). Furthermore, we found that models trained with uPIT generalize well to\nunseen speakers and languages. Finally, we found that a single model, trained\nwith uPIT, can handle both two-speaker, and three-speaker speech mixtures.\n", "category": [0, 0, 1]}
{"abstract": "  In this paper, a concurrent learning based adaptive observer is developed for\na class of second-order nonlinear time-invariant systems with uncertain\ndynamics. The developed technique results in uniformly ultimately bounded state\nand parameter estimation errors. As opposed to persistent excitation which is\nrequired for parameter convergence in traditional adaptive control methods, the\ndeveloped technique only requires excitation over a finite time interval to\nachieve parameter convergence. Simulation results in both noise-free and noisy\nenvironments are presented to validate the design.\n", "category": [1, 0]}
{"abstract": "  In millimeter-wave channels, most of the received energy is carried by a few\npaths. Traditional precoders sweep the angle-of-departure (AoD) and\nangle-of-arrival (AoA) space with directional precoders to identify directions\nwith largest power. Such precoders are heuristic and lead to sub-optimal\nAoD/AoA estimation. We derive optimal precoders, minimizing the Cram\\'{e}r-Rao\nbound (CRB) of the AoD/AoA, assuming a fully digital architecture at the\ntransmitter and spatial filtering of a single path. The precoders are found by\nsolving a suitable convex optimization problem. We demonstrate that the\naccuracy can be improved by at least a factor of two over traditional\nprecoders, and show that there is an optimal number of distinct precoders\nbeyond which the CRB does not improve.\n", "category": [0, 1, 2]}
{"abstract": "  The random coefficients model is an extension of the linear regression model\nthat allows for unobserved heterogeneity in the population by modeling the\nregression coefficients as random variables. Given data from this model, the\nstatistical challenge is to recover information about the joint density of the\nrandom coefficients which is a multivariate and ill-posed problem. Because of\nthe curse of dimensionality and the ill-posedness, pointwise nonparametric\nestimation of the joint density is difficult and suffers from slow convergence\nrates. Larger features, such as an increase of the density along some direction\nor a well-accentuated mode can, however, be much easier detected from data by\nmeans of statistical tests. In this article, we follow this strategy and\nconstruct tests and confidence statements for qualitative features of the joint\ndensity, such as increases, decreases and modes. We propose a multiple testing\napproach based on aggregating single tests which are designed to extract shape\ninformation on fixed scales and directions. Using recent tools for Gaussian\napproximations of multivariate empirical processes, we derive expressions for\nthe critical value. We apply our method to simulated and real data.\n", "category": [6, 7]}
{"abstract": "  In this paper, we propose a novel technique for direct recognition of\nmultiple speech streams given the single channel of mixed speech, without first\nseparating them. Our technique is based on permutation invariant training (PIT)\nfor automatic speech recognition (ASR). In PIT-ASR, we compute the average\ncross entropy (CE) over all frames in the whole utterance for each possible\noutput-target assignment, pick the one with the minimum CE, and optimize for\nthat assignment. PIT-ASR forces all the frames of the same speaker to be\naligned with the same output layer. This strategy elegantly solves the label\npermutation problem and speaker tracing problem in one shot. Our experiments on\nartificially mixed AMI data showed that the proposed approach is very\npromising.\n", "category": [0, 0, 1]}
{"abstract": "  This paper is an axiomatic study of consistent approval-based multi-winner\nrules, i.e., voting rules that select a fixed-size group of candidates based on\napproval ballots. We introduce the class of counting rules and provide an\naxiomatic characterization of this class based on the consistency axiom.\nBuilding upon this result, we axiomatically characterize three important\nconsistent multi-winner rules: Proportional Approval Voting, Multi-Winner\nApproval Voting and the Approval Chamberlin--Courant rule. Our results\ndemonstrate the variety of multi-winner rules and illustrate three different,\northogonal principles that multi-winner voting rules may represent: individual\nexcellence, diversity, and proportionality.\n", "category": [0, 0, 0, 7]}
{"abstract": "  We develop a Bayesian vector autoregressive (VAR) model with multivariate\nstochastic volatility that is capable of handling vast dimensional information\nsets. Three features are introduced to permit reliable estimation of the model.\nFirst, we assume that the reduced-form errors in the VAR feature a factor\nstochastic volatility structure, allowing for conditional equation-by-equation\nestimation. Second, we apply recently developed global-local shrinkage priors\nto the VAR coefficients to cure the curse of dimensionality. Third, we utilize\nrecent innovations to efficiently sample from high-dimensional multivariate\nGaussian distributions. This makes simulation-based fully Bayesian inference\nfeasible when the dimensionality is large but the time series length is\nmoderate. We demonstrate the merits of our approach in an extensive simulation\nstudy and apply the model to US macroeconomic data to evaluate its forecasting\ncapabilities.\n", "category": [6, 7, 6, 6]}
{"abstract": "  In this paper, we provide for the first time an automated,\ncorrect-by-construction, controller synthesis scheme for a class of infinite\ndimensional stochastic systems, namely, retarded jump-diffusion systems. First,\nwe construct finite abstractions approximately bisimilar to non-probabilistic\nretarded systems corresponding to the original systems having some stability\nproperty, namely, incremental input-to-state stability. Then, we provide a\nresult on quantifying the distance between output trajectory of the obtained\nfinite abstraction and that of the original retarded jump-diffusion system in a\nprobabilistic setting. Using the proposed result, one can refine the control\npolicy synthesized using finite abstractions to the original systems while\nproviding guarantee on the probability of satisfaction of high-level\nrequirements. Moreover, we provide sufficient conditions for the proposed\nnotion of incremental stability in terms of the existence of incremental\nLyapunov functions which reduce to some matrix inequalities for the linear\nsystems. Finally, the effectiveness of the proposed results is illustrated by\nsynthesizing a controller regulating the temperatures in a ten-room building\nmodelled as a delayed jump-diffusion system.\n", "category": [2, 0, 1]}
{"abstract": "  Extracting per-frame features using convolutional neural networks for\nreal-time processing of video data is currently mainly performed on powerful\nGPU-accelerated workstations and compute clusters. However, there are many\napplications such as smart surveillance cameras that require or would benefit\nfrom on-site processing. To this end, we propose and evaluate a novel algorithm\nfor change-based evaluation of CNNs for video data recorded with a static\ncamera setting, exploiting the spatio-temporal sparsity of pixel changes. We\nachieve an average speed-up of 8.6x over a cuDNN baseline on a realistic\nbenchmark with a negligible accuracy loss of less than 0.1% and no retraining\nof the network. The resulting energy efficiency is 10x higher than that of\nper-frame evaluation and reaches an equivalent of 328 GOp/s/W on the Tegra X1\nplatform.\n", "category": [0, 0, 0, 0, 1]}
{"abstract": "  Appropriate selection of the penalty parameter is crucial to obtaining good\nperformance from the Alternating Direction Method of Multipliers (ADMM). While\nanalytic results for optimal selection of this parameter are very limited,\nthere is a heuristic method that appears to be relatively successful in a\nnumber of different problems. The contribution of this paper is to demonstrate\nthat their is a potentially serious flaw in this heuristic approach, and to\npropose a modification that at least partially addresses it.\n", "category": [2, 0, 1]}
{"abstract": "  In this paper the focus is on subsampling as well as reconstructing the\nsecond-order statistics of signals residing on nodes of arbitrary undirected\ngraphs. Second-order stationary graph signals may be obtained by graph\nfiltering zero-mean white noise and they admit a well-defined power spectrum\nwhose shape is determined by the frequency response of the graph filter.\nEstimating the graph power spectrum forms an important component of stationary\ngraph signal processing and related inference tasks such as Wiener prediction\nor inpainting on graphs. The central result of this paper is that by sampling a\nsignificantly smaller subset of vertices and using simple least squares, we can\nreconstruct the second-order statistics of the graph signal from the subsampled\nobservations, and more importantly, without any spectral priors. To this end,\nboth a nonparametric approach as well as parametric approaches including moving\naverage and autoregressive models for the graph power spectrum are considered.\nThe results specialize for undirected circulant graphs in that the graph nodes\nleading to the best compression rates are given by the so-called minimal sparse\nrulers. A near-optimal greedy algorithm is developed to design the subsampling\nscheme for the non-parametric and the moving average models, whereas a\nparticular subsampling scheme that allows linear estimation for the\nautoregressive model is proposed. Numerical experiments on synthetic as well as\nreal datasets related to climatology and processing handwritten digits are\nprovided to demonstrate the developed theory.\n", "category": [0, 1, 2]}
{"abstract": "  This paper proposes a valid bootstrap-based distributional approximation for\nM-estimators exhibiting a Chernoff (1964)-type limiting distribution. For\nestimators of this kind, the standard nonparametric bootstrap is inconsistent.\nThe method proposed herein is based on the nonparametric bootstrap, but\nrestores consistency by altering the shape of the criterion function defining\nthe estimator whose distribution we seek to approximate. This modification\nleads to a generic and easy-to-implement resampling method for inference that\nis conceptually distinct from other available distributional approximations. We\nillustrate the applicability of our results with four examples in econometrics\nand machine learning.\n", "category": [2, 7, 6, 6]}
{"abstract": "  In competitive sports it is often very hard to quantify the performance. A\nplayer to score or overtake may depend on only millesimal of seconds or\nmillimeters. In racquet sports like tennis, table tennis and squash many events\nwill occur in a short time duration, whose recording and analysis can help\nreveal the differences in performance. In this paper we show that it is\npossible to architect a framework that utilizes the characteristic sound\npatterns to precisely classify the types of and localize the positions of these\nevents. From these basic information the shot types and the ball speed along\nthe trajectories can be estimated. Comparing these estimates with the optimal\nspeed and target the precision of the shot can be defined. The detailed shot\nstatistics and precision information significantly enriches and improves data\navailable today. Feeding them back to the players and the coaches facilitates\nto describe playing performance objectively and to improve strategy skills. The\nframework is implemented, its hardware and software components are installed\nand tested in a squash court.\n", "category": [1, 0]}
{"abstract": "  This paper uses model symmetries in the instrumental variable (IV) regression\nto derive an invariant test for the causal structural parameter. Contrary to\npopular belief, we show that there exist model symmetries when equation errors\nare heteroskedastic and autocorrelated (HAC). Our theory is consistent with\nexisting results for the homoskedastic model (Andrews, Moreira, and Stock\n(2006) and Chamberlain (2007)). We use these symmetries to propose the\nconditional integrated likelihood (CIL) test for the causality parameter in the\nover-identified model. Theoretical and numerical findings show that the CIL\ntest performs well compared to other tests in terms of power and\nimplementation. We recommend that practitioners use the Anderson-Rubin (AR)\ntest in the just-identified model, and the CIL test in the over-identified\nmodel.\n", "category": [2, 7, 6]}
{"abstract": "  We consider the problem of distributed estimation of a Gaussian vector with\nlinear observation model. Each sensor makes a scalar noisy observation of the\nunknown vector, quantizes its observation, maps it to a digitally modulated\nsymbol, and transmits the symbol over orthogonal power-constrained fading\nchannels to a fusion center (FC). The FC is tasked with fusing the received\nsignals from sensors and estimating the unknown vector. We derive the Bayesian\nFisher Information Matrix (FIM) for three types of receivers: (i) coherent\nreceiver (ii) noncoherent receiver with known channel envelopes (iii)\nnoncoherent receiver with known channel statistics only. We also derive the\nWeiss-Weinstein bound (WWB). We formulate two constrained optimization\nproblems, namely maximizing trace and log-determinant of Bayesian FIM under\nnetwork transmit power constraint, with sensors transmit powers being the\noptimization variables. We show that for coherent receiver, these problems are\nconcave. However, for noncoherent receivers, they are not necessarily concave.\nThe solution to the trace of Bayesian FIM maximization problem can be\nimplemented in a distributed fashion. We numerically investigate how the\nFIM-max power allocation across sensors depends on the sensors observation\nqualities and physical layer parameters as well as the network transmit power\nconstraint. Moreover, we evaluate the system performance in terms of MSE using\nthe solutions of FIM-max schemes, and compare it with the solution obtained\nfrom minimizing the MSE of the LMMSE estimator (MSE-min scheme), and that of\nuniform power allocation. These comparisons illustrate that, although the WWB\nis tighter than the inverse of Bayesian FIM, it is still suitable to use\nFIM-max schemes, since the performance loss in terms of the MSE of the LMMSE\nestimator is not significant.\n", "category": [0, 1, 2]}
{"abstract": "  In this paper, we will provide a comparison between uniform and random\nsampling for speech and music signals. There are various sampling and recovery\nmethods for audio signals. Here, we only investigate uniform and random schemes\nfor sampling and basic low-pass filtering and iterative method with adaptive\nthresholding for recovery. The simulation results indicate that uniform\nsampling with cubic spline interpolation outperforms other sampling and\nrecovery methods.\n", "category": [1, 0, 0]}
{"abstract": "  It is common to assume in empirical research that observables and\nunobservables are additively separable, especially, when the former are\nendogenous. This is done because it is widely recognized that identification\nand estimation challenges arise when interactions between the two are allowed\nfor. Starting from a nonseparable IV model, where the instrumental variable is\nindependent of unobservables, we develop a novel nonparametric test of\nseparability of unobservables. The large-sample distribution of the test\nstatistics is nonstandard and relies on a novel Donsker-type central limit\ntheorem for the empirical distribution of nonparametric IV residuals, which may\nbe of independent interest. Using a dataset drawn from the 2015 US Consumer\nExpenditure Survey, we find that the test rejects the separability in Engel\ncurves for most of the commodities.\n", "category": [2, 7, 6, 6, 6]}
{"abstract": "  Polar codes were recently chosen to protect the control channel information\nin the next-generation mobile communication standard (5G) defined by the 3GPP.\nAs a result, receivers will have to implement blind detection of polar coded\nframes in order to keep complexity, latency, and power consumption tractable.\nAs a newly proposed class of block codes, the problem of polar-code blind\ndetection has received very little attention. In this work, we propose a\nlow-complexity blind-detection algorithm for polar-encoded frames. We base this\nalgorithm on a novel detection metric with update rules that leverage the a\npriori knowledge of the frozen-bit locations, exploiting the inherent\nstructures that these locations impose on a polar-encoded block of data. We\nshow that the proposed detection metric allows to clearly distinguish\npolar-encoded frames from other types of data by considering the cumulative\ndistribution functions of the detection metric, and the receiver operating\ncharacteristic. The presented results are tailored to the 5G standardization\neffort discussions, i.e., we consider a short low-rate polar code concatenated\nwith a CRC.\n", "category": [0, 1, 2]}
{"abstract": "  Massive multi-user (MU) multiple-input multiple-output (MIMO) promises\nsignificant gains in spectral efficiency compared to traditional, small-scale\nMIMO technology. Linear equalization algorithms, such as zero forcing (ZF) or\nminimum mean-square error (MMSE)-based methods, typically rely on centralized\nprocessing at the base station (BS), which results in (i) excessively high\ninterconnect and chip input/output data rates, and (ii) high computational\ncomplexity. In this paper, we investigate the achievable rates of decentralized\nequalization that mitigates both of these issues. We consider two distinct BS\narchitectures that partition the antenna array into clusters, each associated\nwith independent radio-frequency chains and signal processing hardware, and the\nresults of each cluster are fused in a feedforward network. For both\narchitectures, we consider ZF, MMSE, and a novel, non-linear equalization\nalgorithm that builds upon approximate message passing (AMP), and we\ntheoretically analyze the achievable rates of these methods. Our results\ndemonstrate that decentralized equalization with our AMP-based methods incurs\nno or only a negligible loss in terms of achievable rates compared to that of\ncentralized solutions.\n", "category": [0, 1, 2]}
{"abstract": "  This paper deals with linear equalization in massive multi-user\nmultiple-input multiple-output (MU-MIMO) wireless systems. We first provide\nsimple conditions on the antenna configuration for which the well-known linear\nminimum mean-square error (L-MMSE) equalizer provides near-optimal spectral\nefficiency, and we analyze its performance in the presence of parameter\nmismatches in the signal and/or noise powers. We then propose a novel,\noptimally-tuned NOnParametric Equalizer (NOPE) for massive MU-MIMO systems,\nwhich avoids knowledge of the transmit signal and noise powers altogether. We\nshow that NOPE achieves the same performance as that of the L-MMSE equalizer in\nthe large-antenna limit, and we demonstrate its efficacy in realistic,\nfinite-dimensional systems. From a practical perspective, NOPE is\ncomputationally efficient and avoids dedicated training that is typically\nrequired for parameter estimation\n", "category": [0, 1, 2]}
{"abstract": "  Policy iteration (PI) is a recursive process of policy evaluation and\nimprovement for solving an optimal decision-making/control problem, or in other\nwords, a reinforcement learning (RL) problem. PI has also served as the\nfundamental for developing RL methods. In this paper, we propose two PI\nmethods, called differential PI (DPI) and integral PI (IPI), and their\nvariants, for a general RL framework in continuous time and space (CTS), where\nthe environment is modeled by a system of ordinary differential equations\n(ODEs). The proposed methods inherit the current ideas of PI in classical RL\nand optimal control and theoretically support the existing RL algorithms in\nCTS: TD-learning and value-gradient-based (VGB) greedy policy update. We also\nprovide case studies including 1) discounted RL and 2) optimal control tasks.\nFundamental mathematical properties -- admissibility, uniqueness of the\nsolution to the Bellman equation (BE), monotone improvement, convergence, and\noptimality of the solution to the Hamilton-Jacobi-Bellman equation (HJBE) --\nare all investigated in-depth and improved from the existing theory, along with\nthe general and case studies. Finally, the proposed ones are simulated with an\ninverted-pendulum model and their model-based and partially model-free\nimplementations to support the theory and further investigate them beyond.\n", "category": [0, 0, 0, 1]}
{"abstract": "  The seminal work of Gatys et al. demonstrated the power of Convolutional\nNeural Networks (CNNs) in creating artistic imagery by separating and\nrecombining image content and style. This process of using CNNs to render a\ncontent image in different styles is referred to as Neural Style Transfer\n(NST). Since then, NST has become a trending topic both in academic literature\nand industrial applications. It is receiving increasing attention and a variety\nof approaches are proposed to either improve or extend the original NST\nalgorithm. In this paper, we aim to provide a comprehensive overview of the\ncurrent progress towards NST. We first propose a taxonomy of current algorithms\nin the field of NST. Then, we present several evaluation methods and compare\ndifferent NST algorithms both qualitatively and quantitatively. The review\nconcludes with a discussion of various applications of NST and open problems\nfor future research. A list of papers discussed in this review, corresponding\ncodes, pre-trained models and more comparison results are publicly available at\nhttps://github.com/ycjing/Neural-Style-Transfer-Papers.\n", "category": [0, 0, 1, 6]}
{"abstract": "  While convolutional sparse representations enjoy a number of useful\nproperties, they have received limited attention for image reconstruction\nproblems. The present paper compares the performance of block-based and\nconvolutional sparse representations in the removal of Gaussian white noise.\nWhile the usual formulation of the convolutional sparse coding problem is\nslightly inferior to the block-based representations in this problem, the\nperformance of the convolutional form can be boosted beyond that of the\nblock-based form by the inclusion of suitable penalties on the gradients of the\ncoefficient maps.\n", "category": [0, 1]}
{"abstract": "  Given a set of baseline assumptions, a breakdown frontier is the boundary\nbetween the set of assumptions which lead to a specific conclusion and those\nwhich do not. In a potential outcomes model with a binary treatment, we\nconsider two conclusions: First, that ATE is at least a specific value (e.g.,\nnonnegative) and second that the proportion of units who benefit from treatment\nis at least a specific value (e.g., at least 50\\%). For these conclusions, we\nderive the breakdown frontier for two kinds of assumptions: one which indexes\nrelaxations of the baseline random assignment of treatment assumption, and one\nwhich indexes relaxations of the baseline rank invariance assumption. These\nclasses of assumptions nest both the point identifying assumptions of random\nassignment and rank invariance and the opposite end of no constraints on\ntreatment selection or the dependence structure between potential outcomes.\nThis frontier provides a quantitative measure of robustness of conclusions to\nrelaxations of the baseline point identifying assumptions. We derive\n$\\sqrt{N}$-consistent sample analog estimators for these frontiers. We then\nprovide two asymptotically valid bootstrap procedures for constructing lower\nuniform confidence bands for the breakdown frontier. As a measure of\nrobustness, estimated breakdown frontiers and their corresponding confidence\nbands can be presented alongside traditional point estimates and confidence\nintervals obtained under point identifying assumptions. We illustrate this\napproach in an empirical application to the effect of child soldiering on\nwages. We find that sufficiently weak conclusions are robust to simultaneous\nfailures of rank invariance and random assignment, while some stronger\nconclusions are fairly robust to failures of rank invariance but not\nnecessarily to relaxations of random assignment.\n", "category": [6, 7]}
{"abstract": "  A short overview demystifying the midi audio format is presented. The goal is\nto explain the file structure and how the instructions are used to produce a\nmusic signal, both in the case of monophonic signals as for polyphonic signals.\n", "category": [0, 1]}
{"abstract": "  We present a simple continuous-time model of clearing in financial networks.\nFinancial firms are represented as \"tanks\" filled with fluid (money), flowing\nin and out. Once \"pipes\" connecting \"tanks\" are open, the system reaches the\nclearing payment vector in finite time. This approach provides a simple\nrecursive solution to a classical static model of financial clearing in\nbankruptcy, and suggests a practical payment mechanism. With sufficient\nresources, a system of mutual obligations can be restructured into an\nequivalent system that has a cascade structure: there is a group of banks that\npaid off their debts, another group that owes money only to banks in the first\ngroup, and so on. Technically, we use the machinery of Markov chains to analyze\nevolution of a deterministic dynamical system.\n", "category": [7, 5]}
{"abstract": "  A number of recent works have proposed to solve the line spectral estimation\nproblem by applying off-the-grid extensions of sparse estimation techniques.\nThese methods are preferable over classical line spectral estimation algorithms\nbecause they inherently estimate the model order. However, they all have\ncomputation times which grow at least cubically in the problem size, thus\nlimiting their practical applicability in cases with large dimensions. To\nalleviate this issue, we propose a low-complexity method for line spectral\nestimation, which also draws on ideas from sparse estimation. Our method is\nbased on a Bayesian view of the problem. The signal covariance matrix is shown\nto have Toeplitz structure, allowing superfast Toeplitz inversion to be used.\nWe demonstrate that our method achieves estimation accuracy at least as good as\ncurrent methods and that it does so while being orders of magnitudes faster.\n", "category": [1, 0, 2, 6]}
{"abstract": "  For the past couple of decades, numerical optimization has played a central\nrole in addressing wireless resource management problems such as power control\nand beamformer design. However, optimization algorithms often entail\nconsiderable complexity, which creates a serious gap between theoretical\ndesign/analysis and real-time processing. To address this challenge, we propose\na new learning-based approach. The key idea is to treat the input and output of\na resource allocation algorithm as an unknown non-linear mapping and use a deep\nneural network (DNN) to approximate it. If the non-linear mapping can be\nlearned accurately by a DNN of moderate size, then resource allocation can be\ndone in almost real time -- since passing the input through a DNN only requires\na small number of simple operations.\n  In this work, we address both the thereotical and practical aspects of\nDNN-based algorithm approximation with applications to wireless resource\nmanagement. We first pin down a class of optimization algorithms that are\n`learnable' in theory by a fully connected DNN. Then, we focus on DNN-based\napproximation to a popular power allocation algorithm named WMMSE (Shi {\\it et\nal} 2011). We show that using a DNN to approximate WMMSE can be fairly accurate\n-- the approximation error $\\epsilon$ depends mildly [in the order of\n$\\log(1/\\epsilon)$] on the numbers of neurons and layers of the DNN. On the\nimplementation side, we use extensive numerical simulations to demonstrate that\nDNNs can achieve orders of magnitude speedup in computational time compared to\nstate-of-the-art power allocation algorithms based on optimization.\n", "category": [0, 1, 2]}
{"abstract": "  In treatment allocation problems the individuals to be treated often arrive\nsequentially. We study a problem in which the policy maker is not only\ninterested in the expected cumulative welfare but is also concerned about the\nuncertainty/risk of the treatment outcomes. At the outset, the total number of\ntreatment assignments to be made may even be unknown. A sequential treatment\npolicy which attains the minimax optimal regret is proposed. We also\ndemonstrate that the expected number of suboptimal treatments only grows slowly\nin the number of treatments. Finally, we study a setting where outcomes are\nonly observed with delay.\n", "category": [6, 7]}
{"abstract": "  Koopman operator is a composition operator defined for a dynamical system\ndescribed by nonlinear differential or difference equation. Although the\noriginal system is nonlinear and evolves on a finite-dimensional state space,\nthe Koopman operator itself is linear but infinite-dimensional (evolves on a\nfunction space). This linear operator captures the full information of the\ndynamics described by the original nonlinear system. In particular, spectral\nproperties of the Koopman operator play a crucial role in analyzing the\noriginal system. In the first part of this paper, we review the so-called\nKoopman operator theory for nonlinear dynamical systems, with emphasis on modal\ndecomposition and computation that are direct to wide applications. Then, in\nthe second part, we present a series of applications of the Koopman operator\ntheory to power systems technology. The applications are established as\ndata-centric methods, namely, how to use massive quantities of data obtained\nnumerically and experimentally, through spectral analysis of the Koopman\noperator: coherency identification of swings in coupled synchronous generators,\nprecursor diagnostic of instabilities in the coupled swing dynamics, and\nstability assessment of power systems without any use of mathematical models.\nFuture problems of this research direction are identified in the last\nconcluding part of this paper.\n", "category": [0, 1, 2, 3]}
{"abstract": "  Consider a researcher estimating the parameters of a regression function\nbased on data for all 50 states in the United States or on data for all visits\nto a website. What is the interpretation of the estimated parameters and the\nstandard errors? In practice, researchers typically assume that the sample is\nrandomly drawn from a large population of interest and report standard errors\nthat are designed to capture sampling variation. This is common even in\napplications where it is difficult to articulate what that population of\ninterest is, and how it differs from the sample. In this article, we explore an\nalternative approach to inference, which is partly design-based. In a\ndesign-based setting, the values of some of the regressors can be manipulated,\nperhaps through a policy intervention. Design-based uncertainty emanates from\nlack of knowledge about the values that the regression outcome would have taken\nunder alternative interventions. We derive standard errors that account for\ndesign-based uncertainty instead of, or in addition to, sampling-based\nuncertainty. We show that our standard errors in general are smaller than the\nusual infinite-population sampling-based standard errors and provide conditions\nunder which they coincide.\n", "category": [2, 7, 6]}
{"abstract": "  The Machina thought experiments pose to major non-expected utility models\nchallenges that are similar to those posed by the Ellsberg thought experiments\nto subjective expected utility theory (SEUT). We test human choices in the\n`Ellsberg three-color example', confirming typical ambiguity aversion patterns,\nand the `Machina 50/51 and reflection examples', partially confirming the\npreferences hypothesized by Machina. Then, we show that a quantum-theoretic\nframework for decision-making under uncertainty recently elaborated by some of\nus allows faithful modeling of all data on the Ellsberg and Machina paradox\nsituations. In the quantum-theoretic framework subjective probabilities are\nrepresented by quantum probabilities, while quantum state transformations\nenable representations of ambiguity aversion and subjective attitudes toward\nit.\n", "category": [5, 7, 5]}
{"abstract": "  We introduce a simple and linear SNR (strictly speaking, periodic to random\npower ratio) estimator (0dB to 80dB without additional\ncalibration/linearization) for providing reliable descriptions of aperiodicity\nin speech corpus. The main idea of this method is to estimate the background\nrandom noise level without directly extracting the background noise. The\nproposed method is applicable to a wide variety of time windowing functions\nwith very low sidelobe levels. The estimate combines the frequency derivative\nand the time-frequency derivative of the mapping from filter center frequency\nto the output instantaneous frequency. This procedure can replace the\nperiodicity detection and aperiodicity estimation subsystems of recently\nintroduced open source vocoder, YANG vocoder. Source code of MATLAB\nimplementation of this method will also be open sourced.\n", "category": [1, 0, 1]}
{"abstract": "  Recently, impressive denoising results have been achieved by Bayesian\napproaches which assume Gaussian models for the image patches. This improvement\nin performance can be attributed to the use of per-patch models. Unfortunately\nsuch an approach is particularly unstable for most inverse problems beyond\ndenoising. In this work, we propose the use of a hyperprior to model image\npatches, in order to stabilize the estimation procedure. There are two main\nadvantages to the proposed restoration scheme: Firstly it is adapted to\ndiagonal degradation matrices, and in particular to missing data problems (e.g.\ninpainting of missing pixels or zooming). Secondly it can deal with signal\ndependent noise models, particularly suited to digital cameras. As such, the\nscheme is especially adapted to computational photography. In order to\nillustrate this point, we provide an application to high dynamic range imaging\nfrom a single image taken with a modified sensor, which shows the effectiveness\nof the proposed scheme.\n", "category": [0, 1, 6]}
{"abstract": "  Bayesian inference for stochastic volatility models using MCMC methods highly\ndepends on actual parameter values in terms of sampling efficiency. While draws\nfrom the posterior utilizing the standard centered parameterization break down\nwhen the volatility of volatility parameter in the latent state equation is\nsmall, non-centered versions of the model show deficiencies for highly\npersistent latent variable series. The novel approach of\nancillarity-sufficiency interweaving has recently been shown to aid in\novercoming these issues for a broad class of multilevel models. In this paper,\nwe demonstrate how such an interweaving strategy can be applied to stochastic\nvolatility models in order to greatly improve sampling efficiency for all\nparameters and throughout the entire parameter range. Moreover, this method of\n\"combining best of different worlds\" allows for inference for parameter\nconstellations that have previously been infeasible to estimate without the\nneed to select a particular parameterization beforehand.\n", "category": [6, 7, 6, 6]}
{"abstract": "  Structural econometric methods are often criticized for being sensitive to\nfunctional form assumptions. We study parametric estimators of the local\naverage treatment effect (LATE) derived from a widely used class of latent\nthreshold crossing models and show they yield LATE estimates algebraically\nequivalent to the instrumental variables (IV) estimator. Our leading example is\nHeckman's (1979) two-step (\"Heckit\") control function estimator which, with\ntwo-sided non-compliance, can be used to compute estimates of a variety of\ncausal parameters. Equivalence with IV is established for a semi-parametric\nfamily of control function estimators and shown to hold at interior solutions\nfor a class of maximum likelihood estimators. Our results suggest differences\nbetween structural and IV estimates often stem from disagreements about the\ntarget parameter rather than from functional form assumptions per se. In cases\nwhere equivalence fails, reporting structural estimates of LATE alongside IV\nprovides a simple means of assessing the credibility of structural\nextrapolation exercises.\n", "category": [6, 7]}
{"abstract": "  Multinomial choice models are fundamental for empirical modeling of economic\nchoices among discrete alternatives. We analyze identification of binary and\nmultinomial choice models when the choice utilities are nonseparable in\nobserved attributes and multidimensional unobserved heterogeneity with\ncross-section and panel data. We show that derivatives of choice probabilities\nwith respect to continuous attributes are weighted averages of utility\nderivatives in cross-section models with exogenous heterogeneity. In the\nspecial case of random coefficient models with an independent additive effect,\nwe further characterize that the probability derivative at zero is proportional\nto the population mean of the coefficients. We extend the identification\nresults to models with endogenous heterogeneity using either a control function\nor panel data. In time stationary panel models with two periods, we find that\ndifferences over time of derivatives of choice probabilities identify utility\nderivatives \"on the diagonal,\" i.e. when the observed attributes take the same\nvalues in the two periods. We also show that time stationarity does not\nidentify structural derivatives \"off the diagonal\" both in continuous and\nmultinomial choice panel models.\n", "category": [6, 7, 6]}
{"abstract": "  While a number of different algorithms have recently been proposed for\nconvolutional dictionary learning, this remains an expensive problem. The\nsingle biggest impediment to learning from large training sets is the memory\nrequirements, which grow at least linearly with the size of the training set\nsince all existing methods are batch algorithms. The work reported here\naddresses this limitation by extending online dictionary learning ideas to the\nconvolutional context.\n", "category": [0, 0, 1]}
{"abstract": "  In this paper we present tools for applied researchers that re-purpose\noff-the-shelf methods from the computer-science field of machine learning to\ncreate a \"discovery engine\" for data from randomized controlled trials (RCTs).\nThe applied problem we seek to solve is that economists invest vast resources\ninto carrying out RCTs, including the collection of a rich set of candidate\noutcome measures. But given concerns about inference in the presence of\nmultiple testing, economists usually wind up exploring just a small subset of\nthe hypotheses that the available data could be used to test. This prevents us\nfrom extracting as much information as possible from each RCT, which in turn\nimpairs our ability to develop new theories or strengthen the design of policy\ninterventions. Our proposed solution combines the basic intuition of reverse\nregression, where the dependent variable of interest now becomes treatment\nassignment itself, with methods from machine learning that use the data\nthemselves to flexibly identify whether there is any function of the outcomes\nthat predicts (or has signal about) treatment group status. This leads to\ncorrectly-sized tests with appropriate $p$-values, which also have the\nimportant virtue of being easy to implement in practice. One open challenge\nthat remains with our work is how to meaningfully interpret the signal that\nthese methods find.\n", "category": [6, 7, 6, 6]}
{"abstract": "  We propose a hybrid probabilistic process calculus for modelling and\nreasoning on cyber-physical systems (CPSs). The dynamics of the calculus is\nexpressed in terms of a probabilistic labelled transition system in the SOS\nstyle of Plotkin. This is used to define a bisimulation-based probabilistic\nbehavioural semantics which supports compositional reasonings. For a more\ncareful comparison between CPSs, we provide two compositional probabilistic\nmetrics to formalise the notion of behavioural distance between systems, also\nin the case of bounded computations. Finally, we provide a non-trivial case\nstudy, taken from an engineering application, and use it to illustrate our\ndefinitions and our compositional behavioural theory for CPSs.\n", "category": [0, 0, 0, 1]}
{"abstract": "  In the classical herding literature, agents receive a private signal\nregarding a binary state of nature, and sequentially choose an action, after\nobserving the actions of their predecessors. When the informativeness of\nprivate signals is unbounded, it is known that agents converge to the correct\naction and correct belief. We study how quickly convergence occurs, and show\nthat it happens more slowly than it does when agents observe signals. However,\nwe also show that the speed of learning from actions can be arbitrarily close\nto the speed of learning from signals. In particular, the expected time until\nthe agents stop taking the wrong action can be either finite or infinite,\ndepending on the private signal distribution. In the canonical case of Gaussian\nprivate signals we calculate the speed of convergence precisely, and show\nexplicitly that, in this case, learning from actions is significantly slower\nthan learning from signals.\n", "category": [2, 7]}
{"abstract": "  This paper develops theory for feasible estimators of finite-dimensional\nparameters identified by general conditional quantile restrictions, under much\nweaker assumptions than previously seen in the literature. This includes\ninstrumental variables nonlinear quantile regression as a special case. More\nspecifically, we consider a set of unconditional moments implied by the\nconditional quantile restrictions, providing conditions for local\nidentification. Since estimators based on the sample moments are generally\nimpossible to compute numerically in practice, we study feasible estimators\nbased on smoothed sample moments. We propose a method of moments estimator for\nexactly identified models, as well as a generalized method of moments estimator\nfor over-identified models. We establish consistency and asymptotic normality\nof both estimators under general conditions that allow for weakly dependent\ndata and nonlinear structural models. Simulations illustrate the finite-sample\nproperties of the methods. Our in-depth empirical application concerns the\nconsumption Euler equation derived from quantile utility maximization.\nAdvantages of the quantile Euler equation include robustness to fat tails,\ndecoupling of risk attitude from the elasticity of intertemporal substitution,\nand log-linearization without any approximation error. For the four countries\nwe examine, the quantile estimates of discount factor and elasticity of\nintertemporal substitution are economically reasonable for a range of quantiles\nabove the median, even when two-stage least squares estimates are not\nreasonable.\n", "category": [2, 7, 6, 6]}
{"abstract": "  This paper examines signalling when the sender exerts effort and receives\nbenefits over time. Receivers only observe a noisy public signal about the\neffort, which has no intrinsic value.\n  The modelling of signalling in a dynamic context gives rise to novel\nequilibrium outcomes. In some equilibria, a sender with a higher cost of effort\nexerts strictly more effort than his low-cost counterpart. The low-cost type\ncan compensate later for initial low effort, but this is not worthwhile for a\nhigh-cost type. The interpretation of a given signal switches endogenously over\ntime, depending on which type the receivers expect to send it.\n  JEL classification: D82, D83, C73.\n  Keywords: Dynamic games, signalling , incomplete information\n", "category": [5, 7]}
{"abstract": "  We show that the space in which scientific, technological and economic\ndevelopments interplay with each other can be mathematically shaped using\npioneering multilayer network and complexity techniques. We build the\ntri-layered network of human activities (scientific production, patenting, and\nindustrial production) and study the interactions among them, also taking into\naccount the possible time delays. Within this construction we can identify\nwhich capabilities and prerequisites are needed to be competitive in a given\nactivity, and even measure how much time is needed to transform, for instance,\nthe technological know-how into economic wealth and scientific innovation,\nbeing able to make predictions with a very long time horizon. Quite\nunexpectedly, we find empirical evidence that the naive knowledge flow from\nscience, to patents, to products is not supported by data, being instead\ntechnology the best predictor for industrial and scientific production for the\nnext decades.\n", "category": [7, 0, 3, 5]}
{"abstract": "  Although great progresses have been made in automatic speech recognition\n(ASR), significant performance degradation is still observed when recognizing\nmulti-talker mixed speech. In this paper, we propose and evaluate several\narchitectures to address this problem under the assumption that only a single\nchannel of mixed signal is available. Our technique extends permutation\ninvariant training (PIT) by introducing the front-end feature separation module\nwith the minimum mean square error (MSE) criterion and the back-end recognition\nmodule with the minimum cross entropy (CE) criterion. More specifically, during\ntraining we compute the average MSE or CE over the whole utterance for each\npossible utterance-level output-target assignment, pick the one with the\nminimum MSE or CE, and optimize for that assignment. This strategy elegantly\nsolves the label permutation problem observed in the deep learning based\nmulti-talker mixed speech separation and recognition systems. The proposed\narchitectures are evaluated and compared on an artificially mixed AMI dataset\nwith both two- and three-talker mixed speech. The experimental results indicate\nthat our proposed architectures can cut the word error rate (WER) by 45.0% and\n25.0% relatively against the state-of-the-art single-talker speech recognition\nsystem across all speakers when their energies are comparable, for two- and\nthree-talker mixed speech, respectively. To our knowledge, this is the first\nwork on the multi-talker mixed speech recognition on the challenging\nspeaker-independent spontaneous large vocabulary continuous speech task.\n", "category": [0, 0, 0, 1]}
{"abstract": "  Two different approaches have recently been proposed for boundary handling in\nconvolutional sparse representations, avoiding potential boundary artifacts\narising from the circular boundary conditions implied by the use of frequency\ndomain solution methods by introducing a spatial mask into the convolutional\nsparse coding problem. In the present paper we show that, under certain\ncircumstances, these methods fail in their design goal of avoiding boundary\nartifacts. The reasons for this failure are discussed, a solution is proposed,\nand the practical implications are illustrated in an image deblurring problem.\n", "category": [0, 1]}
{"abstract": "  We consider the problem of convergence to a saddle point of a concave-convex\nfunction via gradient dynamics. Since first introduced by Arrow, Hurwicz and\nUzawa in [1] such dynamics have been extensively used in diverse areas, there\nare, however, features that render their analysis non trivial. These include\nthe lack of convergence guarantees when the function considered is not strictly\nconcave-convex and also the non-smoothness of subgradient dynamics. Our aim in\nthis two part paper is to provide an explicit characterization to the\nasymptotic behaviour of general gradient and subgradient dynamics applied to a\ngeneral concave-convex function. We show that despite the nonlinearity and\nnon-smoothness of these dynamics their $\\omega$-limit set is comprised of\ntrajectories that solve only explicit linear ODEs that are characterized within\nthe paper.\n  More precisely, in Part I an exact characterization is provided to the\nasymptotic behaviour of unconstrained gradient dynamics. We also show that when\nconvergence to a saddle point is not guaranteed then the system behaviour can\nbe problematic, with arbitrarily small noise leading to an unbounded variance.\nIn Part II we consider a general class of subgradient dynamics that restrict\ntrajectories in an arbitrary convex domain, and show that when an equilibrium\npoint exists their limiting trajectories are solutions of subgradient dynamics\non only affine subspaces. The latter is a smooth class of dynamics with an\nasymptotic behaviour exactly characterized in Part I, as solutions to explicit\nlinear ODEs. These results are used to formulate corresponding convergence\ncriteria and are demonstrated with several examples and applications presented\nin Part II.\n", "category": [2, 0, 1]}
{"abstract": "  Numerical (and experimental) data analysis often requires the restoration of\na smooth function from a set of sampled integrals over finite bins. We present\nthe bin hierarchy method that efficiently computes the maximally smooth\nfunction from the sampled integrals using essentially all the information\ncontained in the data. We perform extensive tests with different classes of\nfunctions and levels of data quality, including Monte Carlo data suffering from\na severe sign problem and physical data for the Green's function of the\nFr\\\"ohlich polaron.\n", "category": [6, 3, 1, 3]}
{"abstract": "  Direction of Arrival (DOA) estimation of mixed uncorrelated and coherent\nsources is a long existing challenge in array signal processing. Application of\ncompressive sensing to array signal processing has opened up an exciting class\nof algorithms. The authors investigated the application of orthogonal matching\npursuit (OMP) for direction of Arrival (DOA) estimation for different\nscenarios, especially to tackle the case of coherent sources and observed\ninconsistencies in the results. In this paper, a modified OMP algorithm is\nproposed to overcome these deficiencies by exploiting maximum variance based\ncriterion using only one snapshot. This criterion relaxes the imposed\nrestricted isometry property (RIP) on the measurement matrix to obtain the\nsources and hence, reduces the sparsity of the input vector to the local OMP\nalgorithm. Moreover, it also tackles sources irrespective of their coherency.\nThe condition for the weak-1 RIP on decreased sparsity is derived and it is\nshown that how the algorithm gives better result than the OMP algorithm. With\nan addition to this, a simple method is also presented to calculate source\ndistance from the reference point in a uniform linear sensor array. Numerical\nanalysis demonstrates the effectiveness of the proposed algorithm.\n", "category": [1, 0, 2]}
{"abstract": "  One version of the concept of structural controllability defined for\nsingle-input systems by Lin and subsequently generalized to multi-input systems\nby others, states that a parameterized matrix pair $(A, B)$ whose nonzero\nentries are distinct parameters, is structurally controllable if values can be\nassigned to the parameters which cause the resulting matrix pair to be\ncontrollable. In this paper the concept of structural controllability is\nbroadened to allow for the possibility that a parameter may appear in more than\none location in the pair $(A, B)$. Subject to a certain condition on the\nparameterization called the \"binary assumption\", an explicit graph-theoretic\ncharacterization of such matrix pairs is derived.\n", "category": [0, 1]}
{"abstract": "  Objective:Optoacoustic (photoacoustic) tomography is aimed at reconstructing\nmaps of the initial pressure rise induced by the absorption of light pulses in\ntissue. In practice, due to inaccurate assumptions in the forward model, noise\nand other experimental factors, the images are often afflicted by artifacts,\noccasionally manifested as negative values. The aim of the work is to develop\nan inversion method which reduces the occurrence of negative values and\nimproves the quantitative performance of optoacoustic imaging. Methods: We\npresent a novel method for optoacoustic tomography based on an entropy\nmaximization algorithm, which uses logarithmic regularization for attaining\nnon-negative reconstructions. The reconstruction image quality is further\nimproved using structural prior based fluence correction. Results: We report\nthe performance achieved by the entropy maximization scheme on numerical\nsimulation, experimental phantoms and in-vivo samples. Conclusion: The proposed\nalgorithm demonstrates superior reconstruction performance by delivering\nnon-negative pixel values with no visible distortion of anatomical structures.\nSignificance: Our method can enable quantitative optoacoustic imaging, and has\nthe potential to improve pre-clinical and translational imaging applications.\n", "category": [3, 0, 1, 3]}
{"abstract": "  A position-multiplexing based cryptosystem is proposed to enhance the\ninformation security with an ultra-broadband illumination. The simplified\noptical encryption system only contains one diffuser acting as the random phase\nmask (RPM). Light coming from a plaintext passes through this RPM and generates\nthe corresponding ciphertext on a camera. The proposed system effectively\nreduces problems of misalignment and coherent noise that are found in the\ncoherent illumination. Here, the use of ultra-broadband illumination has the\nadvantage of making a strong scattering and complex ciphertext by reducing the\nspeckle contrast. Reduction of the ciphertext size further increases the\nstrength of the ciphering. The unique spatial keys are utilized for the\nindividual decryption as the plaintext locates at different spatial positions,\nand a complete decrypted image could be concatenated with high fidelity.\nBenefiting from the ultra-broadband illumination and position-multiplexing, the\ninformation of interest is scrambled together in a small ciphertext. Only the\nauthorized user can decrypt this information with the correct keys. Therefore,\na high performance security for a cryptosystem could be achieved.\n", "category": [3, 1, 1]}
{"abstract": "  In traditional optical imaging systems, the spatial resolution is limited by\nthe physics of diffraction, which acts as a low-pass filter. The information on\nsub-wavelength features is carried by evanescent waves, never reaching the\ncamera, thereby posing a hard limit on resolution: the so-called diffraction\nlimit. Modern microscopic methods enable super-resolution, by employing\nflorescence techniques. State-of-the-art localization based fluorescence\nsubwavelength imaging techniques such as PALM and STORM achieve sub-diffraction\nspatial resolution of several tens of nano-meters. However, they require tens\nof thousands of exposures, which limits their temporal resolution. We have\nrecently proposed SPARCOM (sparsity based super-resolution correlation\nmicroscopy), which exploits the sparse nature of the fluorophores distribution,\nalongside a statistical prior of uncorrelated emissions, and showed that\nSPARCOM achieves spatial resolution comparable to PALM/STORM, while capturing\nthe data hundreds of times faster. Here, we provide a detailed mathematical\nformulation of SPARCOM, which in turn leads to an efficient numerical\nimplementation, suitable for large-scale problems. We further extend our method\nto a general framework for sparsity based super-resolution imaging, in which\nsparsity can be assumed in other domains such as wavelet or discrete-cosine,\nleading to improved reconstructions in a variety of physical settings.\n", "category": [3, 1]}
{"abstract": "  Multispectral imaging plays an important role in many applications from\nastronomical imaging, earth observation to biomedical imaging. However, the\ncurrent technologies are complex with multiple alignment-sensitive components,\npredetermined spatial and spectral parameters by manufactures. Here, we\ndemonstrate a single-shot multispectral imaging technique that gives\nflexibility to end-users with a very simple optical setup, thank to spatial\ncorrelation and spectral decorrelation of speckle patterns. These seemingly\nrandom speckle patterns are point spreading functions (PSFs) generated by light\nfrom point sources propagating through a strongly scattering medium. The\nspatial correlation of PSFs allows image recovery with deconvolution\ntechniques, while the spectral decorrelation allows them to play the role of\ntune-able spectral filters in the deconvolution process. Our demonstrations\nutilizing optical physics of strongly scattering media and computational\nimaging present the most cost-effective approach for multispectral imaging with\ngreat advantages.\n", "category": [3, 1, 1, 3]}
{"abstract": "  Conditional independence of treatment assignment from potential outcomes is a\ncommonly used but nonrefutable assumption. We derive identified sets for\nvarious treatment effect parameters under nonparametric deviations from this\nconditional independence assumption. These deviations are defined via a\nconditional treatment assignment probability, which makes it straightforward to\ninterpret. Our results can be used to assess the robustness of empirical\nconclusions obtained under the baseline conditional independence assumption.\n", "category": [6, 7]}
{"abstract": "  Optically focusing and imaging through strongly scattering media are\nchallenging tasks but have widespread applications from scientific research to\nbiomedical applications and daily life. Benefiting from the memory effect (ME)\nfor speckle intensity correlations, only one single-shot speckle pattern can be\nused for the high quality recovery of the objects and avoiding some complicated\nprocedures to reduce scattering effects. In spite of all the spatial\ninformation from a large object being embedded in a single speckle image, ME\ngives a strict limitation to the field of view (FOV) for imaging through\nscattering media. Objects beyond the ME region cannot be recovered and only\nproduce unwanted speckle patterns, causing reduction in the speckle contrast\nand recovery quality. Here, we extract the spatial information by utilizing\nthese unavoidable speckle patterns, and enlarge the FOV of the optical imaging\nsystem. Regional point spreading functions (PSFs), which are fixed and only\nneed to be recorded once for all time use, are employed to recover\ncorresponding spatial regions of an object by deconvolution algorithm. Then an\nautomatic weighted averaging in an iterative process is performed to obtain the\nobject with significantly enlarged FOV. Our results present an important step\ntoward an advanced imaging technique with strongly scattering media.\n", "category": [3, 1, 1, 3]}
{"abstract": "  We analyse the autocatalytic structure of technological networks and evaluate\nits significance for the dynamics of innovation patenting. To this aim, we\ndefine a directed network of technological fields based on the International\nPatents Classification, in which a source node is connected to a receiver node\nvia a link if patenting activity in the source field anticipates patents in the\nreceiver field in the same region more frequently than we would expect at\nrandom. We show that the evolution of the technology network is compatible with\nthe presence of a growing autocatalytic structure, i.e. a portion of the\nnetwork in which technological fields mutually benefit from being connected to\none another. We further show that technological fields in the core of the\nautocatalytic set display greater fitness, i.e. they tend to appear in a\ngreater number of patents, thus suggesting the presence of positive spillovers\nas well as positive reinforcement. Finally, we observe that core shifts take\nplace whereby different groups of technology fields alternate within the\nautocatalytic structure; this points to the importance of recombinant\ninnovation taking place between close as well as distant fields of the\nhierarchical classification of technological fields.\n", "category": [7, 0, 5]}
{"abstract": "  When comparing two distributions, it is often helpful to learn at which\nquantiles or values there is a statistically significant difference. This\nprovides more information than the binary \"reject\" or \"do not reject\" decision\nof a global goodness-of-fit test. Framing our question as multiple testing\nacross the continuum of quantiles $\\tau\\in(0,1)$ or values $r\\in\\mathbb{R}$, we\nshow that the Kolmogorov--Smirnov test (interpreted as a multiple testing\nprocedure) achieves strong control of the familywise error rate. However, its\nwell-known flaw of low sensitivity in the tails remains. We provide an\nalternative method that retains such strong control of familywise error rate\nwhile also having even sensitivity, i.e., equal pointwise type I error rates at\neach of $n\\to\\infty$ order statistics across the distribution. Our one-sample\nmethod computes instantly, using our new formula that also instantly computes\ngoodness-of-fit $p$-values and uniform confidence bands. To improve power, we\nalso propose stepdown and pre-test procedures that maintain control of the\nasymptotic familywise error rate. One-sample and two-sample cases are\nconsidered, as well as extensions to regression discontinuity designs and\nconditional distributions. Simulations, empirical examples, and code are\nprovided.\n", "category": [2, 7, 6, 6]}
{"abstract": "  The increase of the quantity of user-generated content experienced in social\nmedia has boosted the importance of analysing and organising the content by its\nquality. Here, we propose a method that uses audio fingerprinting to organise\nand infer the quality of user-generated audio content. The proposed method\ndetects the overlapping segments between different audio clips to organise and\ncluster the data according to events, and to infer the audio quality of the\nsamples. A test setup with concert recordings manually crawled from YouTube is\nused to validate the presented method. The results show that the proposed\nmethod achieves better results than previous methods.\n", "category": [1, 0]}
{"abstract": "  Using solely the information retrieved by audio fingerprinting techniques, we\npropose methods to treat a possibly large dataset of user-generated audio\ncontent, that (1) enable the grouping of several audio files that contain a\ncommon audio excerpt (i.e., are relative to the same event), and (2) give\ninformation about how those files are correlated in terms of time and quality\ninside each event. Furthermore, we use supervised learning to detect incorrect\nmatches that may arise from the audio fingerprinting algorithm itself, whilst\nensuring our model learns with previous predictions. All the presented methods\nwere further validated by user-generated recordings of several different\nconcerts manually crawled from YouTube.\n", "category": [1, 0, 0, 0]}
{"abstract": "  The memory-type control charts, such as EWMA and CUSUM, are powerful tools\nfor detecting small quality changes in univariate and multivariate processes.\nMany papers on economic design of these control charts use the formula proposed\nby Lorenzen and Vance (1986) [Lorenzen, T. J., & Vance, L. C. (1986). The\neconomic design of control charts: A unified approach. Technometrics, 28(1),\n3-10, DOI: 10.2307/1269598]. This paper shows that this formula is not correct\nfor memory-type control charts and its values can significantly deviate from\nthe original values even if the ARL values used in this formula are accurately\ncomputed. Consequently, the use of this formula can result in charts that are\nnot economically optimal. The formula is corrected for memory-type control\ncharts, but unfortunately the modified formula is not a helpful tool from a\ncomputational perspective. We show that simulation-based optimization is a\npossible alternative method.\n", "category": [6, 0, 7, 2, 5]}
{"abstract": "  Approximate Symbol error rate (SER), outage probability and rate expressions\nare derived for receive diversity system employing optimum combining when both\nthe desired and the interfering signals are subjected to Rician fading, for the\ncases of a) equal power uncorrelated interferers b) unequal power interferers\nc) interferer correlation. The derived expressions are applicable for an\narbitrary number of receive antennas and interferers and for any quadrature\namplitude modulation (QAM) constellation. Furthermore, we derive a simple\nclosed form expression for SER in the interference-limited regime, for the\nspecial case of Rayleigh faded interferers. A close match is observed between\nthe SER, outage probability and rate results obtained through the derived\nanalytical expressions and the ones obtained from Monte-Carlo simulations.\n", "category": [1, 0, 2]}
{"abstract": "  Shrinkage estimation usually reduces variance at the cost of bias. But when\nwe care only about some parameters of a model, I show that we can reduce\nvariance without incurring bias if we have additional information about the\ndistribution of covariates. In a linear regression model with homoscedastic\nNormal noise, I consider shrinkage estimation of the nuisance parameters\nassociated with control variables. For at least three control variables and\nexogenous treatment, I establish that the standard least-squares estimator is\ndominated with respect to squared-error loss in the treatment effect even among\nunbiased estimators and even when the target parameter is low-dimensional. I\nconstruct the dominating estimator by a variant of James-Stein shrinkage in a\nhigh-dimensional Normal-means problem. It can be interpreted as an invariant\ngeneralized Bayes estimator with an uninformative (improper) Jeffreys prior in\nthe target parameter.\n", "category": [2, 7, 6, 6]}
{"abstract": "  The two-stage least-squares (2SLS) estimator is known to be biased when its\nfirst-stage fit is poor. I show that better first-stage prediction can\nalleviate this bias. In a two-stage linear regression model with Normal noise,\nI consider shrinkage in the estimation of the first-stage instrumental variable\ncoefficients. For at least four instrumental variables and a single endogenous\nregressor, I establish that the standard 2SLS estimator is dominated with\nrespect to bias. The dominating IV estimator applies James-Stein type shrinkage\nin a first-stage high-dimensional Normal-means problem followed by a\ncontrol-function approach in the second stage. It preserves invariances of the\nstructural instrumental variable equations.\n", "category": [2, 7, 6, 6]}
{"abstract": "  Econometrics and machine learning seem to have one common goal: to construct\na predictive model, for a variable of interest, using explanatory variables (or\nfeatures). However, these two fields developed in parallel, thus creating two\ndifferent cultures, to paraphrase Breiman (2001). The first was to build\nprobabilistic models to describe economic phenomena. The second uses algorithms\nthat will learn from their mistakes, with the aim, most often to classify\n(sounds, images, etc.). Recently, however, learning models have proven to be\nmore effective than traditional econometric techniques (with a price to pay\nless explanatory power), and above all, they manage to manage much larger data.\nIn this context, it becomes necessary for econometricians to understand what\nthese two cultures are, what opposes them and especially what brings them\ncloser together, in order to appropriate tools developed by the statistical\nlearning community to integrate them into Econometric models.\n", "category": [6, 7]}
{"abstract": "  Video coding standards are primarily designed for efficient lossy\ncompression, but it is also desirable to support efficient lossless compression\nwithin video coding standards using small modifications to the lossy coding\narchitecture. A simple approach is to skip transform and quantization, and\nsimply entropy code the prediction residual. However, this approach is\ninefficient at compression. A more efficient and popular approach is to skip\ntransform and quantization but also process the residual block with DPCM, along\nthe horizontal or vertical direction, prior to entropy coding. This paper\nexplores an alternative approach based on processing the residual block with\ninteger-to-integer (i2i) transforms. I2i transforms can map integer pixels to\ninteger transform coefficients without increasing the dynamic range and can be\nused for lossless compression. We focus on lossless intra coding and develop\nnovel i2i approximations of the odd type-3 DST (ODST-3). Experimental results\nwith the HEVC reference software show that the developed i2i approximations of\nthe ODST-3 improve lossless intra-frame compression efficiency with respect to\nHEVC version 2, which uses the popular DPCM method, by an average 2.7% without\na significant effect on computational complexity.\n", "category": [0, 1]}
{"abstract": "  It is known that the common factors in a large panel of data can be\nconsistently estimated by the method of principal components, and principal\ncomponents can be constructed by iterative least squares regressions. Replacing\nleast squares with ridge regressions turns out to have the effect of shrinking\nthe singular values of the common component and possibly reducing its rank. The\nmethod is used in the machine learning literature to recover low-rank matrices.\nWe study the procedure from the perspective of estimating a minimum-rank\napproximate factor model. We show that the constrained factor estimates are\nbiased but can be more efficient in terms of mean-squared errors. Rank\nconsideration suggests a data-dependent penalty for selecting the number of\nfactors. The new criterion is more conservative in cases when the nominal\nnumber of factors is inflated by the presence of weak factors or large\nmeasurement noise. The framework is extended to incorporate a priori linear\nconstraints on the loadings. We provide asymptotic results that can be used to\ntest economic hypotheses.\n", "category": [6, 7]}
{"abstract": "  The most widely used form of convolutional sparse coding uses an $\\ell_1$\nregularization term. While this approach has been successful in a variety of\napplications, a limitation of the $\\ell_1$ penalty is that it is homogeneous\nacross the spatial and filter index dimensions of the sparse representation\narray, so that sparsity cannot be separately controlled across these\ndimensions. The present paper considers the consequences of replacing the\n$\\ell_1$ penalty with a mixed group norm, motivated by recent theoretical\nresults for convolutional sparse representations. Algorithms are developed for\nsolving the resulting problems, which are quite challenging, and the impact on\nthe performance of the denoising problem is evaluated. The mixed group norms\nare found to perform very poorly in this application. While their performance\nis greatly improved by introducing a weighting strategy, such a strategy also\nimproves the performance obtained from the much simpler and computationally\ncheaper $\\ell_1$ norm.\n", "category": [0, 1]}
{"abstract": "  In this paper we propose to use utterance-level Permutation Invariant\nTraining (uPIT) for speaker independent multi-talker speech separation and\ndenoising, simultaneously. Specifically, we train deep bi-directional Long\nShort-Term Memory (LSTM) Recurrent Neural Networks (RNNs) using uPIT, for\nsingle-channel speaker independent multi-talker speech separation in multiple\nnoisy conditions, including both synthetic and real-life noise signals. We\nfocus our experiments on generalizability and noise robustness of models that\nrely on various types of a priori knowledge e.g. in terms of noise type and\nnumber of simultaneous speakers. We show that deep bi-directional LSTM RNNs\ntrained using uPIT in noisy environments can improve the Signal-to-Distortion\nRatio (SDR) as well as the Extended Short-Time Objective Intelligibility\n(ESTOI) measure, on the speaker independent multi-talker speech separation and\ndenoising task, for various noise types and Signal-to-Noise Ratios (SNRs).\nSpecifically, we first show that LSTM RNNs can achieve large SDR and ESTOI\nimprovements, when evaluated using known noise types, and that a single model\nis capable of handling multiple noise types with only a slight decrease in\nperformance. Furthermore, we show that a single LSTM RNN can handle both\ntwo-speaker and three-speaker noisy mixtures, without a priori knowledge about\nthe exact number of speakers. Finally, we show that LSTM RNNs trained using\nuPIT generalize well to noise types not seen during training.\n", "category": [0, 1]}
{"abstract": "  Convolutional sparse representations are a form of sparse representation with\na structured, translation invariant dictionary. Most convolutional dictionary\nlearning algorithms to date operate in batch mode, requiring simultaneous\naccess to all training images during the learning process, which results in\nvery high memory usage and severely limits the training data that can be used.\nVery recently, however, a number of authors have considered the design of\nonline convolutional dictionary learning algorithms that offer far better\nscaling of memory and computational cost with training set size than batch\nmethods. This paper extends our prior work, improving a number of aspects of\nour previous algorithm; proposing an entirely new one, with better performance,\nand that supports the inclusion of a spatial mask for learning from incomplete\ndata; and providing a rigorous theoretical analysis of these methods.\n", "category": [0, 0, 1, 2, 6]}
{"abstract": "  In this paper a spectrum sensing policy employing recency-based exploration\nis proposed for cognitive radio networks. We formulate the problem of finding a\nspectrum sensing policy for multi-band dynamic spectrum access as a stochastic\nrestless multi-armed bandit problem with stationary unknown reward\ndistributions. In cognitive radio networks the multi-armed bandit problem\narises when deciding where in the radio spectrum to look for idle frequencies\nthat could be efficiently exploited for data transmission. We consider two\nmodels for the dynamics of the frequency bands: 1) the independent model where\nthe state of the band evolves randomly independently from the past and 2) the\nGilbert-Elliot model, where the states evolve according to a 2-state Markov\nchain. It is shown that in these conditions the proposed sensing policy attains\nasymptotically logarithmic weak regret. The policy proposed in this paper is an\nindex policy, in which the index of a frequency band is comprised of a sample\nmean term and a recency-based exploration bonus term. The sample mean promotes\nspectrum exploitation whereas the exploration bonus encourages for further\nexploration for idle bands providing high data rates. The proposed recency\nbased approach readily allows constructing the exploration bonus such that it\nwill grow the time interval between consecutive sensing time instants of a\nsuboptimal band exponentially, which then leads to logarithmically increasing\nweak regret. Simulation results confirming logarithmic weak regret are\npresented and it is found that the proposed policy provides often improved\nperformance at low complexity over other state-of-the-art policies in the\nliterature.\n", "category": [1, 0, 2]}
{"abstract": "  The two-terminal key agreement problem with biometric or physical identifiers\nis considered. Two linear code constructions based on Wyner-Ziv coding are\ndeveloped. The first construction uses random linear codes and achieves all\npoints of the key-leakage-storage regions of the generated-secret and\nchosen-secret models. The second construction uses nested polar codes for\nvector quantization during enrollment and for error correction during\nreconstruction. Simulations show that the nested polar codes achieve\nprivacy-leakage and storage rates that improve on existing code designs. One\nproposed code achieves a rate tuple that cannot be achieved by existing\nmethods.\n", "category": [0, 0, 0, 1, 2, 2]}
{"abstract": "  Speech enhancement (SE) aims to reduce noise in speech signals. Most SE\ntechniques focus only on addressing audio information. In this work, inspired\nby multimodal learning, which utilizes data from different modalities, and the\nrecent success of convolutional neural networks (CNNs) in SE, we propose an\naudio-visual deep CNNs (AVDCNN) SE model, which incorporates audio and visual\nstreams into a unified network model. We also propose a multi-task learning\nframework for reconstructing audio and visual signals at the output layer.\nPrecisely speaking, the proposed AVDCNN model is structured as an audio-visual\nencoder-decoder network, in which audio and visual data are first processed\nusing individual CNNs, and then fused into a joint network to generate enhanced\nspeech (the primary task) and reconstructed images (the secondary task) at the\noutput layer. The model is trained in an end-to-end manner, and parameters are\njointly learned through back-propagation. We evaluate enhanced speech using\nfive instrumental criteria. Results show that the AVDCNN model yields a notably\nsuperior performance compared with an audio-only CNN-based SE model and two\nconventional SE approaches, confirming the effectiveness of integrating visual\ninformation into the SE process. In addition, the AVDCNN model also outperforms\nan existing audio-visual SE model, confirming its capability of effectively\ncombining audio and visual information in SE.\n", "category": [0, 0, 1, 6]}
{"abstract": "  This article presents the consensus of a saturated second order multi-agent\nsystem with non-switching dynamics that can be represented by a directed graph.\nThe system is affected by data processing (input delay) and communication\ntime-delays that are assumed to be asynchronous. The agents have saturation\nnonlinearities, each of them is approximated into separate linear and nonlinear\nelements. Nonlinear elements are represented by describing functions.\nDescribing functions and stability of linear elements are used to estimate the\nexistence of limit cycles in the system with multiple control laws. Stability\nanalysis of the linear element is performed using Lyapunov-Krasovskii functions\nand frequency domain analysis. A comparison of pros and cons of both the\nanalyses with respect to time-delay ranges, applicability and computation\ncomplexity is presented. Simulation and corresponding hardware implementation\nresults are demonstrated to support theoretical results.\n", "category": [0, 0, 1]}
{"abstract": "  We add the assumption that players know their opponents' payoff functions and\nrationality to a model of non-equilibrium learning in signaling games. Agents\nare born into player roles and play against random opponents every period.\nInexperienced agents are uncertain about the prevailing distribution of\nopponents' play, but believe that opponents never choose conditionally\ndominated strategies. Agents engage in active learning and update beliefs based\non personal observations. Payoff information can refine or expand learning\npredictions, since patient young senders' experimentation incentives depend on\nwhich receiver responses they deem plausible. We show that with payoff\nknowledge, the limiting set of long-run learning outcomes is bounded above by\nrationality-compatible equilibria (RCE), and bounded below by uniform RCE. RCE\nrefine the Intuitive Criterion (Cho and Kreps, 1987) and include all divine\nequilibria (Banks and Sobel, 1987). Uniform RCE sometimes but not always\nexists, and implies universally divine equilibrium.\n", "category": [7]}
{"abstract": "  Improving speech system performance in noisy environments remains a\nchallenging task, and speech enhancement (SE) is one of the effective\ntechniques to solve the problem. Motivated by the promising results of\ngenerative adversarial networks (GANs) in a variety of image processing tasks,\nwe explore the potential of conditional GANs (cGANs) for SE, and in particular,\nwe make use of the image processing framework proposed by Isola et al. [1] to\nlearn a mapping from the spectrogram of noisy speech to an enhanced\ncounterpart. The SE cGAN consists of two networks, trained in an adversarial\nmanner: a generator that tries to enhance the input noisy spectrogram, and a\ndiscriminator that tries to distinguish between enhanced spectrograms provided\nby the generator and clean ones from the database using the noisy spectrogram\nas a condition. We evaluate the performance of the cGAN method in terms of\nperceptual evaluation of speech quality (PESQ), short-time objective\nintelligibility (STOI), and equal error rate (EER) of speaker verification (an\nexample application). Experimental results show that the cGAN method overall\noutperforms the classical short-time spectral amplitude minimum mean square\nerror (STSA-MMSE) SE algorithm, and is comparable to a deep neural\nnetwork-based SE approach (DNN-SE).\n", "category": [1, 0, 0, 1, 6]}
{"abstract": "  A compact version of the variation evolving method (VEM) is developed in the\nprimal variable space for optimal control computation. Following the idea that\noriginates from the Lyapunov continuous-time dynamics stability theory in the\ncontrol field, the optimal solution is analogized to the stable equilibrium\npoint of a dynamic system and obtained asymptotically through the variation\nmotion. With the introduction of a virtual dimension, namely the variation\ntime, the evolution partial differential equation (EPDE), which seeks the\noptimal solution with a theoretical guarantee, is developed for the optimal\ncontrol problem (OCP) with free terminal states, and the equivalent optimality\nconditions with no employment of costates are established in the primal space.\nThese conditions show that the optimal feedback control law is generally not\nanalytically available because the optimal control is related to the future\nstates. Since the derived EPDE is suitable to be computed with the\nsemi-discrete method in the field of PDE numerical calculation, the optimal\nsolution may be obtained by solving the resulting finite-dimensional\ninitial-value problem (IVP).\n", "category": [1, 0]}
{"abstract": "  Convolutional sparse representations are a form of sparse representation with\na dictionary that has a structure that is equivalent to convolution with a set\nof linear filters. While effective algorithms have recently been developed for\nthe convolutional sparse coding problem, the corresponding dictionary learning\nproblem is substantially more challenging. Furthermore, although a number of\ndifferent approaches have been proposed, the absence of thorough comparisons\nbetween them makes it difficult to determine which of them represents the\ncurrent state of the art. The present work both addresses this deficiency and\nproposes some new approaches that outperform existing ones in certain contexts.\nA thorough set of performance comparisons indicates a very wide range of\nperformance differences among the existing and proposed methods, and clearly\nidentifies those that are the most effective.\n", "category": [0, 1, 6]}
{"abstract": "  The aim of atypicality is to extract small, rare, unusual and interesting\npieces out of big data. This complements statistics about typical data to give\ninsight into data. In order to find such \"interesting\" parts of data, universal\napproaches are required, since it is not known in advance what we are looking\nfor. We therefore base the atypicality criterion on codelength. In a prior\npaper we developed the methodology for discrete-valued data, and the the\ncurrent paper extends this to real-valued data. This is done by using minimum\ndescription length (MDL). We show that this shares a number of theoretical\nproperties with the discrete-valued case. We develop the methodology for a\nnumber of \"universal\" signal processing models, and finally apply them to\nrecorded hydrophone data.\n", "category": [1, 0, 2]}
{"abstract": "  We show that estimators based on spectral regularization converge to the best\napproximation of a structural parameter in a class of nonidentified linear\nill-posed inverse models. Importantly, this convergence holds in the uniform\nand Hilbert space norms. We describe several circumstances when the best\napproximation coincides with a structural parameter, or at least reasonably\napproximates it, and discuss how our results can be useful in the partial\nidentification setting. Lastly, we document that identification failures have\nimportant implications for the asymptotic distribution of a linear functional\nof regularized estimators, which can have a weighted chi-squared component. The\ntheory is illustrated for various high-dimensional and nonparametric IV\nregressions.\n", "category": [2, 7, 6, 6]}
{"abstract": "  We generalize a support vector machine to a support spinor machine by using\nthe mathematical structure of wedge product over vector machine in order to\nextend field from vector field to spinor field. The separated hyperplane is\nextended to Kolmogorov space in time series data which allow us to extend a\nstructure of support vector machine to a support tensor machine and a support\ntensor machine moduli space. Our performance test on support spinor machine is\ndone over one class classification of end point in physiology state of time\nseries data after empirical mode analysis and compared with support vector\nmachine test. We implement algorithm of support spinor machine by using\nHolo-Hilbert amplitude modulation for fully nonlinear and nonstationary time\nseries data analysis.\n", "category": [0, 1, 5, 6]}
{"abstract": "  This note is a contribution to the debate about the optimal algorithm for\nEconomic Complexity that recently appeared on ArXiv [1, 2] . The authors of [2]\neventually agree that the ECI+ algorithm [1] consists just in a renaming of the\nFitness algorithm we introduced in 2012, as we explicitly showed in [3].\nHowever, they omit any comment on the fact that their extensive numerical tests\nclaimed to demonstrate that the same algorithm works well if they name it ECI+,\nbut not if its name is Fitness. They should realize that this eliminates any\ncredibility to their numerical methods and therefore also to their new\nanalysis, in which they consider many algorithms [2]. Since by their own\nadmission the best algorithm is the Fitness one, their new claim became that\nthe search for the best algorithm is pointless and all algorithms are alike.\nThis is exactly the opposite of what they claimed a few days ago and it does\nnot deserve much comments. After these clarifications we also present a\nconstructive analysis of the status of Economic Complexity, its algorithms, its\nsuccesses and its perspectives. For us the discussion closes here, we will not\nreply to further comments.\n", "category": [7, 5]}
{"abstract": "  Sparse representation learning has recently gained a great success in signal\nand image processing, thanks to recent advances in dictionary learning. To this\nend, the $\\ell_0$-norm is often used to control the sparsity level.\nNevertheless, optimization problems based on the $\\ell_0$-norm are non-convex\nand NP-hard. For these reasons, relaxation techniques have been attracting much\nattention of researchers, by priorly targeting approximation solutions (e.g.\n$\\ell_1$-norm, pursuit strategies). On the contrary, this paper considers the\nexact $\\ell_0$-norm optimization problem and proves that it can be solved\neffectively, despite of its complexity. The proposed method reformulates the\nproblem as a Mixed-Integer Quadratic Program (MIQP) and gets the global optimal\nsolution by applying existing optimization software. Because the main\ndifficulty of this approach is its computational time, two techniques are\nintroduced that improve the computational speed. Finally, our method is applied\nto image denoising which shows its feasibility and relevance compared to the\nstate-of-the-art.\n", "category": [0, 1]}
{"abstract": "  This paper proposes some bounds on the maximum of magnitude of a random mask\nin Fourier domain. The random mask is used in random sampling scheme. Having a\nbound on the maximum value of a random mask in Fourier domain is very useful\nfor some iterative recovery methods that use thresholding operator. In this\npaper, we propose some different bounds and compare them with the empirical\nexamples.\n", "category": [1]}
{"abstract": "  This paper proposes an active radio frequency (RF) cancellation solution to\nsuppress the transmitter (TX) passband leakage signal in radio transceivers\nsupporting simultaneous transmission and reception. The proposed technique is\nbased on creating an opposite-phase baseband equivalent replica of the TX\nleakage signal in the transceiver digital front-end through adaptive nonlinear\nfiltering of the known transmit data, to facilitate highly accurate\ncancellation under a nonlinear TX power amplifier (PA). The active RF\ncancellation is then accomplished by employing an auxiliary transmitter chain,\nto generate the actual RF cancellation signal, and combining it with the\nreceived signal at the receiver (RX) low noise amplifier (LNA) input. A\nclosed-loop parameter learning approach, based on the decorrelation principle,\nis also developed to efficiently estimate the coefficients of the nonlinear\ncancellation filter in the presence of a nonlinear TX PA with memory, finite\npassive isolation, and a nonlinear RX LNA. The performance of the proposed\ncancellation technique is evaluated through comprehensive RF measurements\nadopting commercial LTE-Advanced transceiver hardware components. The results\nshow that the proposed technique can provide an additional suppression of up to\n54 dB for the TX passband leakage signal at the RX LNA input, even at\nconsiderably high transmit power levels and with wide transmission bandwidths.\nSuch novel cancellation solution can therefore substantially improve the TX-RX\nisolation, hence reducing the requirements on passive isolation and RF\ncomponent linearity, as well as increasing the efficiency and flexibility of\nthe RF spectrum use in the emerging 5G radio networks.\n", "category": [1]}
{"abstract": "  Visible light communication (VLC) is an emerging technique that uses\nlight-emitting diodes (LED) to combine communication and illumination. It is\nconsidered as a promising scheme for indoor wireless communication that can be\ndeployed at reduced costs while offering high data rate performance. In this\npaper, we focus on the design of the downlink of a multi-user VLC system.\nInherent to multi-user systems is the interference caused by the broadcast\nnature of the medium. Linear precoding based schemes are among the most popular\nsolutions that have recently been proposed to mitigate inter-user interference.\nThis paper focuses on the design of the optimal linear precoding scheme that\nsolves the max-min signal-to-interference-plus-noise ratio (SINR) problem. The\nperformance of the proposed precoding scheme is studied under different working\nconditions and compared with the classical zero-forcing precoding. Simulations\nhave been provided to illustrate the high gain of the proposed scheme.\n", "category": [1]}
{"abstract": "  Channel estimation and optimal training sequence design for full-duplex\none-way relays are investigated. We propose a training scheme to estimate the\nresidual self-interference (RSI) channel and the channels between nodes\nsimultaneously. A maximum likelihood estimator is implemented with\nBroyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm. In the presence of RSI, the\noverall source-to-destination channel becomes an inter-symbol-interference\n(ISI) channel. With the help of estimates of the RSI channel, the destination\nis able to cancel the ISI through equalization. We derive and analyze the\nCramer-Rao bound (CRB) in closed-form by using the asymptotic properties of\nToeplitz matrices. The optimal training sequence is obtained by minimizing the\nCRB. Extensions for the fundamental one-way relay model to the\nfrequency-selective fading channels and the multiple relays case are also\nconsidered. For the former, we propose a training scheme to estimate the\noverall channel, and for the latter the CRB and the optimal number of relays\nare derived when the distance between the source and the destination is fixed.\nSimulations using LTE parameters corroborate our theoretical results.\n", "category": [1]}
{"abstract": "  Generating music has a few notable differences from generating images and\nvideos. First, music is an art of time, necessitating a temporal model. Second,\nmusic is usually composed of multiple instruments/tracks with their own\ntemporal dynamics, but collectively they unfold over time interdependently.\nLastly, musical notes are often grouped into chords, arpeggios or melodies in\npolyphonic music, and thereby introducing a chronological ordering of notes is\nnot naturally suitable. In this paper, we propose three models for symbolic\nmulti-track music generation under the framework of generative adversarial\nnetworks (GANs). The three models, which differ in the underlying assumptions\nand accordingly the network architectures, are referred to as the jamming\nmodel, the composer model and the hybrid model. We trained the proposed models\non a dataset of over one hundred thousand bars of rock music and applied them\nto generate piano-rolls of five tracks: bass, drums, guitar, piano and strings.\nA few intra-track and inter-track objective metrics are also proposed to\nevaluate the generative results, in addition to a subjective user study. We\nshow that our models can generate coherent music of four bars right from\nscratch (i.e. without human inputs). We also extend our models to human-AI\ncooperative music generation: given a specific track composed by human, we can\ngenerate four additional tracks to accompany it. All code, the dataset and the\nrendered audio samples are available at https://salu133445.github.io/musegan/ .\n", "category": [1, 0, 0, 0, 6]}
{"abstract": "  In contrast with traditional video, omnidirectional video enables spherical\nviewing direction with support for head-mounted displays, providing an\ninteractive and immersive experience. Unfortunately, to the best of our\nknowledge, there are few visual quality assessment (VQA) methods, either\nsubjective or objective, for omnidirectional video coding. This paper proposes\nboth subjective and objective methods for assessing quality loss in encoding\nomnidirectional video. Specifically, we first present a new database, which\nincludes the viewing direction data from several subjects watching\nomnidirectional video sequences. Then, from our database, we find a high\nconsistency in viewing directions across different subjects. The viewing\ndirections are normally distributed in the center of the front regions, but\nthey sometimes fall into other regions, related to video content. Given this\nfinding, we present a subjective VQA method for measuring difference mean\nopinion score (DMOS) of the whole and regional omnidirectional video, in terms\nof overall DMOS (O-DMOS) and vectorized DMOS (V-DMOS), respectively. Moreover,\nwe propose two objective VQA methods for encoded omnidirectional video, in\nlight of human perception characteristics of omnidirectional video. One method\nweighs the distortion of pixels with regard to their distances to the center of\nfront regions, which considers human preference in a panorama. The other method\npredicts viewing directions according to video content, and then the predicted\nviewing directions are leveraged to allocate weights to the distortion of each\npixel in our objective VQA method. Finally, our experimental results verify\nthat both the subjective and objective methods proposed in this paper advance\nstate-of-the-art VQA for omnidirectional video.\n", "category": [1]}
{"abstract": "  This work presents an experimental study of Single Input Multiple Output\n(SIMO) channel performance in indoor radio propagation environment. Indoor\nchannel measurements at 2.4 GHz ISM frequency band have been performed using a\nversatile channel sounder testbed platform. A single transmitting antenna, four\nreceiving antennas with two proposed geometries and a four-branch receiver\ncircuitry were used in order to achieve channel sounder measurements exploiting\nbaseband signal processing techniques. Deep investigation on SIMO wireless\nchannel performance was realized through three types of metrics which are\nsignal strength, gain coefficient and capacity. Performance results indicate\nSIMO channel capacity enhancement and illustrate differences between the two\nproposed geometries.\n", "category": [1, 3]}
{"abstract": "  Delay and Sum (DAS), as the most common beamforming algorithm in\nPhotoacoustic Imaging (PAI), having a simple implementation, results in a\nlow-quality image. Delay Multiply and Sum (DMAS) was introduced to improve the\nquality of the reconstructed images using DAS. However, the resolution\nimprovement is now well enough compared to high resolution adaptive\nreconstruction methods such as Eigenspace- Based Minimum Variance (EIBMV). We\nproposed to integrate the EIBMV inside the DMAS formula by replacing the\nexisting DAS algebra inside the expansion of DMAS, called EIBMV-DMAS. It is\nshown that EIBMV-DMAS outperforms DMAS in the terms of levels of sidelobes and\nwidth of mainlobe significantly. For instance, at the depth of 35 mm,\nEIBMV-DMAS outperforms DMAS and EIBMV in the term of sidelobes for about 108\ndB, 98 dB and 44 dB compared to DAS, DMAS, and EIBMV, respectively. The\nquantitative comparison has been conducted using Full-Width-Half-Maximum (FWHM)\nand Signal-to-Noise Ratio (SNR), and it was shown that EIBMV-DMAS reduces the\nFWHM about 1.65 mm and improves the SNR about 15 dB, compared to DMAS.\n", "category": [1, 0, 2, 3]}
{"abstract": "  It is shown how binary sequences can be associated with automatic composition\nof monophonic pieces. We are concerned with the composition of e-music from\nfinite field structures. The information at the input may be either random or\ninformation from a black-and-white, grayscale or color picture. New\ne-compositions and music score are made available, including a new piece from\nthe famous Lenna picture: the score of the e-music <<Between Lenna's eyes in C\nmajor.>> The corresponding stretch of music score are presented. Some\nparticular structures, including clock arithmetic (mod 12), GF(7), GF(8),\nGF(13) and GF(17) are addressed. Further, multilevel block-codes are also used\nin a new approach of e-music composition, engendering a particular style as an\ne-composer. As an example, Pascal multilevel block codes recently introduced\nare handled to generate a new style of electronic music over GF(13).\n", "category": [0, 1]}
{"abstract": "  Smart grid is an energy infrastructure that increases energy efficiency by\nusing communication infrastructure, smart meters, smart appliances, automated\ncontrol and networking, and more. This paper focuses on the Power Line\nCommunication (PLC) aspect and technologies used in the smart grid. There are\nvarious challenges and advancements in the smart grid; this research discusses\nhow PLC can improve smart grid performance. In order to provide applicable\nresults, practical PLC system parameters and other required data was obtained\nfrom Florida Power and Light (FPL). Modeling of the PLC system with different\ntypes of digital modulations was conducted using MATLAB/Simulink software and\nPython. The benefits and design tradeoffs of Amplitude Shift Keying (ASK),\nFrequency Shift Keying (FSK), and Phase Shift Keying (PSK) are discussed. The\nmodulation schemes are compared on the basis of their applicability to a\npractical PLC network by comparing the results of the simulations\n", "category": [1]}
{"abstract": "  We consider designing a robust structured sparse sensing matrix consisting of\na sparse matrix with a few non-zero entries per row and a dense base matrix for\ncapturing signals efficiently We design the robust structured sparse sensing\nmatrix through minimizing the distance between the Gram matrix of the\nequivalent dictionary and the target Gram of matrix holding small mutual\ncoherence. Moreover, a regularization is added to enforce the robustness of the\noptimized structured sparse sensing matrix to the sparse representation error\n(SRE) of signals of interests. An alternating minimization algorithm with\nglobal sequence convergence is proposed for solving the corresponding\noptimization problem. Numerical experiments on synthetic data and natural\nimages show that the obtained structured sensing matrix results in a higher\nsignal reconstruction than a random dense sensing matrix.\n", "category": [1, 0]}
{"abstract": "  Transformers and transmission lines are critical components of a grid\nnetwork. This paper analyzes the statistical properties of the electrical\nparameters of transmission branches and especially examines their\ninterdependence on the voltage levels. Some interesting findings include: (a)\nwith appropriate conversion of MVA rating, a transformers per unit reactance\nexhibits consistent statistical pattern independent of voltage levels and\ncapacity; (b) the distributed reactance (ohms/km) of transmission lines also\nhas some consistent patterns regardless of voltage levels; (c) other parameters\nsuch as the branch resistance, the MVA ratings, the transmission line length,\netc, manifest strong interdependence on the voltage levels which can be\napproximated by a power function with different power constants. The results\nwill be useful in both creation of synthetic power grid test cases and\nvalidation of existing grid models.\n", "category": [1, 3, 3]}
{"abstract": "  Advances in CMOS technology have made high resolution image sensors possible.\nThese image sensor pose significant challenges in terms of the amount of raw\ndata generated, energy efficiency and frame rate. This paper presents a new\ndesign methodology for an imaging system and a simplified novel image sensor\npixel design to be used in such system so that Compressed Sensing (CS)\ntechnique can be implemented easily at the sensor level. This results in\nsignificant energy savings as it not only cuts the raw data rate but also\nreduces transistor count per pixel, decreases pixel size, increases fill\nfactor, simplifies ADC, JPEG encoder and JPEG decoder design and decreases\nwiring as well as address decoder size by half. Thus CS has the potential to\nincrease the resolution of image sensors for a given technology and die size\nwhile significantly decreasing the power consumption and design complexity. We\nshow that it has potential to reduce power consumption by about 23%-65%.\n", "category": [1]}
{"abstract": "  Covert communication conceals the transmission of the message from an\nattentive adversary. Recent work on the limits of covert communication in\nadditive white Gaussian noise (AWGN) channels has demonstrated that a covert\ntransmitter (Alice) can reliably transmit a maximum of\n$\\mathcal{O}\\left(\\sqrt{n}\\right)$ bits to a covert receiver (Bob) without\nbeing detected by an adversary (Warden Willie) in $n$ channel uses. This paper\nfocuses on the scenario where other friendly nodes distributed according to a\ntwo-dimensional Poisson point process with density $m$ are present in the\nenvironment. We propose a strategy where the friendly node closest to the\nadversary, without close coordination with Alice, produces artificial noise. We\nshow that this method allows Alice to reliably and covertly send\n$\\mathcal{O}(\\min\\{{n,m^{\\gamma/2}\\sqrt{n}}\\})$ bits to Bob in $n$ channel\nuses, where $\\gamma$ is the path-loss exponent. Moreover, we also consider a\nsetting where there are $N_{\\mathrm{w}}$ collaborating adversaries uniformly\nand randomly located in the environment and show that in $n$ channel uses,\nAlice can reliably and covertly send\n$\\mathcal{O}\\left(\\min\\left\\{n,\\frac{m^{\\gamma/2}\n\\sqrt{n}}{N_{\\mathrm{w}}^{\\gamma}}\\right\\}\\right)$ bits to Bob when $\\gamma\n>2$, and $\\mathcal{O}\\left(\\min\\left\\{n,\\frac{m\n\\sqrt{n}}{N_{\\mathrm{w}}^{2}\\log^2 {N_{\\mathrm{w}}}}\\right\\}\\right)$ when\n$\\gamma = 2$. Conversely, we demonstrate that no higher covert throughput is\npossible for $\\gamma>2$.\n", "category": [0, 0, 0, 1, 2]}
{"abstract": "  In time-division duplexing (TDD) systems, massive multiple-input\nmultiple-output (MIMO) relies on the channel reciprocity to obtain the downlink\n(DL) channel state information (CSI) with the uplink (UL) CSI. In practice, the\nmismatches in the radio frequency (RF) analog circuits among different antennas\nat the base station (BS) break the end-to-end UL and DL channel reciprocity.\nAntenna calibration is necessary to avoid the severe performance degradation\nwith massive MIMO. Many calibration schemes are available to compensate the RF\ngain mismatches and restore the channel reciprocity in TDD massive MIMO\nsystems. In this paper, we focus on the internal self-calibration scheme where\ndifferent BS antennas are interconnected via hardware transmission lines.\nFirst, we study the resulting calibration performance for an arbitrary\ninterconnection strategy. Next, we obtain closed-form Cramer-Rao lower bound\n(CRLB) expressions for each interconnection strategy at the BS with only (M-1)\ntransmission lines and M denotes the total number of BS antennas. Basing on the\nderived results, we further prove that the star interconnection strategy is\noptimal for internal self-calibration due to its lowest CRLB. In addition, we\nalso put forward efficient recursive algorithms to derive the corresponding\nmaximum-likelihood (ML) estimates of all the calibration coefficients.\nNumerical simulation results are also included to corroborate our theoretical\nanalyses and results.\n", "category": [1]}
{"abstract": "  In this paper, a recently proposed approach to multizone sound field\nsynthesis, referred to as Joint Pressure and Velocity Matching (JPVM), is\ninvestigated analytically using a spherical harmonics representation of the\nsound field. The approach is motivated by the Kirchhoff-Helmholtz integral\nequation and aims at controlling the sound field inside the local listening\nzones by evoking the sound pressure and particle velocity on surrounding\ncontours. Based on the findings of the modal analysis, an improved version of\nJPVM is proposed which provides both better performance and lower complexity.\nIn particular, it is shown analytically that the optimization of the tangential\ncomponent of the particle velocity vector, as is done in the original JPVM\napproach, is very susceptible to errors and thus not pursued anymore. The\nanalysis furthermore provides fundamental insights as to how the spherical\nharmonics used to describe the 3D variant sound field translate into 2D basis\nfunctions as observed on the contours surrounding the zones. By means of\nsimulations, it is verified that discarding the tangential component of the\nparticle velocity vector ultimately leads to an improved performance. Finally,\nthe impact of sensor noise on the reproduction performance is assessed.\n", "category": [1, 0]}
{"abstract": "  The reproducibility issue in science has come under increased scrutiny. One\nconsistent suggestion lies in the use of scripted methods or workflows for data\nanalysis. Image analysis is one area in science in which little can be done in\nscripted methods. The SWIIM Project (Scripted Workflows to Improve Image\nManipulation) is designed to generate workflows from popular image manipulation\ntools. In the project, 2 approaches are being taken to construct workflows in\nthe image analysis area. First, the open-source tool GIMP is being enhanced to\nproduce an active log (which can be run on a stand-alone basis to perform the\nsame manipulation). Second, the R system Shiny tool is being used to construct\na graphical user interface (GUI) which works with EBImage code to modify\nimages, and to produce an active log which can perform the same operations.\nThis process has been successful to date, but is not complete. The basic method\nfor each component is discussed, and example code is shown.\n", "category": [1]}
{"abstract": "  The construction quality of the bolt is directly related to the safety of the\nproject, and as such, it must be tested. In this paper, the improved complete\nensemble empirical mode decomposition (ICEEMD) method is introduced to the bolt\ndetection signal analysis. The ICEEMD is used in order to decompose the anchor\ndetection signal according to the approximate entropy of each intrinsic mode\nfunction (IMF). The noise of the IMFs is eliminated by the wavelet soft\nthreshold de-noising technique. Based on the approximate entropy, and the\nwavelet de-noising principle, the ICEEMD-De anchor signal analysis method is\nproposed. From the analysis of the vibration analog signal, as well as the bolt\ndetection signal, the result shows that the ICEEMD-De method is capable of\ncorrectly separating the different IMFs under noisy conditions, and also that\nthe IMF can effectively identify the reflection signal of the end of the bolt.\n", "category": [1]}
{"abstract": "  The aim of this project was to develop and implement an English language\nText-to-Speech synthesis system. This involved a study of mechanisms of human\nspeech production, a review of techniques in speech synthesis, and analysis of\ntests used to evaluate the effectiveness of synthesized speech. It was\ndetermined that a diphone synthesis system was the most effective choice for\nthe scope of this project. A method of automatically identifying and extracting\ndiphones from prompted speech was designed, allowing for the creation of a\ndiphone database by a speaker in less than 40 minutes. CMUdict was used to\ndetermine the pronunciation of known words. A system for smoothing the\ntransitions between diphone recordings was designed and implemented. CMUdict\nwas then used to train a maximum-likelihood prediction system to determine the\ncorrect pronunciation of unknown English language alphabetic words. Then, a\nPart Of Speech tagger was designed to find the lexical class of words within a\nsentence.\n  A method of altering the pitch, duration, and volume of the produced voice\nover time was designed, being a combination of the TD-PSOLA algorithm and a\nnovel approach referred to as Unvoiced Speech Duration Shifting. This minimises\ndistortion of the voice when shifting the pitch or duration, while maximising\ncomputational efficiency by operating in the time domain. This approach was\nused to add correct lexical stress to vowels within words. A text tokenisation\nsystem was developed to handle arbitrary text input, allowing pronunciation of\nnumerical input tokens and use of appropriate pauses for punctuation. Methods\nfor further improving sentence level speech naturalness were discussed.\nFinally, the system was tested with listeners for its intelligibility and\nnaturalness.\n", "category": [0, 1]}
{"abstract": "  Next generation wireless networks aim at providing substantial improvements\nin spectral efficiency (SE) and energy efficiency (EE). Massive MIMO has been\nproved to be a viable technology to achieve these goals by spatially\nmultiplexing several users using many base station (BS) antennas. A potential\nlimitation of Massive MIMO in multicell systems is pilot contamination, which\narises in the channel estimation process from the interference caused by\nreusing pilots in neighboring cells. A standard method to reduce pilot\ncontamination, known as regular pilot (RP), is to adjust the length of pilot\nsequences while transmitting data and pilot symbols disjointly. An alternative\nmethod, called superimposed pilot (SP), sends a superposition of pilot and data\nsymbols. This allows to use longer pilots which, in turn, reduces pilot\ncontamination. We consider the uplink of a multicell Massive MIMO network using\nmaximum ratio combining detection and compare RP and SP in terms of SE and EE.\nTo this end, we derive rigorous closed-form achievable rates with SP under a\npractical random BS deployment. We prove that the reduction of pilot\ncontamination with SP is outweighed by the additional coherent and non-coherent\ninterference. Numerical results show that when both methods are optimized, RP\nachieves comparable SE and EE to SP in practical scenarios.\n", "category": [1]}
{"abstract": "  We investigate an existing distributed algorithm for learning sparse signals\nor data over networks. The algorithm is iterative and exchanges intermediate\nestimates of a sparse signal over a network. This learning strategy using\nexchange of intermediate estimates over the network requires a limited\ncommunication overhead for information transmission. Our objective in this\narticle is to show that the strategy is good for learning in spite of limited\ncommunication. In pursuit of this objective, we first provide a restricted\nisometry property (RIP)-based theoretical analysis on convergence of the\niterative algorithm. Then, using simulations, we show that the algorithm\nprovides competitive performance in learning sparse signals vis-a-vis an\nexisting alternate distributed algorithm. The alternate distributed algorithm\nexchanges more information including observations and system parameters.\n", "category": [6, 1]}
{"abstract": "  Single-pixel imaging is an indirect imaging technique which utilizes\nsimplified optical hardware and advanced computational methods. It offers novel\nsolutions for hyper-spectral imaging, polarimetric imaging, three-dimensional\nimaging, holographic imaging, optical encryption and imaging through scattering\nmedia. The main limitations for its use come from relatively high measurement\nand reconstruction times. In this paper we propose to reduce the required\nsignal acquisition time by using a novel sampling scheme based on a random\nselection of Morlet wavelets convolved with white noise. While such functions\nexhibit random properties, they are locally determined by Morlet wavelet\nparameters. The proposed method is equivalent to random sampling of the\nproperly selected part of the feature space, which maps the measured images\naccurately both in the spatial and spatial frequency domains. We compare both\nnumerically and experimentally the image quality obtained with our sampling\nprotocol against widely-used sampling with Walsh-Hadamard or noiselet\nfunctions. The results show considerable improvement over the former methods,\nenabling single-pixel imaging at low compression rates on the order of a few\npercent.\n", "category": [1, 0]}
{"abstract": "  Large-scale data collection by means of wireless sensor network and\ninternet-of-things technology poses various challenges in view of the\nlimitations in transmission, computation, and energy resources of the\nassociated wireless devices. Compressive data gathering based on compressed\nsensing has been proven a well-suited solution to the problem. Existing designs\nexploit the spatiotemporal correlations among data collected by a specific\nsensing modality. However, many applications, such as environmental monitoring,\ninvolve collecting heterogeneous data that are intrinsically correlated. In\nthis study, we propose to leverage the correlation from multiple heterogeneous\nsignals when recovering the data from compressive measurements. To this end, we\npropose a novel recovery algorithm---built upon belief-propagation\nprinciples---that leverages correlated information from multiple heterogeneous\nsignals. To efficiently capture the statistical dependencies among diverse\nsensor data, the proposed algorithm uses the statistical model of copula\nfunctions. Experiments with heterogeneous air-pollution sensor measurements\nshow that the proposed design provides significant performance improvements\nagainst state-of-the-art compressive data gathering and recovery schemes that\nuse classical compressed sensing, compressed sensing with side information, and\ndistributed compressed sensing.\n", "category": [1]}
{"abstract": "  The article presents the experimental measurements of the amplitude of the\nmoir\\'e patterns in a digital autostereoscopic barrier-type 3D display across a\nwide angular range with a small increment. The period and orientation of the\nmoir\\'e patterns were also measured as functions of the angle. Simultaneous\nbranches are observed and analyzed. The theoretical interpretation is also\ngiven. The results can help preventing or minimizing the moir\\'e effect in\ndisplays.\n", "category": [1, 0]}
{"abstract": "  Fourier ptychographic microscopy (FPM) is a computational imaging technique\nwith both high resolution and large field-of-view. However, the effective\nnumerical aperture (NA) achievable with a typical LED panel is ambiguous and\nusually relies on the repeated tests of different illumination NAs. The imaging\nquality of each raw image usually depends on the visual assessments, which is\nsubjective and inaccurate especially for those dark field images. Moreover, the\nacquisition process is really time-consuming.In this paper, we propose a\nSNR-based adaptive acquisition method for quantitative evaluation and adaptive\ncollection of each raw image according to the signal-to-noise ration (SNR)\nvalue, to improve the FPM's acquisition efficiency and automatically obtain the\nmaximum achievable NA, reducing the time of collection, storage and subsequent\ncalculation. The widely used EPRY-FPM algorithm is applied without adding any\nalgorithm complexity and computational burden. The performance has been\ndemonstrated in both USAF targets and biological samples with different imaging\nsensors respectively, which have either Poisson or Gaussian noises model.\nFurther combined with the sparse LEDs strategy, the number of collection images\ncan be shorten to around 25 frames while the former needs 361 images, the\nreduction ratio can reach over 90%. This method will make FPM more practical\nand automatic, and can also be used in different configurations of FPM.\n", "category": [1, 3]}
{"abstract": "  We present and discuss different algorithms for converting rectangular\nimagery into elliptical regions. We mainly focus on methods that use\nmathematical mappings with explicit and invertible equations. The key idea is\nto start with invertible mappings between the square and the circular disc then\nextend it to handle rectangles and ellipses. This extension can be done by\nsimply removing the eccentricity and reintroducing it back after using a chosen\nsquare-to-disc mapping.\n", "category": [1, 0]}
{"abstract": "  We present a factorized hierarchical variational autoencoder, which learns\ndisentangled and interpretable representations from sequential data without\nsupervision. Specifically, we exploit the multi-scale nature of information in\nsequential data by formulating it explicitly within a factorized hierarchical\ngraphical model that imposes sequence-dependent priors and sequence-independent\npriors to different sets of latent variables. The model is evaluated on two\nspeech corpora to demonstrate, qualitatively, its ability to transform speakers\nor linguistic content by manipulating different sets of latent variables; and\nquantitatively, its ability to outperform an i-vector baseline for speaker\nverification and reduce the word error rate by as much as 35% in mismatched\ntrain/test scenarios for automatic speech recognition tasks.\n", "category": [0, 0, 0, 1, 6]}
{"abstract": "  Convolutive Non-Negative Matrix Factorization model factorizes a given audio\nspectrogram using frequency templates with a temporal dimension. In this paper,\nwe present a convolutional auto-encoder model that acts as a neural network\nalternative to convolutive NMF. Using the modeling flexibility granted by\nneural networks, we also explore the idea of using a Recurrent Neural Network\nin the encoder. Experimental results on speech mixtures from TIMIT dataset\nindicate that the convolutive architecture provides a significant improvement\nin separation performance in terms of BSSeval metrics.\n", "category": [0, 1]}
{"abstract": "  In Photoacoustic imaging (PA), Delay-and-Sum (DAS) beamformer is a common\nbeamforming algorithm having a simple implementation. However, it results in a\npoor resolution and high sidelobes. To address these challenges, a new\nalgorithm namely Delay-Multiply-and-Sum (DMAS) was introduced having lower\nsidelobes compared to DAS. To improve the resolution of DMAS, a novel\nbeamformer is introduced using Minimum Variance (MV) adaptive beamforming\ncombined with DMAS, so-called Minimum Variance-Based DMAS (MVB-DMAS). It is\nshown that expanding the DMAS equation results in multiple terms representing a\nDAS algebra. It is proposed to use the MV adaptive beamformer instead of the\nexisting DAS. MVB-DMAS is evaluated numerically and experimentally. In\nparticular, at the depth of 45 mm MVB-DMAS results in about 31 dB, 18 dB and 8\ndB sidelobes reduction compared to DAS, MV and DMAS, respectively. The\nquantitative results of the simulations show that MVB-DMAS leads to improvement\nin full-width-half-maximum about 96 %, 94 % and 45 % and signal-to-noise ratio\nabout 89 %, 15 % and 35 % compared to DAS, DMAS, MV, respectively. In\nparticular, at the depth of 33 mm of the experimental images, MVB-DMAS results\nin about 20 dB sidelobes reduction in comparison with other beamformers.\n", "category": [1, 0, 2]}
{"abstract": "  With the increase in awareness about the climate change, there has been a\ntremendous shift towards utilizing renewable energy sources (RES). In this\nregard, smart grid technologies have been presented to facilitate higher\npenetration of RES. Microgrids are the key components of the smart grids.\nMicrogrids allow integration of various distributed energy resources (DER) such\nas the distributed generation (DGs) and energy storage systems (ESSs) into the\ndistribution system and hence remove or delay the need for distribution\nexpansion. One of the crucial requirements for utilities is to ensure that the\nsystem reliability is maintained with the inclusion of microgrid topology.\nTherefore, this paper evaluates the reliability of a microgrid containing\nprioritized loads and distributed RES through a hybrid analytical-simulation\nmethod. The stochasticity of RES introduces complexity to the reliability\nevaluation. The method takes into account the variability of RES through Monte-\nCarlo state sampling simulation. The results indicate the reliability\nenhancement of the overall system in the presence of the microgrid topology. In\nparticular, the highest priority load has the largest improvement in the\nreliability indices. Furthermore, sensitivity analysis is performed to\nunderstand the effects of the failure of microgrid islanding in the case of a\nfault in the upstream network.\n", "category": [1]}
{"abstract": "  As the connectivity of consumer devices is rapidly growing and cloud\ncomputing technologies are becoming more widespread, cloud-aided techniques for\nparameter estimation can be designed to exploit the theoretically unlimited\nstorage memory and computational power of the cloud, while relying on\ninformation provided by multiple sources. With the ultimate goal of developing\nmonitoring and diagnostic strategies, this report focuses on the design of a\nRecursive Least-Squares (RLS) based estimator for identification over a group\nof devices connected to the cloud. The proposed approach, that relies on\nNode-to-Cloud-to-Node (N2C2N) transmissions, is designed so that: (i) estimates\nof the unknown parameters are computed locally and (ii) the local estimates are\nrefined on the cloud. The proposed approach requires minimal changes to local\n(pre-existing) RLS estimators.\n", "category": [0, 0, 0, 1, 2]}
{"abstract": "  A method for statistical parametric speech synthesis incorporating generative\nadversarial networks (GANs) is proposed. Although powerful deep neural networks\n(DNNs) techniques can be applied to artificially synthesize speech waveform,\nthe synthetic speech quality is low compared with that of natural speech. One\nof the issues causing the quality degradation is an over-smoothing effect often\nobserved in the generated speech parameters. A GAN introduced in this paper\nconsists of two neural networks: a discriminator to distinguish natural and\ngenerated samples, and a generator to deceive the discriminator. In the\nproposed framework incorporating the GANs, the discriminator is trained to\ndistinguish natural and generated speech parameters, while the acoustic models\nare trained to minimize the weighted sum of the conventional minimum generation\nloss and an adversarial loss for deceiving the discriminator. Since the\nobjective of the GANs is to minimize the divergence (i.e., distribution\ndifference) between the natural and generated speech parameters, the proposed\nmethod effectively alleviates the over-smoothing effect on the generated speech\nparameters. We evaluated the effectiveness for text-to-speech and voice\nconversion, and found that the proposed method can generate more natural\nspectral parameters and $F_0$ than conventional minimum generation error\ntraining algorithm regardless its hyper-parameter settings. Furthermore, we\ninvestigated the effect of the divergence of various GANs, and found that a\nWasserstein GAN minimizing the Earth-Mover's distance works the best in terms\nof improving synthetic speech quality.\n", "category": [0, 0, 1]}
{"abstract": "  We present the modeling and characterization of a time-reversal routing\ndispersion code multiple access (TR-DCMA) system. We show that this system\nmaintains the low complexity advantage of DCMA transceivers while offering\ndynamic adaptivity for practial communication scenarios. We first derive the\nmathematical model and explain operation principles of the system, and then\ncharacterize its interference, signal to interference ratio, and bit error\nprobability characteristics.\n", "category": [1]}
{"abstract": "  In this paper, we discuss the design rationale and guidelines to build\nmagnet-less circulators based on spatio-temporal modulation of resonant\njunctions consisting of first-order bandstop filters connected in a delta\ntopology. Without modulation, the junction does not allow transmission between\nits ports, however, when the natural oscillation frequencies of the constituent\nLC filters are modulated in time with a suitable phase pattern, a synthetic\nangular-momentum bias can be effectively imparted to the junction and a\ntransmission window opens at one of the output ports, thus realizing a\ncirculator. We develop a rigorous small-signal linear model and find analytical\nexpressions for the harmonic S-parameters of the proposed circuit, which\nsignificantly facilitate the design process. We validate the theory with\nsimulations and further discuss the large signal response, including power\nhandling and non-linearity, and the noise performance. Finally, we present\nmeasured results with unprecedented performance in all metrics for a PCB\nprototype using a Rogers board and off-the-shelf discrete components.\n", "category": [1]}
{"abstract": "  In this paper, we present voltage- and current-mode differential magnetless\nnon-reciprocal devices obtained by pairing two single-ended (SE) circulators,\neach consisting of three first-order bandpass or bandstop LC filters, connected\nin either a wye or a delta topology. The resonant poles of each SE circulator\nare modulated in time with 120 deg phase-shifted periodic signals, resulting in\nsynthetic angular-momentum biasing achieved through spatiotemporal modulation\n(STM). We tailor the two SE circulators to exhibit a constant 180 deg phase\ndifference between their STM biases. Unlike conventional differential\ntime-variant circuits, for which only the even or odd spurs are rejected, we\nshow that the proposed configuration cancels out all intermodulation (IM)\nproducts, thus making them operate alike linear time-invariant (LTI) circuits\nfor an external observer. In turn, this property enhances all metrics of the\nresulting circulator, overcoming the limitations of SE architectures, and\nimproving insertion loss, impedance matching, bandwidth and noise figure. We\nshow that this differential architecture also significantly relaxes the\nrequired modulation parameters, both in frequency and amplitude. We develop a\nrigorous small-signal model to guide the design of the proposed circuits and to\nget insights into their pseudo-LTI characteristics. Then, we validate the\ntheory with simulations and measurements showing remarkable performance\ncompared to the current state of the art of magnetless non-reciprocal devices.\n", "category": [1]}
{"abstract": "  In this paper, we present on-sensor neuromorphic vision hardware\nimplementation of denoising spatial filter. The mean or median spatial filters\nwith fixed window shape are known for its denoising ability, however, have the\ndrawback of blurring the object edges. The effect of blurring increases with an\nincrease in window size. To preserve the edge information, we propose an\nadaptive spatial filter that uses neuron's ability to detect similar pixels and\ncalculates the mean. The analog input differences of neighborhood pixels are\nconverted to the chain of pulses with voltage controlled oscillator and applied\nas neuron input. When the input pulses charge the neuron to equal or greater\nlevel than its threshold, the neuron will fire, and pixels are identified as\nsimilar. The sequence of the neuron's responses for pixels is stored in the\nserial-in-parallel-out shift register. The outputs of shift registers are used\nas input to the selector switches of an averaging circuit making this an\nadaptive mean operation resulting in an edge preserving mean filter. System\nlevel simulation of the hardware is conducted using 150 images from Caltech\ndatabase with added Gaussian noise to test the robustness of edge-preserving\nand denoising ability of the proposed filter. Threshold values of the hardware\nneuron were adjusted so that the proposed edge-preserving spatial filter\nachieves optimal performance in terms of PSNR and MSE, and these results\noutperforms that of the conventional mean and median filters.\n", "category": [1]}
{"abstract": "  Inferring information from a set of acquired data is the main objective of\nany signal processing (SP) method. In particular, the common problem of\nestimating the value of a vector of parameters from a set of noisy measurements\nis at the core of a plethora of scientific and technological advances in the\nlast decades; for example, wireless communications, radar and sonar,\nbiomedicine, image processing, and seismology, just to name a few. Developing\nan estimation algorithm often begins by assuming a statistical model for the\nmeasured data, i.e. a probability density function (pdf) which if correct,\nfully characterizes the behaviour of the collected data/measurements.\nExperience with real data, however, often exposes the limitations of any\nassumed data model since modelling errors at some level are always present.\nConsequently, the true data model and the model assumed to derive the\nestimation algorithm could differ. When this happens, the model is said to be\nmismatched or misspecified. Therefore, understanding the possible performance\nloss or regret that an estimation algorithm could experience under model\nmisspecification is of crucial importance for any SP practitioner. Further,\nunderstanding the limits on the performance of any estimator subject to model\nmisspecification is of practical interest. Motivated by the widespread and\npractical need to assess the performance of a mismatched estimator, the goal of\nthis paper is to help to bring attention to the main theoretical findings on\nestimation theory, and in particular on lower bounds under model\nmisspecification, that have been published in the statistical and econometrical\nliterature in the last fifty years. Secondly, some applications are discussed\nto illustrate the broad range of areas and problems to which this framework\nextends, and consequently the numerous opportunities available for SP\nresearchers.\n", "category": [1]}
{"abstract": "  A counterparty credit limit (CCL) is a limit that is imposed by a financial\ninstitution to cap its maximum possible exposure to a specified counterparty.\nCCLs help institutions to mitigate counterparty credit risk via selective\ndiversification of their exposures. In this paper, we analyze how CCLs impact\nthe prices that institutions pay for their trades during everyday trading. We\nstudy a high-quality data set from a large electronic trading platform in the\nforeign exchange spot market, which enables institutions to apply CCLs. We find\nempirically that CCLs had little impact on the vast majority of trades in this\ndata. We also study the impact of CCLs using a new model of trading. By\nsimulating our model with different underlying CCL networks, we highlight that\nCCLs can have a major impact in some situations.\n", "category": [5, 7, 2, 6, 6]}
{"abstract": "  Despite noise suppression being a mature area in signal processing, it\nremains highly dependent on fine tuning of estimator algorithms and parameters.\nIn this paper, we demonstrate a hybrid DSP/deep learning approach to noise\nsuppression. A deep neural network with four hidden layers is used to estimate\nideal critical band gains, while a more traditional pitch filter attenuates\nnoise between pitch harmonics. The approach achieves significantly higher\nquality than a traditional minimum mean squared error spectral estimator, while\nkeeping the complexity low enough for real-time operation at 48 kHz on a\nlow-power processor.\n", "category": [0, 1]}
{"abstract": "  Compressed air energy storage (CAES) is suitable for large-scale energy\nstorage and can help to increase the penetration of wind power in power\nsystems. A CAES plant consists of compressors, expanders, caverns, and a\nmotor/generator set. Currently used cavern models for CAES are either accurate\nbut highly non-linear or linear but inaccurate. Highly non-linear cavern models\ncannot be directly utilized in power system optimization problems. In this\nregard, an accurate bi-linear cavern model for CAES is proposed in this first\npaper of a two-part series. The charging and discharging processes in a cavern\nare divided into several virtual states and then the first law of\nthermodynamics and ideal gas law are used to derive a cavern model, i.e., model\nfor the variation of temperature and pressure in these processes. Thereafter,\nthe heat transfer between the air in the cavern and the cavern wall is\nconsidered and integrated into the cavern model. By subsequently eliminating\nseveral negligible terms, the cavern model reduces to a bi-linear (linear)\nmodel for CAES with multiple (single) time steps. The accuracy of the proposed\ncavern model is verified via comparison with an accurate non-linear model.\n", "category": [1]}
{"abstract": "  Unit commitment (UC) is one of the most important power system operation\nproblems. To integrate higher penetration of wind power into power systems,\nmore compressed air energy storage (CAES) plants are being built. Existing\ncavern models for the CAES used in power system optimization problems are not\naccurate, which may lead to infeasible solutions, e.g., the air pressure in the\ncavern is outside its operating range. In this regard, an accurate CAES model\nis proposed for the UC problem based on the accurate bi-linear cavern model\nproposed in the first paper of this two-part series. The minimum switch time\nbetween the charging and discharging processes of CAES is considered. The whole\nmodel, i.e., the UC model with an accurate CAES model, is a large-scale mixed\ninteger bi-linear programming problem. To reduce the complexity of the whole\nmodel, three strategies are proposed to reduce the number of bi-linear terms\nwithout sacrificing accuracy. McCormick relaxation and piecewise linearization\nare then used to linearize the whole model. To decrease the solution time, a\nmethod to obtain an initial solution of the linearized model is proposed. A\nmodified RTS-79 system is used to verify the effectiveness of the whole model\nand the solution methodology.\n", "category": [1]}
{"abstract": "  We study multi-user massive multiple-input single-output (MISO) systems and\nfocus on downlink transmission, where the base station (BS) employs a large\nantenna array with low-cost 1-bit digital-to-analog converters (DACs). The\ndirect combination of existing beamforming schemes with 1-bit DACs is shown to\nlead to an error floor at medium-to-high SNR regime, due to the coarse\nquantization of the DACs with limited precision. In this paper, based on the\nconstructive interference we consider both a quantized linear beamforming\nscheme where we analytically obtain the optimal beamforming matrix, and a\nnon-linear mapping scheme where we directly design the transmit signal vector.\nDue to the 1-bit quantization, the formulated optimization for the non-linear\nmapping scheme is shown to be non-convex. To solve this problem, the non-convex\nconstraints of the 1-bit DACs are firstly relaxed, followed by an element-wise\nnormalization to satisfy the 1-bit DAC transmission. We further propose a\nlow-complexity symbol scaling scheme that consists of three stages, in which\nthe quantized transmit signal on each antenna element is selected sequentially.\nNumerical results show that the proposed symbol scaling scheme achieves a\ncomparable performance to the optimization-based non-linear mapping approach,\nwhile its corresponding complexity is negligible compared to that of the\nnon-linear scheme.\n", "category": [1, 0, 2]}
{"abstract": "  Chirping phenomena, in which the instantaneous frequencies of a signal change\nwith time, are abundant in signals related to biological systems. Biosignals\nare non-stationary in nature and the time-frequency analysis is a viable tool\nto analyze them. It is well understood that Gaussian chirplet function is\ncritical in describing chirp signals. Despite the theory of adaptive chirplet\ntransform (ACT) has been established for more than two decades and is well\naccepted in the community of signal processing, application of ACT to\nbio-/biomedical signal analysis is still quite limited, probably because that\nthe power of ACT, as an emerging tool for biosignal analysis, has not yet been\nfully appreciated by the researchers in the field of biomedical engineering. In\nthis paper, we describe a novel ACT algorithm based on the \"coarse-refinement\"\nscheme. Namely, the initial estimate of a chirplet is implemented with the\nmatching-pursuit (MP) algorithm and subsequently it is refined using the\nexpectation-maximization (EM) algorithm, which we coin as MPEM algorithm. We\nemphasize the robustness enhancement of the algorithm in face of noise, which\nis important to biosignal analysis, as they are usually embedded in strong\nbackground noise. We then demonstrate the capability of our algorithm by\napplying it to the analysis of representative biosignals, including visual\nevoked potentials (bioelectrical signals), audible heart sounds and bat\nultrasonic echolocation signals (bioacoustic signals), and human speech. The\nresults show that the MPEM algorithm provides more compact representation of\nsignals under investigation and clearer visualization of their time-frequency\nstructures, indicating considerable promise of ACT in biosignal analysis. The\nMATLAB code repository is hosted on GitHub for free download\n(https://github.com/jiecui/mpact).\n", "category": [1]}
{"abstract": "  This present research investigated the relationship between personal\nimpressions and the acoustic nonverbal communication conveyed by employees\nbeing interviewed. First, we investigated the extent to which different\nconversation topics addressed during the interview induced changes in the\ninterviewees' acoustic parameters. Next, we attempted to predict the observed\nand self-assessed attitudes and body language of the interviewees based on the\nacoustic data. The results showed that topicality caused significant deviations\nin the acoustic parameters statistics, but the ability to predict the personal\nperceptions of the interviewees based on their acoustic non-verbal\ncommunication was relatively independent of topicality, due to the natural\nredundancy inherent in acoustic attributes. Our findings suggest that joint\nmodeling of speech and visual cues may improve the assessment of interviewee\nprofiles.\n", "category": [0, 1]}
{"abstract": "  Compressed sensing is an important problem in many fields of science and\nengineering. It reconstructs signals by finding sparse solutions to\nunderdetermined linear equations. In this work we propose a deterministic and\nnon-parametric algorithm SSD (Shortest-Solution guided Decimation) to construct\nsupport of the sparse solution under the guidance of the dense least-squares\nsolution of the recursively decimated linear equation. The most significant\nfeature of SSD is its insensitivity to correlations in the sampling matrix.\nUsing extensive numerical experiments we show that SSD greatly outperforms\nL1-norm based methods, Orthogonal Least Squares, Orthogonal Matching Pursuit,\nand Approximate Message Passing when the sampling matrix contains strong\ncorrelations. This nice property of correlation tolerance makes SSD a versatile\nand robust tool for different types of real-world signal acquisition tasks.\n", "category": [1, 0, 2, 3]}
{"abstract": "  Discrete wavelet transform of finite-length signals must necessarily handle\nthe signal boundaries. The state-of-the-art approaches treat such boundaries in\na complicated and inflexible way, using special prolog or epilog phases. This\nholds true in particular for images decomposed into a number of scales,\nexemplary in JPEG 2000 coding system. In this paper, the state-of-the-art\napproaches are extended to perform the treatment using a compact streaming\ncore, possibly in multi-scale fashion. We present the core focused on CDF 5/3\nwavelet and the symmetric border extension method, both employed in the JPEG\n2000. As a result of our work, every input sample is visited only once, while\nthe results are produced immediately, i.e. without buffering.\n", "category": [1, 0]}
{"abstract": "  Data traffic demand in cellular networks has been tremendously growing and\nhas led to creating congested RF environment. Accordingly, innovative\napproaches for spectrum sharing have been proposed and implemented to\naccommodate several systems within the same frequency band. Spectrum sharing\nbetween radar and communication systems is one of the important research and\ndevelopment areas. In this paper, we present the fundamental spectrum sharing\nconcepts and technologies, then we provide an updated and comprehensive survey\nof spectrum sharing techniques that have been developed to enable some of the\nwireless communication systems to coexist in the same band as radar systems.\n", "category": [1]}
{"abstract": "  The general theoretical approach to the asymptotic extraction of the signal\nseries from the perturbed signal with the help of Singular Spectrum Analysis\n(briefly, SSA) was already outlined in Nekrutkin 2010, SII, v. 3, 297--319.\n  In this paper we consider the example of such an analysis applied to the\nincreasing exponential signal and the sinusoidal noise. It is proved that if\nthe signal rapidly tends to infinity, then the so-called reconstruction errors\nof SSA do not uniformly tend to zero as the series length tends to infinity.\nMore precisely, in this case any finite number of last terms of the error\nseries do not tend to any finite or infinite values.\n  On the contrary, for the \"discretization\" scheme with the bounded from above\nexponential signal, all elements of the error series tend to zero.\n  This effect shows that the discretization model can be an effective tool in\nthe theoretical SSA considerations with increasing signals.\n", "category": [1, 2]}
{"abstract": "  A fundamental assumption underling any Hypothesis Testing (HT) problem is\nthat the available data follow the parametric model assumed to derive the test\nstatistic. Nevertheless, a perfect match between the true and the assumed data\nmodels cannot be achieved in many practical applications. In all these cases,\nit is advisable to use a robust decision test, i.e. a test whose statistic\npreserves (at least asymptotically) the same probability density function (pdf)\nfor a suitable set of possible input data models under the null hypothesis.\nBuilding upon the seminal work of Kent (1982), in this paper we investigate the\nimpact of the model mismatch in a recurring HT problem in radar signal\nprocessing applications: testing the mean of a set of Complex Elliptically\nSymmetric (CES) distributed random vectors under a possible misspecified,\nGaussian data model. In particular, by using this general misspecified\nframework, a new look to two popular detectors, the Kelly's Generalized\nLikelihood Ration Test (GLRT) and the Adaptive Matched Filter (AMF), is\nprovided and their robustness properties investigated.\n", "category": [1]}
{"abstract": "  Linear-Quadratic-Gaussian (LQG) control is concerned with the design of an\noptimal controller and estimator for linear Gaussian systems with imperfect\nstate information. Standard LQG assumes the set of sensor measurements, to be\nfed to the estimator, to be given. However, in many problems, arising in\nnetworked systems and robotics, one may not be able to use all the available\nsensors, due to power or payload constraints, or may be interested in using the\nsmallest subset of sensors that guarantees the attainment of a desired control\ngoal. In this paper, we introduce the sensing-constrained LQG control problem,\nin which one has to jointly design sensing, estimation, and control, under\ngiven constraints on the resources spent for sensing. We focus on the realistic\ncase in which the sensing strategy has to be selected among a finite set of\npossible sensing modalities. While the computation of the optimal sensing\nstrategy is intractable, we present the first scalable algorithm that computes\na near-optimal sensing strategy with provable sub-optimality guarantees. To\nthis end, we show that a separation principle holds, which allows the design of\nsensing, estimation, and control policies in isolation. We conclude the paper\nby discussing two applications of sensing-constrained LQG control, namely,\nsensing-constrained formation control and resource-constrained robot\nnavigation.\n", "category": [2, 0, 0, 1, 2]}
{"abstract": "  This article reviews recent advances in fixed effect estimation of panel data\nmodels for long panels, where the number of time periods is relatively large.\nWe focus on semiparametric models with unobserved individual and time effects,\nwhere the distribution of the outcome variable conditional on covariates and\nunobserved effects is specified parametrically, while the distribution of the\nunobserved effects is left unrestricted. Compared to existing reviews on long\npanels (Arellano and Hahn 2007; a section in Arellano and Bonhomme 2011) we\ndiscuss models with both individual and time effects, split-panel Jackknife\nbias corrections, unbalanced panels, distribution and quantile effects, and\nother extensions. Understanding and correcting the incidental parameter bias\ncaused by the estimation of many fixed effects is our main focus, and the\nunifying theme is that the order of this bias is given by the simple formula\np/n for all models discussed, with p the number of estimated parameters and n\nthe total sample size.\n", "category": [7]}
{"abstract": "  This paper considers the identification of treatment effects on conditional\ntransition probabilities. We show that even under random assignment only the\ninstantaneous average treatment effect is point identified. Since treated and\ncontrol units drop out at different rates, randomization only ensures the\ncomparability of treatment and controls at the time of randomization, so that\nlong-run average treatment effects are not point identified. Instead we derive\ninformative bounds on these average treatment effects. Our bounds do not impose\n(semi)parametric restrictions, for example, proportional hazards. We also\nexplore various assumptions such as monotone treatment response, common shocks\nand positively correlated outcomes that tighten the bounds.\n", "category": [7]}
{"abstract": "  We propose an inference procedure for estimators defined by mathematical\nprogramming problems, focusing on the important special cases of linear\nprogramming (LP) and quadratic programming (QP). In these settings, the\ncoefficients in both the objective function and the constraints of the\nmathematical programming problem may be estimated from data and hence involve\nsampling error. Our inference approach exploits the characterization of the\nsolutions to these programming problems by complementarity conditions; by doing\nso, we can transform the problem of doing inference on the solution of a\nconstrained optimization problem (a non-standard inference problem) into one\ninvolving inference based on a set of inequalities with pre-estimated\ncoefficients, which is much better understood. We evaluate the performance of\nour procedure in several Monte Carlo simulations and an empirical application\nto the classic portfolio selection problem in finance.\n", "category": [7]}
{"abstract": "  This paper establishes a general equivalence between discrete choice and\nrational inattention models. Matejka and McKay (2015, AER) showed that when\ninformation costs are modelled using the Shannon entropy function, the\nresulting choice probabilities in the rational inattention model take the\nmultinomial logit form. By exploiting convex-analytic properties of the\ndiscrete choice model, we show that when information costs are modelled using a\nclass of generalized entropy functions, the choice probabilities in any\nrational inattention model are observationally equivalent to some additive\nrandom utility discrete choice model and vice versa. Thus any additive random\nutility model can be given an interpretation in terms of boundedly rational\nbehavior. This includes empirically relevant specifications such as the probit\nand nested logit models.\n", "category": [7, 0, 2, 6]}
{"abstract": "  We analyze the empirical content of the Roy model, stripped down to its\nessential features, namely sector specific unobserved heterogeneity and\nself-selection on the basis of potential outcomes. We characterize sharp bounds\non the joint distribution of potential outcomes and testable implications of\nthe Roy self-selection model under an instrumental constraint on the joint\ndistribution of potential outcomes we call stochastically monotone instrumental\nvariable (SMIV). We show that testing the Roy model selection is equivalent to\ntesting stochastic monotonicity of observed outcomes relative to the\ninstrument. We apply our sharp bounds to the derivation of a measure of\ndeparture from Roy self-selection to identify values of observable\ncharacteristics that induce the most costly misallocation of talent and sector\nand are therefore prime targets for intervention. Special emphasis is put on\nthe case of binary outcomes, which has received little attention in the\nliterature to date. For richer sets of outcomes, we emphasize the distinction\nbetween pointwise sharp bounds and functional sharp bounds, and its importance,\nwhen constructing sharp bounds on functional features, such as inequality\nmeasures. We analyze a Roy model of college major choice in Canada and Germany\nwithin this framework, and we take a new look at the under-representation of\nwomen in~STEM.\n", "category": [7]}
{"abstract": "  The ongoing net neutrality debate has generated a lot of heated discussions\non whether or not monetary interactions should be regulated between content and\naccess providers. Among the several topics discussed, `differential pricing'\nhas recently received attention due to `zero-rating' platforms proposed by some\nservice providers. In the differential pricing scheme, Internet Service\nProviders (ISPs) can exempt data access charges for on content from certain CPs\n(zero-rated) while no exemption is on content from other CPs. This allows the\npossibility for Content Providers (CPs) to make `sponsorship' agreements to\nzero-rate their content and attract more user traffic. In this paper, we study\nthe effect of differential pricing on various players in the Internet. We first\nconsider a model with a monopolistic ISP and multiple CPs where users select\nCPs based on the quality of service (QoS) and data access charges. We show that\nin a differential pricing regime 1) a CP offering low QoS can make have higher\nsurplus than a CP offering better QoS through sponsorships. 2) Overall QoS\n(mean delay) for end users can degrade under differential pricing schemes. In\nthe oligopolistic market with multiple ISPs, users tend to select the ISP with\nlowest ISP resulting in same type of conclusions as in the monopolistic market.\nWe then study how differential pricing effects the revenue of ISPs.\n", "category": [7]}
{"abstract": "  This paper presents an intertemporal bimodal network to analyze the evolution\nof the semantic content of a scientific field within the framework of topic\nmodeling, namely using the Latent Dirichlet Allocation (LDA). The main\ncontribution is the conceptualization of the topic dynamics and its\nformalization and codification into an algorithm. To benchmark the\neffectiveness of this approach, we propose three indexes which track the\ntransformation of topics over time, their rate of birth and death, and the\nnovelty of their content. Applying the LDA, we test the algorithm both on a\ncontrolled experiment and on a corpus of several thousands of scientific papers\nover a period of more than 100 years which account for the history of the\neconomic thought.\n", "category": [0, 7, 5]}
{"abstract": "  This paper derives conditions under which preferences and technology are\nnonparametrically identified in hedonic equilibrium models, where products are\ndifferentiated along more than one dimension and agents are characterized by\nseveral dimensions of unobserved heterogeneity. With products differentiated\nalong a quality index and agents characterized by scalar unobserved\nheterogeneity, single crossing conditions on preferences and technology provide\nidentifying restrictions in Ekeland, Heckman and Nesheim (2004) and Heckman,\nMatzkin and Nesheim (2010). We develop similar shape restrictions in the\nmulti-attribute case. These shape restrictions, which are based on optimal\ntransport theory and generalized convexity, allow us to identify preferences\nfor goods differentiated along multiple dimensions, from the observation of a\nsingle market. We thereby derive nonparametric identification results for\nnonseparable simultaneous equations and multi-attribute hedonic equilibrium\nmodels with (possibly) multiple dimensions of unobserved heterogeneity. One of\nour results is a proof of absolute continuity of the distribution of\nendogenously traded qualities, which is of independent interest.\n", "category": [7, 7]}
{"abstract": "  In many macroeconomic applications, confidence intervals for impulse\nresponses are constructed by estimating VAR models in levels - ignoring\ncointegration rank uncertainty. We investigate the consequences of ignoring\nthis uncertainty. We adapt several methods for handling model uncertainty and\nhighlight their shortcomings. We propose a new method -\nWeighted-Inference-by-Model-Plausibility (WIMP) - that takes rank uncertainty\ninto account in a data-driven way. In simulations the WIMP outperforms all\nother methods considered, delivering intervals that are robust to rank\nuncertainty, yet not overly conservative. We also study potential ramifications\nof rank uncertainty on applied macroeconomic analysis by re-assessing the\neffects of fiscal policy shocks.\n", "category": [7, 6, 6]}
{"abstract": "  The uncertainty and robustness of Computable General Equilibrium models can\nbe assessed by conducting a Systematic Sensitivity Analysis. Different methods\nhave been used in the literature for SSA of CGE models such as Gaussian\nQuadrature and Monte Carlo methods. This paper explores the use of Quasi-random\nMonte Carlo methods based on the Halton and Sobol' sequences as means to\nimprove the efficiency over regular Monte Carlo SSA, thus reducing the\ncomputational requirements of the SSA. The findings suggest that by using\nlow-discrepancy sequences, the number of simulations required by the regular MC\nSSA methods can be notably reduced, hence lowering the computational time\nrequired for SSA of CGE models.\n", "category": [7]}
{"abstract": "  We propose a method of estimating the linear-in-means model of peer effects\nin which the peer group, defined by a social network, is endogenous in the\noutcome equation for peer effects. Endogeneity is due to unobservable\nindividual characteristics that influence both link formation in the network\nand the outcome of interest. We propose two estimators of the peer effect\nequation that control for the endogeneity of the social connections using a\ncontrol function approach. We leave the functional form of the control function\nunspecified and treat it as unknown. To estimate the model, we use a sieve\nsemiparametric approach, and we establish asymptotics of the semiparametric\nestimator.\n", "category": [7]}
{"abstract": "  Gaussian graphical models are recently used in economics to obtain networks\nof dependence among agents. A widely-used estimator is the Graphical Lasso\n(GLASSO), which amounts to a maximum likelihood estimation regularized using\nthe $L_{1,1}$ matrix norm on the precision matrix $\\Omega$. The $L_{1,1}$ norm\nis a lasso penalty that controls for sparsity, or the number of zeros in\n$\\Omega$. We propose a new estimator called Structured Graphical Lasso\n(SGLASSO) that uses the $L_{1,2}$ mixed norm. The use of the $L_{1,2}$ penalty\ncontrols for the structure of the sparsity in $\\Omega$. We show that when the\nnetwork size is fixed, SGLASSO is asymptotically equivalent to an infeasible\nGLASSO problem which prioritizes the sparsity-recovery of high-degree nodes.\nMonte Carlo simulation shows that SGLASSO outperforms GLASSO in terms of\nestimating the overall precision matrix and in terms of estimating the\nstructure of the graphical model. In an empirical illustration using a classic\nfirms' investment dataset, we obtain a network of firms' dependence that\nexhibits the core-periphery structure, with General Motors, General Electric\nand U.S. Steel forming the core group of firms.\n", "category": [7, 6]}
{"abstract": "  This paper considers the problem of forecasting a collection of short time\nseries using cross sectional information in panel data. We construct point\npredictors using Tweedie's formula for the posterior mean of heterogeneous\ncoefficients under a correlated random effects distribution. This formula\nutilizes cross-sectional information to transform the unit-specific (quasi)\nmaximum likelihood estimator into an approximation of the posterior mean under\na prior distribution that equals the population distribution of the random\ncoefficients. We show that the risk of a predictor based on a non-parametric\nestimate of the Tweedie correction is asymptotically equivalent to the risk of\na predictor that treats the correlated-random-effects distribution as known\n(ratio-optimality). Our empirical Bayes predictor performs well compared to\nvarious competitors in a Monte Carlo study. In an empirical application we use\nthe predictor to forecast revenues for a large panel of bank holding companies\nand compare forecasts that condition on actual and severely adverse\nmacroeconomic conditions.\n", "category": [7]}
{"abstract": "  There is a fast growing literature that set-identifies structural vector\nautoregressions (SVARs) by imposing sign restrictions on the responses of a\nsubset of the endogenous variables to a particular structural shock\n(sign-restricted SVARs). Most methods that have been used to construct\npointwise coverage bands for impulse responses of sign-restricted SVARs are\njustified only from a Bayesian perspective. This paper demonstrates how to\nformulate the inference problem for sign-restricted SVARs within a\nmoment-inequality framework. In particular, it develops methods of constructing\nconfidence bands for impulse response functions of sign-restricted SVARs that\nare valid from a frequentist perspective. The paper also provides a comparison\nof frequentist and Bayesian coverage bands in the context of an empirical\napplication - the former can be substantially wider than the latter.\n", "category": [7]}
{"abstract": "  We systematically investigate the effect heterogeneity of job search\nprogrammes for unemployed workers. To investigate possibly heterogeneous\nemployment effects, we combine non-experimental causal empirical models with\nLasso-type estimators. The empirical analyses are based on rich administrative\ndata from Swiss social security records. We find considerable heterogeneities\nonly during the first six months after the start of training. Consistent with\nprevious results of the literature, unemployed persons with fewer employment\nopportunities profit more from participating in these programmes. Furthermore,\nwe also document heterogeneous employment effects by residence status. Finally,\nwe show the potential of easy-to-implement programme participation rules for\nimproving average employment effects of these active labour market programmes.\n", "category": [7]}
{"abstract": "  Dynamic contracts with multiple agents is a classical decentralized\ndecision-making problem with asymmetric information. In this paper, we extend\nthe single-agent dynamic incentive contract model in continuous-time to a\nmulti-agent scheme in finite horizon and allow the terminal reward to be\ndependent on the history of actions and incentives. We first derive a set of\nsufficient conditions for the existence of optimal contracts in the most\ngeneral setting and conditions under which they form a Nash equilibrium. Then\nwe show that the principal's problem can be converted to solving\nHamilton-Jacobi-Bellman (HJB) equation requiring a static Nash equilibrium.\nFinally, we provide a framework to solve this problem by solving partial\ndifferential equations (PDE) derived from backward stochastic differential\nequations (BSDE).\n", "category": [7]}
{"abstract": "  To quantify uncertainty around point estimates of conditional objects such as\nconditional means or variances, parameter uncertainty has to be taken into\naccount. Attempts to incorporate parameter uncertainty are typically based on\nthe unrealistic assumption of observing two independent processes, where one is\nused for parameter estimation, and the other for conditioning upon. Such\nunrealistic foundation raises the question whether these intervals are\ntheoretically justified in a realistic setting. This paper presents an\nasymptotic justification for this type of intervals that does not require such\nan unrealistic assumption, but relies on a sample-split approach instead. By\nshowing that our sample-split intervals coincide asymptotically with the\nstandard intervals, we provide a novel, and realistic, justification for\nconfidence intervals of conditional objects. The analysis is carried out for a\nrich class of time series models.\n", "category": [7, 2, 6]}
{"abstract": "  This paper presents a new estimator of the intercept of a linear regression\nmodel in cases where the outcome varaible is observed subject to a selection\nrule. The intercept is often in this context of inherent interest; for example,\nin a program evaluation context, the difference between the intercepts in\noutcome equations for participants and non-participants can be interpreted as\nthe difference in average outcomes of participants and their counterfactual\naverage outcomes if they had chosen not to participate. The new estimator can\nunder mild conditions exhibit a rate of convergence in probability equal to\n$n^{-p/(2p+1)}$, where $p\\ge 2$ is an integer that indexes the strength of\ncertain smoothness assumptions. This rate of convergence is shown in this\ncontext to be the optimal rate of convergence for estimation of the intercept\nparameter in terms of a minimax criterion. The new estimator, unlike other\nproposals in the literature, is under mild conditions consistent and\nasymptotically normal with a rate of convergence that is the same regardless of\nthe degree to which selection depends on unobservables in the outcome equation.\nSimulation evidence and an empirical example are included.\n", "category": [7]}
{"abstract": "  Identification of the parameters of stable linear dynamical systems is a\nwell-studied problem in the literature, both in the low and high-dimensional\nsettings. However, there are hardly any results for the unstable case,\nespecially regarding finite time bounds. For this setting, classical results on\nleast-squares estimation of the dynamics parameters are not applicable and\ntherefore new concepts and technical approaches need to be developed to address\nthe issue. Unstable linear systems arise in key real applications in control\ntheory, econometrics, and finance. This study establishes finite time bounds\nfor the identification error of the least-squares estimates for a fairly large\nclass of heavy-tailed noise distributions, and transition matrices of such\nsystems. The results relate the time length (samples) required for estimation\nto a function of the problem dimension and key characteristics of the true\nunderlying transition matrix and the noise distribution. To establish them,\nappropriate concentration inequalities for random matrices and for sequences of\nmartingale differences are leveraged.\n", "category": [0, 7, 1, 2, 6]}
{"abstract": "  Gale, Kuhn and Tucker (1950) introduced two ways to reduce a zero-sum game by\npackaging some strategies with respect to a probability distribution on them.\nIn terms of value, they gave conditions for a desirable reduction. We show that\na probability distribution for a desirable reduction relies on optimal\nstrategies in the original game. Also, we correct an improper example given by\nthem to show that the reverse of a theorem does not hold.\n", "category": [7]}
{"abstract": "  In empirical work it is common to estimate parameters of models and report\nassociated standard errors that account for \"clustering\" of units, where\nclusters are defined by factors such as geography. Clustering adjustments are\ntypically motivated by the concern that unobserved components of outcomes for\nunits within clusters are correlated. However, this motivation does not provide\nguidance about questions such as: (i) Why should we adjust standard errors for\nclustering in some situations but not others? How can we justify the common\npractice of clustering in observational studies but not randomized experiments,\nor clustering by state but not by gender? (ii) Why is conventional clustering a\npotentially conservative \"all-or-nothing\" adjustment, and are there alternative\nmethods that respond to data and are less conservative? (iii) In what settings\ndoes the choice of whether and how to cluster make a difference? We address\nthese questions using a framework of sampling and design inference. We argue\nthat clustering can be needed to address sampling issues if sampling follows a\ntwo stage process where in the first stage, a subset of clusters are sampled\nfrom a population of clusters, and in the second stage, units are sampled from\nthe sampled clusters. Then, clustered standard errors account for the existence\nof clusters in the population that we do not see in the sample. Clustering can\nbe needed to account for design issues if treatment assignment is correlated\nwith membership in a cluster. We propose new variance estimators to deal with\nintermediate settings where conventional cluster standard errors are\nunnecessarily conservative and robust standard errors are too small.\n", "category": [2, 7, 6]}
{"abstract": "  In this paper, a unified approach is proposed to derive the exact local\nasymptotic power for panel unit root tests, which is one of the most important\nissues in nonstationary panel data literature. Two most widely used panel unit\nroot tests known as Levin-Lin-Chu (LLC, Levin, Lin and Chu (2002)) and\nIm-Pesaran-Shin (IPS, Im, Pesaran and Shin (2003)) tests are systematically\nstudied for various situations to illustrate our method. Our approach is\ncharacteristic function based, and can be used directly in deriving the moments\nof the asymptotic distributions of these test statistics under the null and the\nlocal-to-unity alternatives. For the LLC test, the approach provides an\nalternative way to obtain the results that can be derived by the existing\nmethod. For the IPS test, the new results are obtained, which fills the gap in\nthe literature where few results exist, since the IPS test is non-admissible.\nMoreover, our approach has the advantage in deriving Edgeworth expansions of\nthese tests, which are also given in the paper. The simulations are presented\nto illustrate our theoretical findings.\n", "category": [7, 6, 6]}
{"abstract": "  With the advent of Big Data, nowadays in many applications databases\ncontaining large quantities of similar time series are available. Forecasting\ntime series in these domains with traditional univariate forecasting procedures\nleaves great potentials for producing accurate forecasts untapped. Recurrent\nneural networks (RNNs), and in particular Long Short-Term Memory (LSTM)\nnetworks, have proven recently that they are able to outperform\nstate-of-the-art univariate time series forecasting methods in this context\nwhen trained across all available time series. However, if the time series\ndatabase is heterogeneous, accuracy may degenerate, so that on the way towards\nfully automatic forecasting methods in this space, a notion of similarity\nbetween the time series needs to be built into the methods. To this end, we\npresent a prediction model that can be used with different types of RNN models\non subgroups of similar time series, which are identified by time series\nclustering techniques. We assess our proposed methodology using LSTM networks,\na widely popular RNN variant. Our method achieves competitive results on\nbenchmarking datasets under competition evaluation procedures. In particular,\nin terms of mean sMAPE accuracy, it consistently outperforms the baseline LSTM\nmodel and outperforms all other methods on the CIF2016 forecasting competition\ndataset.\n", "category": [0, 0, 7, 6, 6]}
{"abstract": "  Given a sample of bids from independent auctions, this paper examines the\nquestion of inference on auction fundamentals (e.g. valuation distributions,\nwelfare measures) under weak assumptions on information structure. The question\nis important as it allows us to learn about the valuation distribution in a\nrobust way, i.e., without assuming that a particular information structure\nholds across observations. We leverage the recent contributions of\n\\cite{Bergemann2013} in the robust mechanism design literature that exploit the\nlink between Bayesian Correlated Equilibria and Bayesian Nash Equilibria in\nincomplete information games to construct an econometrics framework for\nlearning about auction fundamentals using observed data on bids. We showcase\nour construction of identified sets in private value and common value auctions.\nOur approach for constructing these sets inherits the computational simplicity\nof solving for correlated equilibria: checking whether a particular valuation\ndistribution belongs to the identified set is as simple as determining whether\na {\\it linear} program is feasible. A similar linear program can be used to\nconstruct the identified set on various welfare measures and counterfactual\nobjects. For inference and to summarize statistical uncertainty, we propose\nnovel finite sample methods using tail inequalities that are used to construct\nconfidence regions on sets. We also highlight methods based on Bayesian\nbootstrap and subsampling. A set of Monte Carlo experiments show adequate\nfinite sample properties of our inference procedures. We illustrate our methods\nusing data from OCS auctions.\n", "category": [7, 0, 0, 2, 6]}
{"abstract": "  This paper examines and proposes several attribution modeling methods that\nquantify how revenue should be attributed to online advertising inputs. We\nadopt and further develop relative importance method, which is based on\nregression models that have been extensively studied and utilized to\ninvestigate the relationship between advertising efforts and market reaction\n(revenue). Relative importance method aims at decomposing and allocating\nmarginal contributions to the coefficient of determination (R^2) of regression\nmodels as attribution values. In particular, we adopt two alternative\nsubmethods to perform this decomposition: dominance analysis and relative\nweight analysis. Moreover, we demonstrate an extension of the decomposition\nmethods from standard linear model to additive model. We claim that our new\napproaches are more flexible and accurate in modeling the underlying\nrelationship and calculating the attribution values. We use simulation examples\nto demonstrate the superior performance of our new approaches over traditional\nmethods. We further illustrate the value of our proposed approaches using a\nreal advertising campaign dataset.\n", "category": [7, 6]}
{"abstract": "  This paper characterizes the minimax linear estimator of the value of an\nunknown function at a boundary point of its domain in a Gaussian white noise\nmodel under the restriction that the first-order derivative of the unknown\nfunction is Lipschitz continuous (the second-order H\\\"{o}lder class). The\nresult is then applied to construct the minimax optimal estimator for the\nregression discontinuity design model, where the parameter of interest involves\nfunction values at boundary points.\n", "category": [7, 2, 6]}
{"abstract": "  We review recent advances in modal regression studies using kernel density\nestimation. Modal regression is an alternative approach for investigating\nrelationship between a response variable and its covariates. Specifically,\nmodal regression summarizes the interactions between the response variable and\ncovariates using the conditional mode or local modes. We first describe the\nunderlying model of modal regression and its estimators based on kernel density\nestimation. We then review the asymptotic properties of the estimators and\nstrategies for choosing the smoothing bandwidth. We also discuss useful\nalgorithms and similar alternative approaches for modal regression, and propose\nfuture direction in this field.\n", "category": [6, 7]}
{"abstract": "  The recent research report of U.S. Department of Energy prompts us to\nre-examine the pricing theories applied in electricity market design. The\ntheory of spot pricing is the basis of electricity market design in many\ncountries, but it has two major drawbacks: one is that it is still based on the\ntraditional hourly scheduling/dispatch model, ignores the crucial time\ncontinuity in electric power production and consumption and does not treat the\ninter-temporal constraints seriously; the second is that it assumes that the\nelectricity products are homogeneous in the same dispatch period and cannot\ndistinguish the base, intermediate and peak power with obviously different\ntechnical and economic characteristics. To overcome the shortcomings, this\npaper presents a continuous time commodity model of electricity, including spot\npricing model and load duration model. The market optimization models under the\ntwo pricing mechanisms are established with the Riemann and Lebesgue integrals\nrespectively and the functional optimization problem are solved by the\nEuler-Lagrange equation to obtain the market equilibria. The feasibility of\npricing according to load duration is proved by strict mathematical derivation.\nSimulation results show that load duration pricing can correctly identify and\nvalue different attributes of generators, reduce the total electricity\npurchasing cost, and distribute profits among the power plants more equitably.\nThe theory and methods proposed in this paper will provide new ideas and\ntheoretical foundation for the development of electric power markets.\n", "category": [7, 5]}
{"abstract": "  We generalize the approach of Carlier (2001) and provide an existence proof\nfor the multidimensional screening problem with general nonlinear preferences.\nWe first formulate the principal's problem as a maximization problem with\n$G$-convexity constraints and then use $G$-convex analysis to prove existence.\n", "category": [7, 2]}
{"abstract": "  This study proposes a simple technique for propensity score matching for\nmultiple treatment levels under the strong unconfoundedness assumption with the\nhelp of the Aitchison distance proposed in the field of compositional data\nanalysis (CODA).\n", "category": [7]}
{"abstract": "  Binary classification is highly used in credit scoring in the estimation of\nprobability of default. The validation of such predictive models is based both\non rank ability, and also on calibration (i.e. how accurately the probabilities\noutput by the model map to the observed probabilities). In this study we cover\nthe current best practices regarding calibration for binary classification, and\nexplore how different approaches yield different results on real world credit\nscoring data. The limitations of evaluating credit scoring models using only\nrank ability metrics are explored. A benchmark is run on 18 real world\ndatasets, and results compared. The calibration techniques used are Platt\nScaling and Isotonic Regression. Also, different machine learning models are\nused: Logistic Regression, Random Forest Classifiers, and Gradient Boosting\nClassifiers. Results show that when the dataset is treated as a time series,\nthe use of re-calibration with Isotonic Regression is able to improve the long\nterm calibration better than the alternative methods. Using re-calibration, the\nnon-parametric models are able to outperform the Logistic Regression on Brier\nScore Loss.\n", "category": [7, 6]}
{"abstract": "  Ratio of medians or other suitable quantiles of two distributions is widely\nused in medical research to compare treatment and control groups or in\neconomics to compare various economic variables when repeated cross-sectional\ndata are available. Inspired by the so-called growth incidence curves\nintroduced in poverty research, we argue that the ratio of quantile functions\nis a more appropriate and informative tool to compare two distributions. We\npresent an estimator for the ratio of quantile functions and develop\ncorresponding simultaneous confidence bands, which allow to assess significance\nof certain features of the quantile functions ratio. Derived simultaneous\nconfidence bands rely on the asymptotic distribution of the quantile functions\nratio and do not require re-sampling techniques. The performance of the\nsimultaneous confidence bands is demonstrated in simulations. Analysis of the\nexpenditure data from Uganda in years 1999, 2002 and 2005 illustrates the\nrelevance of our approach.\n", "category": [6, 7, 6]}
{"abstract": "  Constraining the maximum likelihood density estimator to satisfy a\nsufficiently strong constraint, $\\log-$concavity being a common example, has\nthe effect of restoring consistency without requiring additional parameters.\nSince many results in economics require densities to satisfy a regularity\ncondition, these estimators are also attractive for the structural estimation\nof economic models. In all of the examples of regularity conditions provided by\nBagnoli and Bergstrom (2005) and Ewerhart (2013), $\\log-$concavity is\nsufficient to ensure that the density satisfies the required conditions.\nHowever, in many cases $\\log-$concavity is far from necessary, and it has the\nunfortunate side effect of ruling out sub-exponential tail behavior.\n  In this paper, we use optimal transport to formulate a shape constrained\ndensity estimator. We initially describe the estimator using a $\\rho-$concavity\nconstraint. In this setting we provide results on consistency, asymptotic\ndistribution, convexity of the optimization problem defining the estimator, and\nformulate a test for the null hypothesis that the population density satisfies\na shape constraint. Afterward, we provide sufficient conditions for these\nresults to hold using an arbitrary shape constraint. This generalization is\nused to explore whether the California Department of Transportation's decision\nto award construction contracts with the use of a first price auction is cost\nminimizing. We estimate the marginal costs of construction firms subject to\nMyerson's (1981) regularity condition, which is a requirement for the first\nprice reverse auction to be cost minimizing. The proposed test fails to reject\nthat the regularity condition is satisfied.\n", "category": [7, 6]}
{"abstract": "  We present the calibrated-projection MATLAB package implementing the method\nto construct confidence intervals proposed by Kaido, Molinari and Stoye (2017).\nThis manual provides details on how to use the package for inference on\nprojections of partially identified parameters. It also explains how to use the\nMATLAB functions we developed to compute confidence intervals on solutions of\nnonlinear optimization problems with estimated constraints.\n", "category": [7, 6]}
{"abstract": "  In this paper we study methods for estimating causal effects in settings with\npanel data, where some units are exposed to a treatment during some periods and\nthe goal is estimating counterfactual (untreated) outcomes for the treated\nunit/period combinations. We propose a class of matrix completion estimators\nthat uses the observed elements of the matrix of control outcomes corresponding\nto untreated unit/periods to impute the \"missing\" elements of the control\noutcome matrix, corresponding to treated units/periods. This leads to a matrix\nthat well-approximates the original (incomplete) matrix, but has lower\ncomplexity according to the nuclear norm for matrices. We generalize results\nfrom the matrix completion literature by allowing the patterns of missing data\nto have a time series dependency structure that is common in social science\napplications. We present novel insights concerning the connections between the\nmatrix completion literature, the literature on interactive fixed effects\nmodels and the literatures on program evaluation under unconfoundedness and\nsynthetic control methods. We show that all these estimators can be viewed as\nfocusing on the same objective function. They differ solely in the way they\ndeal with identification, in some cases solely through regularization (our\nproposed nuclear norm matrix completion estimator) and in other cases primarily\nthrough imposing hard restrictions (the unconfoundedness and synthetic control\napproaches). The proposed method outperforms unconfoundedness-based or\nsynthetic control estimators in simulations based on real data.\n", "category": [2, 7, 6]}
{"abstract": "  Artificial intelligence (AI) has achieved superhuman performance in a growing\nnumber of tasks, but understanding and explaining AI remain challenging. This\npaper clarifies the connections between machine-learning algorithms to develop\nAIs and the econometrics of dynamic structural models through the case studies\nof three famous game AIs. Chess-playing Deep Blue is a calibrated value\nfunction, whereas shogi-playing Bonanza is an estimated value function via\nRust's (1987) nested fixed-point method. AlphaGo's \"supervised-learning policy\nnetwork\" is a deep neural network implementation of Hotz and Miller's (1993)\nconditional choice probability estimation; its \"reinforcement-learning value\nnetwork\" is equivalent to Hotz, Miller, Sanders, and Smith's (1994) conditional\nchoice simulation method. Relaxing these AIs' implicit econometric assumptions\nwould improve their structural interpretability.\n", "category": [7, 0, 0]}
{"abstract": "  We consider an index model of dyadic link formation with a homophily effect\nindex and a degree heterogeneity index. We provide nonparametric identification\nresults in a single large network setting for the potentially nonparametric\nhomophily effect function, the realizations of unobserved individual fixed\neffects and the unknown distribution of idiosyncratic pairwise shocks, up to\nnormalization, for each possible true value of the unknown parameters. We\npropose a novel form of scale normalization on an arbitrary interquantile\nrange, which is not only theoretically robust but also proves particularly\nconvenient for the identification analysis, as quantiles provide direct\nlinkages between the observable conditional probabilities and the unknown index\nvalues. We then use an inductive \"in-fill and out-expansion\" algorithm to\nestablish our main results, and consider extensions to more general settings\nthat allow nonseparable dependence between homophily and degree heterogeneity,\nas well as certain extents of network sparsity and weaker assumptions on the\nsupport of unobserved heterogeneity. As a byproduct, we also propose a concept\ncalled \"modeling equivalence\" as a refinement of \"observational equivalence\",\nand use it to provide a formal discussion about normalization, identification\nand their interplay with counterfactuals.\n", "category": [7]}
{"abstract": "  Peer-to-peer (P2P) lending is a fast growing financial technology (FinTech)\ntrend that is displacing traditional retail banking. Studies on P2P lending\nhave focused on predicting individual interest rates or default probabilities.\nHowever, the relationship between aggregated P2P interest rates and the general\neconomy will be of interest to investors and borrowers as the P2P credit market\nmatures. We show that the variation in P2P interest rates across grade types\nare determined by three macroeconomic latent factors formed by Canonical\nCorrelation Analysis (CCA) - macro default, investor uncertainty, and the\nfundamental value of the market. However, the variation in P2P interest rates\nacross term types cannot be explained by the general economy.\n", "category": [7, 6]}
{"abstract": "  In this paper, we provide some new results for the Weibull-R family of\ndistributions (Alzaghal, Ghosh and Alzaatreh (2016)). We derive some new\nstructural properties of the Weibull-R family of distributions. We provide\nvarious characterizations of the family via conditional moments, some functions\nof order statistics and via record values.\n", "category": [2, 7, 6]}
{"abstract": "  Double machine learning provides $\\sqrt{n}$-consistent estimates of\nparameters of interest even when high-dimensional or nonparametric nuisance\nparameters are estimated at an $n^{-1/4}$ rate. The key is to employ\nNeyman-orthogonal moment equations which are first-order insensitive to\nperturbations in the nuisance parameters. We show that the $n^{-1/4}$\nrequirement can be improved to $n^{-1/(2k+2)}$ by employing a $k$-th order\nnotion of orthogonality that grants robustness to more complex or\nhigher-dimensional nuisance parameters. In the partially linear regression\nsetting popular in causal inference, we show that we can construct second-order\northogonal moments if and only if the treatment residual is not normally\ndistributed. Our proof relies on Stein's lemma and may be of independent\ninterest. We conclude by demonstrating the robustness benefits of an explicit\ndoubly-orthogonal estimation procedure for treatment effect.\n", "category": [0, 7, 2, 6, 6]}
{"abstract": "  We assess the relationship between model size and complexity in the\ntime-varying parameter VAR framework via thorough predictive exercises for the\nEuro Area, the United Kingdom and the United States. It turns out that\nsophisticated dynamics through drifting coefficients are important in small\ndata sets while simpler models tend to perform better in sizeable data sets. To\ncombine best of both worlds, novel shrinkage priors help to mitigate the curse\nof dimensionality, resulting in competitive forecasts for all scenarios\nconsidered. Furthermore, we discuss dynamic model selection to improve upon the\nbest performing individual model for each point in time.\n", "category": [6, 7, 6, 6]}
